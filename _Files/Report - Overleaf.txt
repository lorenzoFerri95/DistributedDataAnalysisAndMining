\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{lscape}
\usepackage[title]{appendix}
\usepackage{rotating}
\usepackage[margin=2cm]{geometry}
\usepackage{natbib}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{wrapfig}
\graphicspath{ {figures/} }
\usepackage{graphicx}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{amsmath}
\usepackage{comment}
\addtolength{\topmargin}{0 cm}
\addtolength{\textheight}{0 cm}

\usepackage{float}
\usepackage{subfig}

\pagenumbering{roman}


\usepackage{multicol}
\setlength{\columnsep}{1cm}

\title{\includegraphics[width=8cm]{UNIPI_Nodo.png}\\Distributed Analysis and Mining of a Bank Loan Status Dataset \newline\newline
\large Data Preparation, Data Understanding, Classification and Pattern Mining in a Hadoop/Spark Computing Framework}


\author{Ferri Lorenzo (607828) \\ email: \href{mailto: l.ferri11@studenti.unipi.it}{l.ferri11@studenti.unipi.it} \and Ilic Ema (602796) \\ email: \href{mailto:ema.ilic9@gmail.com}{ema.ilic9@gmail.com} \and Pappolla Roberta (534109) \\ email:
 \href{mailto:r.pappolla@studenti.unipi.it}{r.pappolla@studenti.unipi.it} }
\date{Distributed Data Analysis and Mining (687AA), Academic Year 2020/2021}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic} %conversione in numeri arabi

\begin{multicols}{2}
\section{Context}
%The goal of the Distributed Data Analysis and Mining course and the project is to simulate the Big Data Analytics process. Given that the huge quantities of data generated nowadays cannot be successfully processed by a regular personal computer, the Hadoop/Spark Framework is used to tackle the issue. The idea behind it is the following: dividing a large problem into smaller ones that are concurrently solved by many single processors. This is called the MapReduce paradigm.
The dataset was taken from the Kaggle website. The goal of the Analysis is to predict whether the person in question will default on paying his debt or not, based on the features of this debtor/loan combination available in the dataset.
Even though the datasets seemed clean at a first glance, after the careful analysis it was clear that some serious data preparation was needed due to numerous missing values, outliers and errors. This will be tackled in the second chapter (Data Preparation) of this report. The following chapter (Data Understanding), on the other hand, discusses the final, cleaned dataset which was used as an input for the Machine Learning methods (Chapter 4).


\section{Data Preparation, Clustering and Features Engineering}
All the main changes to the dataset were made with RDD's and are well documented in the notebook '1\_DataPreparation\_Clustering\_FeaturesEngineering'. They mainly concern modifications to the attributes in order to make them more consistent, e.g. by changing the currency denomination of the money quantities from Rubles to Euros and by adjusting some attributes values to make them numeric rather than strings (e.g. values '10+ years' or '<1 year' in 'Years\_In\_Current\_Job'). Some errors, such as the erroneous '0's at the end of the 'Credit\_Score' quantities, '99,999,999' value of 'Current\_Loan\_Amount' and the duplicate rows have been corrected as well. After these improvements the dataset was characterized by the uniqueness of the ID's (none of 'Loan\_ID' and 'Customer\_ID' were duplicated), and they were dropped. The only thing left to do was to fill the missing values. The following strategy were chosen: Attributes with few Missing Values have been filled with the Median of some given sensible grouping sets; the others have been filled againg with the Median of some grouping sets and by adding to these groupings the Label of the best clustering we found.
The best clustering was found fitting models on the Standardised Numerical Features of the entire dataset and searching among three different models and several K's iterations.
This Clustering turned out to be a "Gaussian Mixture Model" with 2 Clusters, achieving a Silhouette Score of 0.778. The resulting plot of both the clusters and the dataset output class (for comparison purposes) is displayed below (the dataset dimensionalities are reduced with the principal component analysis).

\begin{figure}[H] \centering
    \includegraphics[width=8cm]{Clusters_Labels.png}
\end{figure}


\noindent Further features were engineered from the existing ones: 'Installment\_Rate', 'Debt\_Income\_Rate', and finally 'Credit\_Problems\_Perc', that was dropped later on after having observed a too high correlation with the variable 'Number\_of\_Credit\_Problems' from which it derives. \newline
The final dataset was exported to the HDFS in the "Parquet" (columnar store) format.



\section{Data Understanding and Dimensionality Reduction}
\subsection{Variable Interpretations and Types}

Even though all the definitions can be found in the notebook, we provide the definitions of the most prominent variables here.

%'Annual\_Income': Annual income of the person in question (numerical continuous). \newline
%'Bankruptcies': Number of bankruptcies the relevant person has experienced so far  (numerical discrete).\newline
\noindent 'Loan\_Status': The label in output:  Whether the Loan has been paid off or not(categorical, class variable). \newline
'Credit\_Score': the lower the Credit Score, the more likely the debtor is to default (numerical continuous).\newline
'Current\_Credit\_Balance': the amount of money that a client of a financial institution has in his or her account (numerical continuous).\newline
'Current\_Loan\_Amount':  Loan Amount is the amount the borrower promises to repay, as set forth in the loan contract (numerical continuous). \newline
%'Home\_Ownership': Type of home the debtor is in posses of (categorical).\newline
'Maximum\_Open\_Credit': An open credit is a financial arrangement between a lender and a borrower that allows the latter to access credit repeatedly up to a specific maximum limit (numerical continuous). \newline
%'Monthly\_Debt': Monthly debt payments are any payments you make to pay back a creditor or lender for money you borrowed (numerical continuous).\newline
%'Months\_since\_last\_delinquent': Number of months since the last law offense or violation (if the person never committed such an act, the value is -1) (numerical discrete). \newline
'Number\_of\_Credit\_Problems': Credit problems include lack of enough credit history, denied credit application, fraud and identity theft (numerical discrete). \newline
%'Number\_of\_Open\_Accounts': The open account definition is an account which remains to be paid. Open account is also known as an account payable by the bearer (numerical discrete).\newline 
%'Purpose': Purpose of the debt (categorical).\newline
'Tax\_Liens': A tax lien is a lien imposed by law upon a property to secure the payment of taxes. A tax lien may be imposed for delinquent taxes owed on real property or personal property, or as a result of failure to pay income taxes or other taxes (numerical discrete).\newline
'Term': Time the debtor has to pay off the debt. Categorical variable taking on only two values: Short or Long Term (categorical).\newline
%'Years\_in\_current\_job': For how many years has the person been employed at the current position (numerical discrete). \newline
'Installment\_Rate': the percentage of the Monthly Debt over the overall Loan Amount; \newline
'Debt\_Income\_Rate': the percentage of the Annual Debt over the Annual Income;


\subsection{Statistics and Exploarative Analysis}
The clean training set contained 72997 values, of which 50783 (70\%) rows had the dependent binary variable 'Loan\_Status' as 'Fully Paid'   and 22214 (30\%) values as 'Charged Off'. Thus, the dataset was not considered imbalanced and no further balancing methods were used.

\subsubsection{Numerical Variables}
\noindent The statistical information relevant for each of the numerical values, such as average, median, number of distinct values, minimum and maximum value can be observed in the spark dataframe tables in  notebook '2\_Data\_Understanding\_\&\_Statistics'. For the sake of visualizing a Big Data scenario,
Spark Dataframe was sampled to approximately 0.3\% and the output classes were balanced to approximately 50:50. In figure below a correlation heatmap matrix  can be observed displaying the correlations between different attributes. As no pair of attributes had a particularly high correlation, there was no need for eliminating any attributes.


\begin{figure}[H] \centering
    \includegraphics[width=9 cm]{corr matrix.png}
\end{figure}


\noindent Another interesting visualization is this scattermatrix, representing the sampled plot of the most prominent numerical attributes.


\begin{figure} [H]
\centering
    \includegraphics[width=9 cm]{scatter matrix.png}
\end{figure}


\noindent Additionally, four prominent density plots  were chosen and included in the report, but for the sake of the conciseness of the report, only two will be discussed. Namely, we can observe how both 'Years\_in\_Current\_Job' and 'Bankruptcies' have a bimodal distribution. From the small sample of only 178 values, most debtors had no previous bankruptcies, whereas a very small number had one bankruptcy, and an even smaller number of debtors had five bankruptcies. In this case, these are considered outliers. The peak at the Years\_in\_Current\_Job equal to 10 is explained by the fact that in the original dataset, this attribute used to be a categorical string, and as mentioned in data preparation, all of the '10+ years' values were mapped to a number 10. Thus, one could say that the true mean is the one at approximately 2 years.


\begin{figure}[H] \centering
    \includegraphics[width=4cm]{Years of Credit History.png}
    \includegraphics[width=4cm]{years in current job dist.png}
\end{figure}
\begin{figure}[H] \centering
    \includegraphics[width=4cm]{credit score dist.png}
    \includegraphics[width=4cm]{Bankruptcies.png}
    \end{figure}


\subsubsection{Categorical Variables}
\noindent On the figures below, the Corsstab representing different types of home ownership and purposes can be observed with respect to the class variable. These plots refers to the entire dataset because we got their values with a query on the Spark Dataframe.

\begin{figure}[H] \centering
    \includegraphics[width=4cm]{home ownership.png}
    \includegraphics[width=4cm]{purpose.png}
\end{figure}

\noindent Aditionally, but just for the sampled dataset, the boxplots below goes more in-depth on the distribution of Credit Score per  different Home Ownership categories with respect to the Loan Status variable, as well as its outliers.
\begin{figure}[H] \centering
    \includegraphics[width=8cm]{box home.png}
\end{figure}



\subsection{Dimensionality Reduction with PCA}
Dimensionality Reduction method called Principal Component Analysis was applied in order to see whether the Dataset can be reduced  to k components preserving the highest variance without significant loss in accuracy when a classifier is applied. The results of 2-dimensional and 3-dimensional PCA analysis are displayed in figures below. Note that the plots portrayed here are different than the ones in the notebook: Namely, the plots in this report do not contain the numerical attributes with categorical-looking values (e.g. 'Years\_in\_current\_job', 'Tax\_Liens'...) which distort the visualizations.


\begin{figure}[H] \centering
    \includegraphics[width=6cm]{2pca.png}
    \includegraphics[width=8cm]{3pca.png}
\end{figure}


\noindent The results in terms of accuracy were impressive. Namely, it was seen that the number of dimensions can be cut down to a half (from the original 14 down to 7) at the cost of just one percent of accuracy! As much of a revelation that this is, it was decided that the further analysis will be continued with the non-reduced dataset for the sake of continuing the analysis on the original attributes.



\section{Classification, Regression and Pattern Mining}
A function was defined with the scope of preparing the dataset in different ways for Machine Learning analysis. This function, when needed, encodes categorical variables, standardizes the dataset, vectorizes numerical features and encodes the dependent (class) variable. These transformations have been always applied to the training set and test set, after having fitted the transformations Pipeline itself only on the entire Dataframe. This fact is important in order to make transformed values (eg. scaled values) coherent between each other in both the training and test sets.


\subsection{Classification for predicting 'Loan\_Status'}
The classification was conducted in ML for the most part, even though also MLlib was tested.
Grid search 3-fold Cross Validation was used in order to find the right parameters for all of the classifiers. Parallelism hyperparameter in the grid search function was set to 10, which means that 10 models will be trained and evaluated simultaneously each time to make better use of cluster resources. Finally, below each model we can observe the hyperparameters chosen as the best by the grid search function by maximizing the Area Under the Curve metric. All the results can be observed in the '3\_Classification\_Regression\_PatternMining' notebook.


%\subsubsection{Data Preparation for Numeric Classifiers}
\noindent  For the Numeric classifiers (Logistic Regression and Support Vector Machines), only the numeric columns were isolated, vectorized and standardized, in order to be used for training and testing the models.


%\subsubsection{Data Preparation for Mixed Categorical and Numeric Classifiers}
\noindent For mixed Categorical and Numerical Classifiers (Naive Bayes, Decision Tree and Random Forest) all of the categorical features were encoded and the resulting numeric features (including obviously the original numeric ones) were vectorized together.

\subsubsection{Results}

\begin{center}
    \begin{tabular}{ | m{1.7cm} | m{1.3cm} |m{1.3cm} |m{1.0cm} |m{0.8cm} | } 
        \hline
        Classifier & Accuracy & Precision & Recall & F1 \\
        \hline
        Logistic Regression (ML) & 0.698 & 0.654 & 0.698 & 0.592 \\
        \hline
        Support Vector Machines & 0.696 & 0.484 & 0.696 & 0.571 \\
        \hline
        Naive Bayes Classifier & 0.436 & 0.617 & 0.436 & 0.427 \\
        \hline
        Decision Tree Classifier & 0.6 & 0.603 & 0.6 & 0.602\\
        \hline
        Random Forest Classifier & 0.701 & 0.666 & 0.701 & 0.606 \\
        \hline
        Logistic Regression (MLlib) & 0.587 & 0.645 & 0.575 & 0.605  \\
        \hline
    \end{tabular}
\end{center}


\noindent Some of the more prominent hyperparameter settings  relevant for each of the classifiers are stated below:
\newline
\textbf{Logistic Regression}: Elastic Net Mixing Parameter: 0, Regularization Parameter: 0.01;
\newline
\textbf{Naive Bayes Classifier}: smoothing parameter: 0.7
\newline
\textbf{Decision Tree Classifier}: Maximum Depth of the Tree: 30, Impurity measure: Entropy;
\newline
\textbf{Random Forest Classifier}: Maximum depth: 10, Number of trees to train: 30.

\noindent As can be seen from the above table, the classifier which obtained the best results for all the four measures is the Random Forest Ensemble classifier. Thus, the composition of all the trees in the model can be found in the Jupyter notebook, and the feature importance analysis  was conducted for this classifier. On the plot below, it is visible that 'Credit\_Score' discriminates the 'Loan\_Status' variable the best, followed by 'Term' and 'Annual\_Income'. This is unsurprising as 'Credit\_Score' is ultimately the measure which serves to decide whether the person gets the loan or not.

\begin{figure}[H] \centering
    \includegraphics[width=8cm]{Classification_featuresImportance.png}
\end{figure}

\noindent It is interesting to observe how the 'Debt\_Income\_Rate', a feature which was additionally engineered and 'Credit\_Score' discriminate between the values of 'Loan\_Status'. For higher 'Debt\_Income\_Rate' and Credit\_Score, there are less 'Charged\_Off' values. More importantly, rows with  lower 'Credit\_Score' values are more likely to take on 'Charged\_Off' value of output.

\begin{figure}[H] \centering
    \includegraphics[width=8cm]{CreditScore_DebtIncomeRate.png}
\end{figure}

\noindent As 'Term' was shown to be the second variable which discriminates the class in output the best, a graphical representation of the result follows: Long term debts have a higher relative rate of 'Charged Off' values, meaning that the long term debts are riskier.
\begin{figure}[H] \centering
    \includegraphics[width=8cm]{Term_Crosstab.png}

\end{figure}

\noindent Finally, as Random Forest came out as the best model, the next thing that was done on it was re-fitting the model on the PCA-reduced dataset. Namely,  the best RF model  was inserted in the pipeline with PCA, and 3-fold grid-search was applied to find out whether the dataset can be reduced without a significant trade-off in terms of relevant metrics. The results showed that the dataset can be reduced from 19 dimensions (numerical + encoded categorical values) down to 16 with absolutely no loss in terms of AUC. Fitting the model and calculating the remaining metrics on the reduced dataset, the results were confirmed: Accuracy, Precision, Recall and F1 changed only by the fourth decimal. Just as before with the Logistic Regression, PCA proved to be a successful tool in dealing with big data.
Thus, the best model chosen to predict the 'Loan\_Status' variable in this  Machine Learning analysis is Random Forest reduced to 16 columns using PCA.

\subsection{Linear and Random Forest Regression for predicting 'Credit\_Score'}

Since 'Credit\_Score' was found to be the most important attribute in predicting the output label 'Loan\_Status', a predictive analysis was performed on its values. The features used in the regression were only those which directly concern either the client, or the client's bank history. The features relevant to the nature of the debt ('Term', 'Purpose') were excluded. The models used were the 'Linear Regression' and the 'Random Forest Regressor', with two different Data Preparation steps: for the Linear Regression only the scaled numerical features were used, whereas for the Random Forest Regressor both numerical and encoded categorical features were used, but without the scaling step. Again, Grid Search was used in order to find the best parameters for the models. Here are the best parameters:
\newline
\textbf{'Linear Regression'}: loss function type to be optimized in the algorithm: 'huber', regularization entity, 0.5;
\newline
\textbf{'Random Forest Regressor'} : number of trees of the Ensamble method: 30, maximum depth of the trees: 10. \newline
Feature Importance analysis showed the most important feature to be 'Current\_Loan\_Amount', with an importance of 22.2\% for the prediction of Credit\_Score. All the importances are given in the following figure.



\begin{figure}[H] \centering
    \includegraphics[width=8cm]{Regression_featuresImportance.png}
\end{figure}

\noindent However, none of the regressors showed impressive results in terms of the relevant metrics: Namely, Linear Regressor and Random Forest Regressor obtained an R squared of -0.02 and 0.15, respectively. Thus, the conclusion is that a good regression model cannot be obtained with this dataset.



\end{multicols}
\end{document}