{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# librerie\n",
    "\n",
    "from numpy.random import choice as sample\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "######################## ml ###############################\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "######################## MLlib ###############################\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors as Vectors_mllib\n",
    "from pyspark.mllib.feature import StandardScaler as StandardScaler_mllib\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans as KMeans_mllib\n",
    "from pyspark.mllib.clustering import BisectingKMeans as BisectingKMeans_mllib\n",
    "from pyspark.mllib.clustering import GaussianMixture as GaussianMixture_mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessione\n",
    "\n",
    "sc = SparkContext(appName=\"DDAM_Project\", master=\"local[*]\")\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DDAM_Project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: integer (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Annual_Income: integer (nullable = true)\n",
      " |-- Years_in_current_job: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: integer (nullable = true)\n",
      " |-- Number_of_Credit_Problems: integer (nullable = true)\n",
      " |-- Current_Credit_Balance: integer (nullable = true)\n",
      " |-- Maximum_Open_Credit: integer (nullable = true)\n",
      " |-- Bankruptcies: string (nullable = true)\n",
      " |-- Tax_Liens: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/credit_train.csv\", sep=\",\",\n",
    "                     inferSchema=True, header=True)\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "# rinominare le colonne sotituendo lo spazio con l'underscore\n",
    "for col in columns:\n",
    "    sdf = sdf.withColumnRenamed(col, col.replace(' ', '_'))\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nbr_nulls(spark_df, view_name, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori nulli presenti in ogni attributo\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    columns_temp = spark_df.schema.names\n",
    "    \n",
    "    Project = []\n",
    "    for col in columns_temp:\n",
    "        Project.append('SUM(CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_nulls = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_nulls.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "        \n",
    "    return nbr_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:        514\n",
      "Customer_ID:        514\n",
      "Loan_Status:        514\n",
      "Current_Loan_Amount:        514\n",
      "Term:        514\n",
      "Credit_Score:      19668\n",
      "Annual_Income:      19668\n",
      "Years_in_current_job:        514\n",
      "Home_Ownership:        514\n",
      "Purpose:        514\n",
      "Monthly_Debt:        514\n",
      "Years_of_Credit_History:        514\n",
      "Months_since_last_delinquent:        514\n",
      "Number_of_Open_Accounts:        514\n",
      "Number_of_Credit_Problems:        514\n",
      "Current_Credit_Balance:        514\n",
      "Maximum_Open_Credit:        516\n",
      "Bankruptcies:        514\n",
      "Tax_Liens:        514\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbr_distincts(spark_df, view_name, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori distinti di ciascun attributo.\n",
    "    il valore nullo non viene contato come valore distinto\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    columns_temp = spark_df.schema.names\n",
    "\n",
    "    Project = []\n",
    "    for col in columns_temp:\n",
    "        Project.append('COUNT(DISTINCT {0}) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_distincts = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_distincts.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "\n",
    "    return nbr_distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:      81999\n",
      "Customer_ID:      81999\n",
      "Loan_Status:          2\n",
      "Current_Loan_Amount:      22004\n",
      "Term:          2\n",
      "Credit_Score:        324\n",
      "Annual_Income:      36174\n",
      "Years_in_current_job:         12\n",
      "Home_Ownership:          4\n",
      "Purpose:         16\n",
      "Monthly_Debt:      65765\n",
      "Years_of_Credit_History:        506\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Current_Credit_Balance:      32730\n",
      "Maximum_Open_Credit:      44596\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modifiche agli attributi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'le Row sono tipi particolari di Tuple, quindi sono oggetti immutabili.\\nin tutto il notebook allora per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni)\\nin RDD di Dictionaries.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sdf.rdd  # di default questa trasformazione genera un RDD di Row()\n",
    "\n",
    "'''le Row sono tipi particolari di Tuple, quindi sono oggetti immutabili.\n",
    "in tutto il notebook allora per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni)\n",
    "in RDD di Dictionaries.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gli attributi che riguardano somme di denaro sono quantità denominate in valuta russa (Rubli). Il dataset si riferisce a dati del 2016, quindi per rendere più comprensibile il significato di queste somme tutti questi attributi vengono convertiti in Euro dividendo tutti i loro valori per il tasso di cambio EUR/RUB medio arrotondato alle decine dell'anno 2016: 70.\n",
    "\n",
    "questa modifica non impatta assolutamente nessuna analisi perché tutte le quantità monetarie vengono trasformate alla stessa maniera e quindi le proporzioni vengono mantenute. Sarà sempre possibile trasformare facilmente di nuovo in Rubli qualora sia necessario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Current_Loan_Amount'] is not None:\n",
    "        d['Current_Loan_Amount'] = round(d['Current_Loan_Amount']/70)\n",
    "    \n",
    "    if d['Annual_Income'] is not None:    \n",
    "        d['Annual_Income'] = round(d['Annual_Income']/70)\n",
    "    \n",
    "    if d['Monthly_Debt'] is not None:\n",
    "        d['Monthly_Debt'] = round(float(d['Monthly_Debt']/70), 4)\n",
    "        \n",
    "    if d['Current_Credit_Balance'] is not None:\n",
    "        d['Current_Credit_Balance'] = round(d['Current_Credit_Balance']/70)\n",
    "        \n",
    "    if d['Maximum_Open_Credit'] is not None:\n",
    "        d['Maximum_Open_Credit'] = round(d['Maximum_Open_Credit']/70)\n",
    "    \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ci sono alcuni attributi che hanno un Data Type incoerente con il significato dell'attributo: sono letti come stringhe ma in realtà la loro semantica ci suggerisce di trasformarli in numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|Years_in_current_job|count|\n",
      "+--------------------+-----+\n",
      "|           10+ years|31121|\n",
      "|             2 years| 9134|\n",
      "|             3 years| 8169|\n",
      "|            < 1 year| 8164|\n",
      "|             5 years| 6787|\n",
      "|              1 year| 6460|\n",
      "|             4 years| 6143|\n",
      "|             6 years| 5686|\n",
      "|             7 years| 5577|\n",
      "|             8 years| 4582|\n",
      "|                 n/a| 4222|\n",
      "|             9 years| 3955|\n",
      "|                null|  514|\n",
      "+--------------------+-----+\n",
      "\n",
      "+----------------------------+-----+\n",
      "|Months_since_last_delinquent|count|\n",
      "+----------------------------+-----+\n",
      "|                          NA|53141|\n",
      "|                          13|  922|\n",
      "|                          12|  902|\n",
      "|                          14|  877|\n",
      "|                          15|  865|\n",
      "|                          10|  861|\n",
      "|                           8|  856|\n",
      "|                           9|  849|\n",
      "|                          18|  847|\n",
      "|                          16|  837|\n",
      "|                           6|  836|\n",
      "|                           7|  825|\n",
      "|                          21|  812|\n",
      "|                          17|  809|\n",
      "|                          19|  791|\n",
      "|                          11|  779|\n",
      "|                          22|  775|\n",
      "|                          23|  773|\n",
      "|                          20|  750|\n",
      "|                          28|  746|\n",
      "|                          25|  738|\n",
      "|                          30|  726|\n",
      "|                          27|  723|\n",
      "|                          26|  723|\n",
      "|                          24|  715|\n",
      "|                           5|  703|\n",
      "|                          38|  697|\n",
      "|                          31|  697|\n",
      "|                          29|  686|\n",
      "|                          33|  679|\n",
      "|                          40|  672|\n",
      "|                          32|  668|\n",
      "|                          34|  648|\n",
      "|                          36|  644|\n",
      "|                          39|  637|\n",
      "|                          41|  624|\n",
      "|                          44|  623|\n",
      "|                          45|  621|\n",
      "|                          35|  616|\n",
      "|                          42|  611|\n",
      "|                          43|  605|\n",
      "|                          37|  592|\n",
      "|                          48|  583|\n",
      "|                          47|  572|\n",
      "|                          46|  566|\n",
      "|                        null|  514|\n",
      "|                          49|  514|\n",
      "|                           4|  513|\n",
      "|                           3|  445|\n",
      "|                          53|  444|\n",
      "|                          61|  444|\n",
      "|                          51|  437|\n",
      "|                          59|  427|\n",
      "|                          54|  419|\n",
      "|                           2|  418|\n",
      "|                          60|  417|\n",
      "|                          68|  415|\n",
      "|                          57|  414|\n",
      "|                          63|  406|\n",
      "|                          58|  405|\n",
      "|                          52|  403|\n",
      "|                          55|  402|\n",
      "|                          69|  401|\n",
      "|                          50|  400|\n",
      "|                          56|  391|\n",
      "|                          65|  389|\n",
      "|                          62|  389|\n",
      "|                          64|  388|\n",
      "|                          66|  381|\n",
      "|                          73|  378|\n",
      "|                          71|  377|\n",
      "|                          67|  365|\n",
      "|                          72|  363|\n",
      "|                          74|  359|\n",
      "|                          70|  353|\n",
      "|                          75|  343|\n",
      "|                          78|  342|\n",
      "|                          77|  331|\n",
      "|                          76|  318|\n",
      "|                          80|  309|\n",
      "|                          79|  297|\n",
      "|                           1|  289|\n",
      "|                          81|  287|\n",
      "|                           0|  216|\n",
      "|                          82|  110|\n",
      "|                          83|   17|\n",
      "|                          85|   11|\n",
      "|                          84|    6|\n",
      "|                          87|    4|\n",
      "|                          97|    3|\n",
      "|                         120|    2|\n",
      "|                          96|    2|\n",
      "|                         108|    2|\n",
      "|                         176|    2|\n",
      "|                          88|    2|\n",
      "|                          89|    2|\n",
      "|                          94|    2|\n",
      "|                          86|    2|\n",
      "|                          92|    2|\n",
      "|                          91|    2|\n",
      "|                         139|    1|\n",
      "|                         110|    1|\n",
      "|                         107|    1|\n",
      "|                         100|    1|\n",
      "|                         130|    1|\n",
      "|                         118|    1|\n",
      "|                         104|    1|\n",
      "|                         115|    1|\n",
      "|                         131|    1|\n",
      "|                         143|    1|\n",
      "|                          90|    1|\n",
      "|                         129|    1|\n",
      "|                          93|    1|\n",
      "|                         114|    1|\n",
      "|                         141|    1|\n",
      "|                         106|    1|\n",
      "|                         148|    1|\n",
      "|                         152|    1|\n",
      "+----------------------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|Bankruptcies|count|\n",
      "+------------+-----+\n",
      "|           0|88774|\n",
      "|           1|10475|\n",
      "|        null|  514|\n",
      "|           2|  417|\n",
      "|          NA|  204|\n",
      "|           3|   93|\n",
      "|           4|   27|\n",
      "|           5|    7|\n",
      "|           6|    2|\n",
      "|           7|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|Tax_Liens|count|\n",
      "+---------+-----+\n",
      "|        0|98062|\n",
      "|        1| 1343|\n",
      "|     null|  514|\n",
      "|        2|  374|\n",
      "|        3|  111|\n",
      "|        4|   58|\n",
      "|        5|   16|\n",
      "|        6|   12|\n",
      "|       NA|   10|\n",
      "|        7|    7|\n",
      "|        9|    3|\n",
      "|       11|    2|\n",
      "|       15|    1|\n",
      "|       10|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dei valori distinti degli attributi con data type incoerente\n",
    "\n",
    "problematic_columns = ['Years_in_current_job', 'Months_since_last_delinquent', 'Bankruptcies', 'Tax_Liens']\n",
    "\n",
    "for col in problematic_columns:\n",
    "    sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Years_in_current_job'\n",
    "\n",
    "rendiamo numerico quest'attributo in questo modo:\n",
    "\n",
    "years viene tolto da ogni valore.\n",
    "\n",
    "sono presenti 4222 valori uguali a 'n/a'. Pensiamo si possa trattare di soggetti per cui non si può dire qunati anni hanno lavorato nel lavoro corrente perché sono attualmente senza occupazione. Il valore viene quindi sostituito con 0.\n",
    "\n",
    "10+ viene trasformato in 10 perché 10+ non ha valenza semantica non conoscendo la distribuzione dei valori specifici per questa categoria. Questa trasformazione non comporta problematiche per algoritmi di machine learning come il Decision Tree perché l'ordinamento dei valori numerici è preservato (basterà tenere presente che un eventuale split sul valore 10 sarebbe in realtà riferito a valori anche maggiori di 10). L'unica problematica apparente potrebbe manifestarsi per algoritmi basati sulle distanze (es. Clustering, PCA, KNN ecc...) perché la distanza dal valore 10 potrebbe in realtà essere una distanza molto maggiore. Ma non ci preoccupiamo di questo perché le ennuple coinvolte sono relativamente poche e l'errore non avrebbe un impatto rilevante sul calcolo della distanza multidimensionale. E anche soprattutto perché arrivati a 10 anni di lavoro in una posizione si ritiene che il fattore lavorativo non sia ormai più un problema e quindi un'ipotetica \"distanza\" ad esempio tra 10 e 25 anni è tutto sommato meno rilevante di una distanza, anche minore, ma ad esempio tra 10 e 1.\n",
    "\n",
    "< 1 viene trasformato nella media dei valori tra 0 e 1, cioè 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Years_in_current_job'] is not None:\n",
    "        d['Years_in_current_job'] = str(d['Years_in_current_job']).replace(' years', '')\n",
    "        d['Years_in_current_job'] = str(d['Years_in_current_job']).replace(' year', '')\n",
    "        \n",
    "        if d['Years_in_current_job'] == 'n/a':\n",
    "            d['Years_in_current_job'] = 0\n",
    "        if d['Years_in_current_job'] == '10+':\n",
    "            d['Years_in_current_job'] = 10\n",
    "        if d['Years_in_current_job'] == '< 1':\n",
    "            d['Years_in_current_job'] = 0.5\n",
    "        \n",
    "        d['Years_in_current_job'] = float(d['Years_in_current_job'])\n",
    "    \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Months_since_last_delinquent'\n",
    "\n",
    "sono presenti 53141 valori uguali a 'NA' (la maggior parte). la nostra interpretazione di questo valore è che il soggetto in questione non ha mai commesso nessun reato (interpretazione coerente con il fatto che i valori 'NA' sono la maggior parte).\n",
    "\n",
    "il conteggio degli altri valori distinti è basso, decidiamo quindi di rendere l'attributo categorico raccogliendo i valori in questi range:\n",
    "\n",
    "[0, 12); [12, 48); [48, 96); [96, +inf); 'Never committed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Months_since_last_delinquent'] is not None:\n",
    "        if d['Months_since_last_delinquent'] == 'NA':\n",
    "            d['Months_since_last_delinquent'] = 'Never committed'\n",
    "        elif int(d['Months_since_last_delinquent']) < 12:\n",
    "            d['Months_since_last_delinquent'] = '[0, 12)'\n",
    "        elif int(d['Months_since_last_delinquent']) < 48:\n",
    "            d['Months_since_last_delinquent'] = '[12, 48)'\n",
    "        elif int(d['Months_since_last_delinquent']) < 96:\n",
    "            d['Months_since_last_delinquent'] = '[48, 96)'\n",
    "        else:\n",
    "            d['Months_since_last_delinquent'] = '[96, +inf)'\n",
    "            \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Bankruptcies'\n",
    "\n",
    "l'attributo viene letto come stringa perché sono presenti 204 valori uguali a 'NA'. Poichè il valore 0 è presente si pensa possa trattarsi di missing values, li trasformiamo quindi in None e rendiamo numerico l'attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Bankruptcies'] is not None:\n",
    "        if d['Bankruptcies'] == 'NA':\n",
    "            d['Bankruptcies'] = None\n",
    "        else:\n",
    "            d['Bankruptcies'] = int(d['Bankruptcies'])\n",
    "        \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Tax_Liens'\n",
    "\n",
    "l'attributo viene letto come stringa perché sono presenti 10 valori uguali a 'NA'. Poichè il valore 0 è presente si pensa possa trattarsi di missing values, li trasformiamo quindi in None e rendiamo numerico l'attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "\n",
    "    if d['Tax_Liens'] is not None:\n",
    "        if d['Tax_Liens'] == 'NA':\n",
    "            d['Tax_Liens'] = None\n",
    "        else:\n",
    "            d['Tax_Liens'] = int(d['Tax_Liens'])\n",
    "        \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = rdd.toDF()\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|Years_in_current_job|count|\n",
      "+--------------------+-----+\n",
      "|                10.0|31121|\n",
      "|                 2.0| 9134|\n",
      "|                 3.0| 8169|\n",
      "|                 0.5| 8164|\n",
      "|                 5.0| 6787|\n",
      "|                 1.0| 6460|\n",
      "|                 4.0| 6143|\n",
      "|                 6.0| 5686|\n",
      "|                 7.0| 5577|\n",
      "|                 8.0| 4582|\n",
      "|                 0.0| 4222|\n",
      "|                 9.0| 3955|\n",
      "|                null|  514|\n",
      "+--------------------+-----+\n",
      "\n",
      "+----------------------------+-----+\n",
      "|Months_since_last_delinquent|count|\n",
      "+----------------------------+-----+\n",
      "|             Never committed|53141|\n",
      "|                    [12, 48)|25789|\n",
      "|                    [48, 96)|13453|\n",
      "|                     [0, 12)| 7590|\n",
      "|                        null|  514|\n",
      "|                  [96, +inf)|   27|\n",
      "+----------------------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|Bankruptcies|count|\n",
      "+------------+-----+\n",
      "|           0|88774|\n",
      "|           1|10475|\n",
      "|        null|  718|\n",
      "|           2|  417|\n",
      "|           3|   93|\n",
      "|           4|   27|\n",
      "|           5|    7|\n",
      "|           6|    2|\n",
      "|           7|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|Tax_Liens|count|\n",
      "+---------+-----+\n",
      "|        0|98062|\n",
      "|        1| 1343|\n",
      "|     null|  524|\n",
      "|        2|  374|\n",
      "|        3|  111|\n",
      "|        4|   58|\n",
      "|        5|   16|\n",
      "|        6|   12|\n",
      "|        7|    7|\n",
      "|        9|    3|\n",
      "|       11|    2|\n",
      "|       10|    1|\n",
      "|       15|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in problematic_columns:\n",
    "    sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestione degli errori\n",
    "\n",
    "alcuni attributi hanno valori errati o non coerenti con il significato che noi reputiamo possa avere l'attributo. Alcuni di questi errori sono stati scoperti in fasi più avanzate del progetto (ex Data Understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Credit_Score'\n",
    "\n",
    "ci sono 4551 valori dell'attributo 'Credit_Score' con valore superiore a 5000, che è un valore troppo distante rispetto a quelli generici che assume questo attributo (da 500 a 800). Controllando i valori distinti si è scoperto che quei valori sono errati: c'è uno '0' di troppo in fondo al numero, che eliminamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4551"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def problematic_values_count(row):\n",
    "    if row['Credit_Score'] is not None:\n",
    "        return row['Credit_Score'] > 5000\n",
    "\n",
    "rdd.filter(problematic_values_count).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Credit_Score|count|\n",
      "+------------+-----+\n",
      "|        null|19668|\n",
      "|         747| 1825|\n",
      "|         740| 1746|\n",
      "|         746| 1742|\n",
      "|         741| 1732|\n",
      "|         742| 1723|\n",
      "|         739| 1624|\n",
      "|         745| 1612|\n",
      "|         748| 1598|\n",
      "|         743| 1555|\n",
      "|         725| 1548|\n",
      "|         724| 1522|\n",
      "|         738| 1495|\n",
      "|         744| 1485|\n",
      "|         721| 1465|\n",
      "|         723| 1421|\n",
      "|         737| 1405|\n",
      "|         722| 1387|\n",
      "|         718| 1261|\n",
      "|         750| 1234|\n",
      "|         717| 1218|\n",
      "|         720| 1216|\n",
      "|         736| 1156|\n",
      "|         734| 1147|\n",
      "|         735| 1134|\n",
      "|         719| 1118|\n",
      "|         732| 1084|\n",
      "|         733| 1073|\n",
      "|         715| 1070|\n",
      "|         716| 1061|\n",
      "|         714| 1046|\n",
      "|         713| 1017|\n",
      "|         731| 1010|\n",
      "|         712| 1006|\n",
      "|         730|  984|\n",
      "|         728|  931|\n",
      "|         729|  925|\n",
      "|         708|  902|\n",
      "|         709|  865|\n",
      "|         710|  857|\n",
      "|         726|  853|\n",
      "|         749|  827|\n",
      "|         707|  814|\n",
      "|         727|  806|\n",
      "|         711|  791|\n",
      "|         706|  791|\n",
      "|         705|  727|\n",
      "|         751|  723|\n",
      "|         703|  717|\n",
      "|         704|  716|\n",
      "|         699|  660|\n",
      "|         702|  644|\n",
      "|         701|  620|\n",
      "|         700|  612|\n",
      "|         698|  595|\n",
      "|         697|  552|\n",
      "|         693|  509|\n",
      "|         695|  506|\n",
      "|         694|  506|\n",
      "|         692|  504|\n",
      "|         696|  483|\n",
      "|         691|  386|\n",
      "|         689|  384|\n",
      "|         686|  369|\n",
      "|         685|  369|\n",
      "|         687|  367|\n",
      "|         683|  365|\n",
      "|         680|  346|\n",
      "|         688|  338|\n",
      "|         684|  327|\n",
      "|         690|  320|\n",
      "|         678|  298|\n",
      "|         682|  294|\n",
      "|         679|  285|\n",
      "|         676|  284|\n",
      "|         681|  277|\n",
      "|         674|  262|\n",
      "|         675|  262|\n",
      "|         677|  234|\n",
      "|         669|  230|\n",
      "|         668|  227|\n",
      "|         670|  227|\n",
      "|         672|  219|\n",
      "|         673|  212|\n",
      "|         671|  208|\n",
      "|         663|  182|\n",
      "|         667|  176|\n",
      "|         664|  173|\n",
      "|         656|  171|\n",
      "|         661|  168|\n",
      "|         666|  167|\n",
      "|         665|  161|\n",
      "|         659|  148|\n",
      "|         655|  147|\n",
      "|         660|  144|\n",
      "|         662|  137|\n",
      "|         654|  129|\n",
      "|         649|  126|\n",
      "|         657|  123|\n",
      "|         652|  122|\n",
      "|         658|  118|\n",
      "|         653|  118|\n",
      "|        7390|  108|\n",
      "|        7370|  106|\n",
      "|         651|  101|\n",
      "|         645|   99|\n",
      "|         650|   98|\n",
      "|        7400|   97|\n",
      "|        7380|   93|\n",
      "|        7330|   88|\n",
      "|         640|   88|\n",
      "|        7340|   87|\n",
      "|        7410|   87|\n",
      "|         643|   87|\n",
      "|        7360|   85|\n",
      "|        7420|   84|\n",
      "|        7240|   84|\n",
      "|        7320|   83|\n",
      "|         642|   81|\n",
      "|        7280|   77|\n",
      "|         641|   77|\n",
      "|        7350|   76|\n",
      "|         648|   76|\n",
      "|        7460|   76|\n",
      "|         646|   75|\n",
      "|        7250|   75|\n",
      "|        7270|   74|\n",
      "|        7170|   74|\n",
      "|        7310|   73|\n",
      "|        7300|   73|\n",
      "|         644|   73|\n",
      "|         635|   72|\n",
      "|         638|   71|\n",
      "|         647|   71|\n",
      "|        7430|   70|\n",
      "|        7190|   68|\n",
      "|        7160|   68|\n",
      "|         632|   67|\n",
      "|        7260|   64|\n",
      "|        7230|   63|\n",
      "|        7200|   63|\n",
      "|         636|   62|\n",
      "|        7140|   62|\n",
      "|        7290|   60|\n",
      "|        7220|   59|\n",
      "|        7440|   58|\n",
      "|        7090|   58|\n",
      "|         633|   58|\n",
      "|        7210|   57|\n",
      "|        7180|   57|\n",
      "|        7450|   55|\n",
      "|        7100|   54|\n",
      "|        7130|   53|\n",
      "|        7070|   52|\n",
      "|         624|   52|\n",
      "|        7470|   51|\n",
      "|        7120|   51|\n",
      "|         622|   50|\n",
      "|        7080|   50|\n",
      "|        7040|   49|\n",
      "|         637|   47|\n",
      "|        6990|   46|\n",
      "|        7010|   46|\n",
      "|        7150|   45|\n",
      "|        7060|   45|\n",
      "|        7110|   45|\n",
      "|         634|   45|\n",
      "|         628|   44|\n",
      "|        7050|   44|\n",
      "|         627|   43|\n",
      "|        7480|   43|\n",
      "|         639|   42|\n",
      "|         619|   42|\n",
      "|         621|   41|\n",
      "|        6950|   39|\n",
      "|         626|   39|\n",
      "|        7030|   39|\n",
      "|        6960|   38|\n",
      "|        6940|   38|\n",
      "|        7020|   37|\n",
      "|        6980|   36|\n",
      "|         625|   36|\n",
      "|        6930|   36|\n",
      "|        6840|   35|\n",
      "|        7000|   35|\n",
      "|         630|   35|\n",
      "|         616|   34|\n",
      "|         614|   33|\n",
      "|         631|   33|\n",
      "|         618|   32|\n",
      "|        6970|   32|\n",
      "|         612|   32|\n",
      "|         617|   31|\n",
      "|        6810|   30|\n",
      "|         615|   30|\n",
      "|         629|   30|\n",
      "|         601|   29|\n",
      "|         611|   29|\n",
      "|        6920|   29|\n",
      "|        6890|   29|\n",
      "|        6860|   28|\n",
      "|         610|   28|\n",
      "|         623|   28|\n",
      "|        6870|   27|\n",
      "|         620|   26|\n",
      "|         609|   26|\n",
      "|        6800|   25|\n",
      "|        6850|   25|\n",
      "|        6650|   25|\n",
      "|        6900|   25|\n",
      "|         600|   25|\n",
      "|        6740|   24|\n",
      "|        7500|   24|\n",
      "|        6880|   24|\n",
      "|        6830|   24|\n",
      "|         613|   23|\n",
      "|        7490|   23|\n",
      "|         597|   23|\n",
      "|         605|   23|\n",
      "|        6910|   22|\n",
      "|        6750|   22|\n",
      "|        6820|   21|\n",
      "|        6630|   21|\n",
      "|         606|   20|\n",
      "|         588|   20|\n",
      "|        6780|   20|\n",
      "|         595|   19|\n",
      "|         603|   19|\n",
      "|         608|   19|\n",
      "|        6660|   19|\n",
      "|        6790|   18|\n",
      "|        6670|   17|\n",
      "|        6700|   17|\n",
      "|        6720|   17|\n",
      "|        6680|   17|\n",
      "|        6730|   16|\n",
      "|        6690|   16|\n",
      "|         602|   16|\n",
      "|        6770|   16|\n",
      "|         599|   15|\n",
      "|        6520|   15|\n",
      "|         596|   15|\n",
      "|        6710|   14|\n",
      "|        6450|   13|\n",
      "|         604|   13|\n",
      "|        6500|   13|\n",
      "|        6580|   13|\n",
      "|        6610|   13|\n",
      "|        6620|   13|\n",
      "|         607|   13|\n",
      "|        6530|   12|\n",
      "|        6590|   12|\n",
      "|         585|   12|\n",
      "|        6760|   12|\n",
      "|        6560|   12|\n",
      "|        6410|   12|\n",
      "|         587|   11|\n",
      "|        6600|   11|\n",
      "|        6570|   11|\n",
      "|         594|   10|\n",
      "|        6640|    9|\n",
      "|        6400|    9|\n",
      "|         598|    9|\n",
      "|        7510|    9|\n",
      "|         591|    9|\n",
      "|        6510|    9|\n",
      "|        6380|    9|\n",
      "|        6550|    8|\n",
      "|        6240|    8|\n",
      "|         590|    8|\n",
      "|         593|    7|\n",
      "|        6390|    7|\n",
      "|         586|    7|\n",
      "|        6260|    7|\n",
      "|        6440|    7|\n",
      "|        6540|    7|\n",
      "|        6220|    7|\n",
      "|        6320|    7|\n",
      "|        6490|    7|\n",
      "|        6470|    6|\n",
      "|         589|    6|\n",
      "|        6230|    6|\n",
      "|        6160|    6|\n",
      "|        6300|    6|\n",
      "|        6330|    6|\n",
      "|        6480|    6|\n",
      "|        6420|    6|\n",
      "|        6350|    6|\n",
      "|        6290|    5|\n",
      "|        6180|    5|\n",
      "|        6340|    4|\n",
      "|         592|    4|\n",
      "|        6090|    4|\n",
      "|        6250|    4|\n",
      "|        6430|    4|\n",
      "|        6170|    4|\n",
      "|        6270|    4|\n",
      "|        6360|    4|\n",
      "|        6310|    4|\n",
      "|        6110|    4|\n",
      "|        6130|    3|\n",
      "|        6280|    3|\n",
      "|        6210|    3|\n",
      "|        5940|    3|\n",
      "|        5950|    3|\n",
      "|        6150|    3|\n",
      "|        6460|    3|\n",
      "|        6200|    2|\n",
      "|        6190|    2|\n",
      "|        5850|    2|\n",
      "|        6100|    2|\n",
      "|        6140|    2|\n",
      "|        6120|    2|\n",
      "|        6080|    2|\n",
      "|        6060|    2|\n",
      "|        5930|    2|\n",
      "|        6370|    2|\n",
      "|        6070|    1|\n",
      "|        6050|    1|\n",
      "|        5860|    1|\n",
      "|        5890|    1|\n",
      "|        6010|    1|\n",
      "|        5960|    1|\n",
      "|        5900|    1|\n",
      "|        5920|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = 'Credit_Score'\n",
    "sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100514"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def errors_correction(row):\n",
    "    d = row.asDict()\n",
    "    if d['Credit_Score'] is not None and d['Credit_Score'] > 5000:\n",
    "        d['Credit_Score'] = int(str(d['Credit_Score'])[:-1])\n",
    "    return Row(**d)\n",
    "    \n",
    "rdd = rdd.map(errors_correction)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono diversi valori dell'attributo 'Current_Loan_Amount' che sono pari a 1.428.571 (99.999.999 in Rubli).  un numero eccessivamente più alto rispetto a tutti gli altri valori e che per questo viene eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|Current_Loan_Amount|count|\n",
      "+-------------------+-----+\n",
      "|            1428571|11484|\n",
      "|               null|  514|\n",
      "|               3209|   70|\n",
      "|               3094|   68|\n",
      "|               3088|   67|\n",
      "|               3099|   67|\n",
      "|               3077|   66|\n",
      "|               3138|   64|\n",
      "|               3187|   62|\n",
      "|               1576|   62|\n",
      "|               3176|   62|\n",
      "|               3110|   61|\n",
      "|               3066|   61|\n",
      "|               3195|   61|\n",
      "|               3192|   59|\n",
      "|               3132|   58|\n",
      "|               3089|   57|\n",
      "|               1845|   57|\n",
      "|               3185|   56|\n",
      "|               3182|   56|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('Current_Loan_Amount').groupBy('Current_Loan_Amount').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89030"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def errors_correction(row):\n",
    "    if row['Current_Loan_Amount'] is not None:\n",
    "        return row['Current_Loan_Amount'] != 1428571\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(errors_correction)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eliminiamo le righe dove entrambi gli attributi 'Loan_ID' e 'Customer_ID' sono nulli, che corrispondono alle righe dove tutti i valori di tutti gli attributi sono nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88516"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.filter(lambda row: not ( (row['Loan_ID'] is None) and (row['Customer_ID'] is None) ))\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eliminiamo le righe duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78301"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.distinct()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema: ci sono coppie di righe con tutti i valori duplicati eccetto per le due colonne 'Credit_Score' e 'Annual_Income', per le quali uno dei due valori è presente e l'altro è nullo.\n",
    "\n",
    "Soluzione: si raggruppa per tutti gli attributi tranne quei due e poi si calcola la media di quei due. In questo modo se le uniche due righe uguali sono quelle con un valore nullo e uno non nullo per quegli attributi, lo media sarà uguale al valore non nullo; se invece ci fossero altre righe ugauli ma con altri valori diversi non nulli per quegli attributi, viene effettivamente calcolata la media, il che è auspicabile considerando che tutto il resto della riga è uguale e quindi si tratta molto probabilmente dello stesso oggetto, duplicato per errore, di cui dunque prendiamo un valore medio tra quelli presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rdd.toDF()\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "columns_temp = [col for col in columns if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}, BIGINT(AVG(Credit_Score)) AS Credit_Score, BIGINT(AVG(Annual_Income)) AS Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY {0}\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "come si nota i Customer e Loan ID che si ripetevano nel dataset originale erano solo righe duplicate. Il dataset pulito non presenta nessuna riga uguale negli ID e possiamo quindi eliminarli. Anche escludendo questi attributi tutte le righe rimangono distinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+\n",
      "|nbr_rows|nbr_customers|nbr_loans|\n",
      "+--------+-------------+---------+\n",
      "|   74094|        74094|    74094|\n",
      "+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows, COUNT(DISTINCT Customer_ID) AS nbr_customers, COUNT(DISTINCT Loan_ID) AS nbr_loans\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in columns if col != 'Customer_ID' and col != 'Loan_ID']\n",
    "\n",
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]\n",
    "\n",
    "sdf = sdf.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestione dei Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14947\n",
      "Annual_Income:      14947\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          2\n",
      "Bankruptcies:        156\n",
      "Tax_Liens:          7\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filliamo i Missing values degli attributi \"Maximum_Open_Credit\", \"Bankruptcies\" e \"Tax_Liens\" usando la Mediana di diversi tipi di raggruppamenti. Per questi attributi è stata fatta questa scelta perché presentano un numero relativamente basso di Missing Values.\n",
    "\n",
    "per farlo usiamo la sintassi dell'SQL analitico per generare le nuove colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    CASE WHEN Maximum_Open_Credit IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Maximum_Open_Credit, 0.5) OVER(PARTITION BY Years_in_current_job, Home_Ownership, Number_of_Open_Accounts, Years_of_Credit_History) )\n",
    "    ELSE Maximum_Open_Credit END AS Maximum_Open_Credit_new,\n",
    "    \n",
    "    CASE WHEN Bankruptcies IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Bankruptcies, 0.5) OVER(PARTITION BY Months_since_last_delinquent, Number_of_Credit_Problems) )\n",
    "    ELSE Bankruptcies END AS Bankruptcies_new,\n",
    "    \n",
    "    CASE WHEN Tax_Liens IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Tax_Liens, 0.5) OVER(PARTITION BY Months_since_last_delinquent, Number_of_Credit_Problems) )\n",
    "    ELSE Tax_Liens END AS Tax_Liens_new\n",
    "    \n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "sdf = sdf.drop('Maximum_Open_Credit').withColumnRenamed(\"Maximum_Open_Credit_new\", \"Maximum_Open_Credit\")\n",
    "sdf = sdf.drop('Bankruptcies').withColumnRenamed(\"Bankruptcies_new\", \"Bankruptcies\")\n",
    "sdf = sdf.drop('Tax_Liens').withColumnRenamed(\"Tax_Liens_new\", \"Tax_Liens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    BIGINT( PERCENTILE(Maximum_Open_Credit, 0.5) OVER(PARTITION BY Years_in_current_job,\n",
    "                                                Home_Ownership,\n",
    "                                                Number_of_Open_Accounts,\n",
    "                                                Years_of_Credit_History) ) AS toFill_Maximum_Open_Credit,\n",
    "    BIGINT( PERCENTILE(Bankruptcies, 0.5) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                                Number_of_Credit_Problems) ) AS toFill_Bankruptcies,\n",
    "    BIGINT( PERCENTILE(Tax_Liens, 0.5) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                            Number_of_Credit_Problems) ) AS toFill_Tax_Liens\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Maximum_Open_Credit'] is None:\n",
    "        d['Maximum_Open_Credit'] = d['toFill_Maximum_Open_Credit']\n",
    "    if d['Bankruptcies'] is None:\n",
    "        d['Bankruptcies'] = d['toFill_Bankruptcies']\n",
    "    if d['Tax_Liens'] is None:\n",
    "        d['Tax_Liens'] = d['toFill_Tax_Liens']\n",
    "    return Row(**d)\n",
    "\n",
    "sdf = sdf.rdd.map(fill_nulls).toDF().select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14947\n",
      "Annual_Income:      14947\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df=sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gli attributi \"Credit_Score\" e \"Annual_Income\" hanno invece un numero rilevante di Missing Values. Filliamoli quindi dividendo il dataset in clusters e usando la Mediana dei valori in ciascun cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Eseguiamo il Clustering con la libreria ML. Con MLlib non è possibile iterare con più valori per il parametro k e non è possibile calcolare l'SSE per diversi tipi di modelli.\n",
    "\n",
    "Proviamo 3 diversi tipi di clustering per diverse iterazioni con un k randomico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current_Loan_Amount',\n",
       " 'Years_in_current_job',\n",
       " 'Monthly_Debt',\n",
       " 'Years_of_Credit_History',\n",
       " 'Number_of_Open_Accounts',\n",
       " 'Number_of_Credit_Problems',\n",
       " 'Current_Credit_Balance',\n",
       " 'Maximum_Open_Credit',\n",
       " 'Bankruptcies',\n",
       " 'Tax_Liens']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per effettuare il clustering dobbiamo considerare solo le colonne numeriche diverse da 'Credit_Score' e 'Annual_Income'.\n",
    "\n",
    "columns_clustering = [col for col in columns_numerical if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "\n",
    "columns_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_clustering(spark_df, columns_clustering, models=['kmeans', 'biskmeans', 'gaussmix'], k_clusters_iter=[2, 10]):\n",
    "    \n",
    "    \"\"\"funzione che prende in input tutto il dataframe, specificando le colonne su cui si vuole eseguire il clustering.\n",
    "    standardizza i dati rendendo ciascuna colonna a media 0 e varianza unitaria.\n",
    "    esegue fino a 3 tipi diversi di clustering, iterando inoltre per un numero di clusters specificato.\n",
    "    restituisce dataframe, modello e Silhouette del clustering migliore in termini di Silhouette.\"\"\"\n",
    "    \n",
    "    columns = spark_df.schema.names\n",
    "    \n",
    "    sdf = VectorAssembler(inputCols=columns_clustering, outputCol=\"columns_clustering\").transform(spark_df)\n",
    "    scaler = StandardScaler(inputCol=\"columns_clustering\", outputCol=\"scaled_columns_clustering\", withStd=True, withMean=True)\n",
    "    sdf = scaler.fit(sdf).transform(sdf).select(columns + ['scaled_columns_clustering'])\n",
    "    \n",
    "    best_silhouette = -1\n",
    "    \n",
    "    for k_clusters in k_clusters_iter:\n",
    "        \n",
    "        if 'kmeans' in models:\n",
    "            model = KMeans(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                           maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "                \n",
    "        if 'biskmeans' in models:\n",
    "            model = BisectingKMeans(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                                    maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "                  \n",
    "        if 'gaussmix' in models:\n",
    "            model = GaussianMixture(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                                    maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=\"clusters_label\",\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "    \n",
    "    sdf_clustered = sdf_clustered.select(columns + ['clusters_label'])\n",
    "    \n",
    "    return sdf_clustered, nbr_clusters, best_model, best_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_clusters_list = [k for k in [2, 3, 4, 5, 8, 10] + list(sample(range(11, 30), size=5, replace=False))]\n",
    "\n",
    "k_clusters_list = [2]\n",
    "\n",
    "################################################################### NON Runnare\n",
    "\n",
    "sdf, nbr_clusters, clustering_model, silhouette_score = best_clustering(spark_df=sdf,\n",
    "                                                                        columns_clustering=columns_clustering,\n",
    "                                                                        k_clusters_iter=k_clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture_c5ad9924221d"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbr_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067610880648694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il migior modello di Clustering è un Gaussian Mixture Model con k=2, cioè un modello che trova due distribuzioni Normali nel dataset che massimizzano la Log-Likelihood.\n",
    "\n",
    "Usiamo i clusters generati da questo modello per fillare i Missing Values mancanti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** sei qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Credit_Score`' given input columns: []; line 3 pos 14;\\n'Project [CASE WHEN isnull('Credit_Score) THEN 'BIGINT('PERCENTILE('Credit_Score, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Credit_Score END AS Credit_Score_new#19578, CASE WHEN isnull('Annual_Income) THEN 'BIGINT('PERCENTILE('Annual_Income, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Annual_Income END AS Annual_Income_new#19579, 'FROM AS Bank_Loan_Dataset#19580]\\n+- OneRowRelation\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o29.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Credit_Score`' given input columns: []; line 3 pos 14;\n'Project [CASE WHEN isnull('Credit_Score) THEN 'BIGINT('PERCENTILE('Credit_Score, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Credit_Score END AS Credit_Score_new#19578, CASE WHEN isnull('Annual_Income) THEN 'BIGINT('PERCENTILE('Annual_Income, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Annual_Income END AS Annual_Income_new#19579, 'FROM AS Bank_Loan_Dataset#19580]\n+- OneRowRelation\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-80732a38d9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Credit_Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Credit_Score_new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Credit_Score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Credit_Score`' given input columns: []; line 3 pos 14;\\n'Project [CASE WHEN isnull('Credit_Score) THEN 'BIGINT('PERCENTILE('Credit_Score, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Credit_Score END AS Credit_Score_new#19578, CASE WHEN isnull('Annual_Income) THEN 'BIGINT('PERCENTILE('Annual_Income, 0.5) windowspecdefinition('cluster_label, 'Years_in_current_job, 'Home_Ownership, 'Number_of_Open_Accounts, 'Years_of_Credit_History, unspecifiedframe$())) ELSE 'Annual_Income END AS Annual_Income_new#19579, 'FROM AS Bank_Loan_Dataset#19580]\\n+- OneRowRelation\\n\""
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    CASE WHEN Credit_Score IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Credit_Score, 0.5) OVER(PARTITION BY cluster_label, Years_in_current_job, Home_Ownership, Number_of_Open_Accounts, Years_of_Credit_History) )\n",
    "    ELSE Credit_Score END AS Credit_Score_new,\n",
    "    \n",
    "    CASE WHEN Annual_Income IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Annual_Income, 0.5) OVER(PARTITION BY cluster_label, Years_in_current_job, Home_Ownership, Number_of_Open_Accounts, Years_of_Credit_History) )\n",
    "    ELSE Annual_Income END AS Annual_Income_new,\n",
    "    \n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "sdf = sdf.drop('Credit_Score').withColumnRenamed(\"Credit_Score_new\", \"Credit_Score\")\n",
    "sdf = sdf.drop('Annual_Income').withColumnRenamed(\"Annual_Income_new\", \"Annual_Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14947\n",
      "Annual_Income:      14947\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n",
      "clusters_label:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New variables creation - Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND((Monthly_Debt/Current_Loan_Amount)*100, 4) AS Installment_Rate\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si è deciso di creare una nuova variabile \"(Monthly_Debt*12)/Annual_Income \" per comprendere quanto del reddito annuale percepiuto viene utilizzato per pagare il debito creato verso la banca poichè può influenzare il Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND(((Monthly_Debt*12)/Annual_Income)*100, 4) AS Debt_Income_Rate\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number_of_Credit_Problems/Number_of_Open_Accounts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND((Number_of_Credit_Problems/Number_of_Open_Accounts)*100, 4) AS Credit_Problems_Perc\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- clusters_label: integer (nullable = false)\n",
      " |-- Installment_Rate: double (nullable = true)\n",
      " |-- Debt_Income_Rate: double (nullable = true)\n",
      " |-- Credit_Problems_Perc: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la nuova variabile Installment_Rate è coerente, infatti il suo valore medio nel dataset (il tasso medio della rata del prestito) coincide con il tasso governativo decennale Russo del 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------+------+--------+-------+\n",
      "|avg(Installment_Rate)|DistinctVal|MinVal|  MaxVal| MEDIAN|\n",
      "+---------------------+-----------+------+--------+-------+\n",
      "|     7.98956803249926|      55721|   0.0|282.5451|5.74305|\n",
      "+---------------------+-----------+------+--------+-------+\n",
      "\n",
      "+---------------------+-----------+------+-------+-------+\n",
      "|avg(Debt_Income_Rate)|DistinctVal|MinVal| MaxVal| MEDIAN|\n",
      "+---------------------+-----------+------+-------+-------+\n",
      "|   17.313526831453842|      14531|   0.0|40.0007|16.9998|\n",
      "+---------------------+-----------+------+-------+-------+\n",
      "\n",
      "+-------------------------+-----------+------+------+------+\n",
      "|avg(Credit_Problems_Perc)|DistinctVal|MinVal|MaxVal|MEDIAN|\n",
      "+-------------------------+-----------+------+------+------+\n",
      "|       1.7908512295524486|        104|   0.0| 300.0|   0.0|\n",
      "+-------------------------+-----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_columns=['Installment_Rate', 'Debt_Income_Rate' , 'Credit_Problems_Perc']\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "for col in new_columns:\n",
    "    sql = \"\"\"\n",
    "    SELECT  AVG({0}), COUNT(DISTINCT {0}) AS DistinctVal, MIN(CAST({0} AS DOUBLE)) AS MinVal, MAX(CAST({0} AS DOUBLE)) AS MaxVal, PERCENTILE(CAST({0} AS DOUBLE),0.5) AS MEDIAN\n",
    "    FROM Bank_Loan_Dataset\n",
    "    \"\"\".format(col)\n",
    "\n",
    "    spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo le correlazione tra le nuove variabili e quelle da cui hanno origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.32990332, -0.38271919],\n",
       "       [ 0.32990332,  1.        ,  0.43415914],\n",
       "       [-0.38271919,  0.43415914,  1.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Installment_Rate', 'Monthly_Debt', 'Current_Loan_Amount'])\\\n",
    "                .rdd.map(lambda row: Vectors_mllib.dense(row)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan, nan],\n",
       "       [nan,  1., nan],\n",
       "       [nan, nan,  1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Debt_Income_Rate', 'Monthly_Debt','Annual_Income'])\\\n",
    "                .rdd.map(lambda row: Vectors_mllib.dense(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,         nan,         nan],\n",
       "       [        nan,  1.        , -0.01296336],\n",
       "       [        nan, -0.01296336,  1.        ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Credit_Problems_Perc', 'Number_of_Credit_Problems', 'Number_of_Open_Accounts'])\\\n",
    "                .rdd.map(lambda row: Vectors_mllib.dense(row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check finale ed Export nell'HDFS del Dataset definitivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = sdf.schema.names\n",
    "\n",
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il dataset poi viene letto tramite:\n",
    "# sdf = spark.read.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|nbr_rows|\n",
      "+--------+\n",
      "|   74094|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Loan_Status|nbr_rows|\n",
      "+-----------+--------+\n",
      "| Fully Paid|   51455|\n",
      "|Charged Off|   22639|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Loan_Status| avg_Credit_Score|\n",
      "+-----------+-----------------+\n",
      "| Fully Paid|719.9762023988363|\n",
      "|Charged Off| 710.390470656595|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Credit_Score) AS avg_Credit_Score\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|Loan_Status| avg_Annual_Income|\n",
      "+-----------+------------------+\n",
      "| Fully Paid|20161.219233612323|\n",
      "|Charged Off|18111.277164439278|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Annual_Income) AS avg_Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ricordati!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anche se non esistono valori distinti  COUNT(\\*)  può differire da COUNT(DISTINCT \\*).\n",
    "\n",
    "perché il primo conta tutte le righe mentre il secondo conta tutte e sole le righe dove non è presente neanche un NULL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(Credit_Score) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(DISTINCT *) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tentativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  tentativo non riuscito\\nrdd = sdf.rdd.map(lambda row: row.asDict())\\n\\nrdd.keyBy(lambda row: (row['Years_in_current_job'],\\n                             row['Home_Ownership'],\\n                             row['Number_of_Open_Accounts'],\\n                             row['Years_of_Credit_History'])).groupByKey()\\n                             \\ndef fill_null(d):\\n    if d['Maximum_Open_Credit'] == None:\\n        d['Maximum_Open_Credit'] = \\n    return d\\n\\nrdd.mapValues(fill_null)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  tentativo non riuscito per fillare i missing values\n",
    "rdd = sdf.rdd.map(lambda row: row.asDict())\n",
    "\n",
    "rdd.keyBy(lambda row: (row['Years_in_current_job'],\n",
    "                             row['Home_Ownership'],\n",
    "                             row['Number_of_Open_Accounts'],\n",
    "                             row['Years_of_Credit_History'])).groupByKey()\n",
    "                             \n",
    "def fill_null(d):\n",
    "    if d['Maximum_Open_Credit'] == None:\n",
    "        d['Maximum_Open_Credit'] = \n",
    "    return d\n",
    "\n",
    "rdd.mapValues(fill_null)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = sdf.select(columns_clustering).rdd.map(lambda row: Vectors_mllib.dense(row))\\n\\nX_scaled = StandardScaler_mllib(withMean=True, withStd=True).fit(X).transform(X)\\n\\n\\n questo da un errore inspiegabilmente\\nfor k_clusters in sample(range(2, 10), size=1, replace=False):\\n    current_clustering_model = KMeans.train(X_scaled, k=k_clusters, maxIterations=20, initializationMode=\"random\")\\n\\n\\n\\nKMeans_model = KMeans_mllib.train(X_scaled, k = 1, maxIterations=10, initializationMode=\"random\")\\n\\n\\ndef error(point):\\n    center = KMeans_model.centers[KMeans_model.predict(point)]\\n    return sqrt(sum([x**2 for x in (point - center)]))\\n\\nWSSE = X_scaled.map(lambda point: error(point)).reduce(lambda x, y: x + y)\\nprint(\"Best Within Sum of Squared Error = \", WSSE)\\n\\n\\nGaussianMixture_model = GaussianMixture_mllib.train(X_scaled, k=1)\\n\\nBisectingKMeans_model = BisectingKMeans_mllib.train(X_scaled, k=1, maxIterations=10)\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X = sdf.select(columns_clustering).rdd.map(lambda row: Vectors_mllib.dense(row))\n",
    "\n",
    "X_scaled = StandardScaler_mllib(withMean=True, withStd=True).fit(X).transform(X)\n",
    "\n",
    "\n",
    "'''''' questo da un errore inspiegabilmente\n",
    "for k_clusters in sample(range(2, 10), size=1, replace=False):\n",
    "    current_clustering_model = KMeans.train(X_scaled, k=k_clusters, maxIterations=20, initializationMode=\"random\")\n",
    "''''''\n",
    "\n",
    "\n",
    "KMeans_model = KMeans_mllib.train(X_scaled, k = 1, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "\n",
    "def error(point):\n",
    "    center = KMeans_model.centers[KMeans_model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSE = X_scaled.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Best Within Sum of Squared Error = \", WSSE)\n",
    "\n",
    "\n",
    "GaussianMixture_model = GaussianMixture_mllib.train(X_scaled, k=1)\n",
    "\n",
    "BisectingKMeans_model = BisectingKMeans_mllib.train(X_scaled, k=1, maxIterations=10)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sdf_prova.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    BIGINT( PERCENTILE(Credit_Score, 0.5) OVER(PARTITION BY cluster_label) ) AS toFill_Credit_Score,\n",
    "    BIGINT( PERCENTILE(Annual_Income, 0.5) OVER(PARTITION BY cluster_label) ) AS toFill_Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf_prova = spark.sql(sql)\n",
    "\n",
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Credit_Score'] is None:\n",
    "        d['Credit_Score'] = d['toFill_Credit_Score']\n",
    "    if d['Annual_Income'] is None:\n",
    "        d['Annual_Income'] = d['toFill_Annual_Income']\n",
    "    return Row(**d)\n",
    "\n",
    "sdf_prova = sdf_prova.rdd.map(fill_nulls).toDF().select(columns_prova)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
