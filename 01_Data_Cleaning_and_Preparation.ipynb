{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# librerie\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessione\n",
    "\n",
    "sc = SparkContext(appName=\"DDAM_Project\", master=\"local[*]\")\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DDAM_Project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: integer (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Annual_Income: integer (nullable = true)\n",
      " |-- Years_in_current_job: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: integer (nullable = true)\n",
      " |-- Number_of_Credit_Problems: integer (nullable = true)\n",
      " |-- Current_Credit_Balance: integer (nullable = true)\n",
      " |-- Maximum_Open_Credit: integer (nullable = true)\n",
      " |-- Bankruptcies: string (nullable = true)\n",
      " |-- Tax_Liens: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/credit_train.csv\", sep=\",\",\n",
    "                     inferSchema=True, header=True)\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "# rinominare le colonne sotituendo lo spazio con l'underscore\n",
    "for col in columns:\n",
    "    sdf = sdf.withColumnRenamed(col, col.replace(' ', '_'))\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nbr_nulls(spark_df, view_name, columns, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori nulli presenti in ogni attributo\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    Project = []\n",
    "    for col in columns:\n",
    "        Project.append('SUM(CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_nulls = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_nulls.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "        \n",
    "    return nbr_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:        514\n",
      "Customer_ID:        514\n",
      "Loan_Status:        514\n",
      "Current_Loan_Amount:        514\n",
      "Term:        514\n",
      "Credit_Score:      19668\n",
      "Annual_Income:      19668\n",
      "Years_in_current_job:        514\n",
      "Home_Ownership:        514\n",
      "Purpose:        514\n",
      "Monthly_Debt:        514\n",
      "Years_of_Credit_History:        514\n",
      "Months_since_last_delinquent:        514\n",
      "Number_of_Open_Accounts:        514\n",
      "Number_of_Credit_Problems:        514\n",
      "Current_Credit_Balance:        514\n",
      "Maximum_Open_Credit:        516\n",
      "Bankruptcies:        514\n",
      "Tax_Liens:        514\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset', columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbr_distincts(spark_df, view_name, columns, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori distinti di ciascun attributo.\n",
    "    il valore nullo non viene contato come valore distinto\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "\n",
    "    Project = []\n",
    "    for col in columns:\n",
    "        Project.append('COUNT(DISTINCT {0}) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_distincts = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_distincts.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "\n",
    "    return nbr_distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:      81999\n",
      "Customer_ID:      81999\n",
      "Loan_Status:          2\n",
      "Current_Loan_Amount:      22004\n",
      "Term:          2\n",
      "Credit_Score:        324\n",
      "Annual_Income:      36174\n",
      "Years_in_current_job:         12\n",
      "Home_Ownership:          4\n",
      "Purpose:         16\n",
      "Monthly_Debt:      65765\n",
      "Years_of_Credit_History:        506\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Current_Credit_Balance:      32730\n",
      "Maximum_Open_Credit:      44596\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset', columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributi con Data Type incoerente\n",
    "\n",
    "controlliamo alcuni \"attributi problematici\", cio√® quelli con un numero di valori distinti basso (inferiore a 200) ma che hanno valori di tipo numerico. Se necessario dobbiamo modificare il loro data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years_in_current_job:         12\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "# attributi problematici:\n",
    "\n",
    "problematic_columns = []\n",
    "for key, value in nbr_distincts.items():\n",
    "    if value < 200 and key not in ['Loan_Status', 'Term', 'Home_Ownership', 'Purpose']:\n",
    "        problematic_columns.append(key)\n",
    "        print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Years_in_current_job|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|              1 year|\n",
      "|           10+ years|\n",
      "|             2 years|\n",
      "|             3 years|\n",
      "|             4 years|\n",
      "|             5 years|\n",
      "|             6 years|\n",
      "|             7 years|\n",
      "|             8 years|\n",
      "|             9 years|\n",
      "|            < 1 year|\n",
      "|                 n/a|\n",
      "+--------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|Months_since_last_delinquent|\n",
      "+----------------------------+\n",
      "|                        null|\n",
      "|                           0|\n",
      "|                           1|\n",
      "|                          10|\n",
      "|                         100|\n",
      "|                         104|\n",
      "|                         106|\n",
      "|                         107|\n",
      "|                         108|\n",
      "|                          11|\n",
      "|                         110|\n",
      "|                         114|\n",
      "|                         115|\n",
      "|                         118|\n",
      "|                          12|\n",
      "|                         120|\n",
      "|                         129|\n",
      "|                          13|\n",
      "|                         130|\n",
      "|                         131|\n",
      "|                         139|\n",
      "|                          14|\n",
      "|                         141|\n",
      "|                         143|\n",
      "|                         148|\n",
      "|                          15|\n",
      "|                         152|\n",
      "|                          16|\n",
      "|                          17|\n",
      "|                         176|\n",
      "|                          18|\n",
      "|                          19|\n",
      "|                           2|\n",
      "|                          20|\n",
      "|                          21|\n",
      "|                          22|\n",
      "|                          23|\n",
      "|                          24|\n",
      "|                          25|\n",
      "|                          26|\n",
      "|                          27|\n",
      "|                          28|\n",
      "|                          29|\n",
      "|                           3|\n",
      "|                          30|\n",
      "|                          31|\n",
      "|                          32|\n",
      "|                          33|\n",
      "|                          34|\n",
      "|                          35|\n",
      "|                          36|\n",
      "|                          37|\n",
      "|                          38|\n",
      "|                          39|\n",
      "|                           4|\n",
      "|                          40|\n",
      "|                          41|\n",
      "|                          42|\n",
      "|                          43|\n",
      "|                          44|\n",
      "|                          45|\n",
      "|                          46|\n",
      "|                          47|\n",
      "|                          48|\n",
      "|                          49|\n",
      "|                           5|\n",
      "|                          50|\n",
      "|                          51|\n",
      "|                          52|\n",
      "|                          53|\n",
      "|                          54|\n",
      "|                          55|\n",
      "|                          56|\n",
      "|                          57|\n",
      "|                          58|\n",
      "|                          59|\n",
      "|                           6|\n",
      "|                          60|\n",
      "|                          61|\n",
      "|                          62|\n",
      "|                          63|\n",
      "|                          64|\n",
      "|                          65|\n",
      "|                          66|\n",
      "|                          67|\n",
      "|                          68|\n",
      "|                          69|\n",
      "|                           7|\n",
      "|                          70|\n",
      "|                          71|\n",
      "|                          72|\n",
      "|                          73|\n",
      "|                          74|\n",
      "|                          75|\n",
      "|                          76|\n",
      "|                          77|\n",
      "|                          78|\n",
      "|                          79|\n",
      "|                           8|\n",
      "|                          80|\n",
      "|                          81|\n",
      "|                          82|\n",
      "|                          83|\n",
      "|                          84|\n",
      "|                          85|\n",
      "|                          86|\n",
      "|                          87|\n",
      "|                          88|\n",
      "|                          89|\n",
      "|                           9|\n",
      "|                          90|\n",
      "|                          91|\n",
      "|                          92|\n",
      "|                          93|\n",
      "|                          94|\n",
      "|                          96|\n",
      "|                          97|\n",
      "|                          NA|\n",
      "+----------------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Number_of_Open_Accounts|\n",
      "+-----------------------+\n",
      "|                   null|\n",
      "|                      0|\n",
      "|                      1|\n",
      "|                      2|\n",
      "|                      3|\n",
      "|                      4|\n",
      "|                      5|\n",
      "|                      6|\n",
      "|                      7|\n",
      "|                      8|\n",
      "|                      9|\n",
      "|                     10|\n",
      "|                     11|\n",
      "|                     12|\n",
      "|                     13|\n",
      "|                     14|\n",
      "|                     15|\n",
      "|                     16|\n",
      "|                     17|\n",
      "|                     18|\n",
      "|                     19|\n",
      "|                     20|\n",
      "|                     21|\n",
      "|                     22|\n",
      "|                     23|\n",
      "|                     24|\n",
      "|                     25|\n",
      "|                     26|\n",
      "|                     27|\n",
      "|                     28|\n",
      "|                     29|\n",
      "|                     30|\n",
      "|                     31|\n",
      "|                     32|\n",
      "|                     33|\n",
      "|                     34|\n",
      "|                     35|\n",
      "|                     36|\n",
      "|                     37|\n",
      "|                     38|\n",
      "|                     39|\n",
      "|                     40|\n",
      "|                     41|\n",
      "|                     42|\n",
      "|                     43|\n",
      "|                     44|\n",
      "|                     45|\n",
      "|                     47|\n",
      "|                     48|\n",
      "|                     52|\n",
      "|                     56|\n",
      "|                     76|\n",
      "+-----------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|Number_of_Credit_Problems|\n",
      "+-------------------------+\n",
      "|                     null|\n",
      "|                        0|\n",
      "|                        1|\n",
      "|                        2|\n",
      "|                        3|\n",
      "|                        4|\n",
      "|                        5|\n",
      "|                        6|\n",
      "|                        7|\n",
      "|                        8|\n",
      "|                        9|\n",
      "|                       10|\n",
      "|                       11|\n",
      "|                       12|\n",
      "|                       15|\n",
      "+-------------------------+\n",
      "\n",
      "+------------+\n",
      "|Bankruptcies|\n",
      "+------------+\n",
      "|        null|\n",
      "|           0|\n",
      "|           1|\n",
      "|           2|\n",
      "|           3|\n",
      "|           4|\n",
      "|           5|\n",
      "|           6|\n",
      "|           7|\n",
      "|          NA|\n",
      "+------------+\n",
      "\n",
      "+---------+\n",
      "|Tax_Liens|\n",
      "+---------+\n",
      "|     null|\n",
      "|        0|\n",
      "|        1|\n",
      "|       10|\n",
      "|       11|\n",
      "|       15|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        9|\n",
      "|       NA|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dei valori distinti degli attributi problematici\n",
    "\n",
    "for col in problematic_columns:\n",
    "    sdf.select(col).distinct().orderBy(col).show(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'le Row sono tipi particolari di Tuple, quindi sono oggetti immutabili.\\nin tutto il notebook quindi per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni)\\nin RDD di Dictionaries.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sdf.rdd  # di default questa trasformazione genera un RDD di Row()\n",
    "\n",
    "'''le Row sono tipi particolari di Tuple, quindi sono oggetti immutabili.\n",
    "in tutto il notebook quindi per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni)\n",
    "in RDD di Dictionaries.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Years_in_current_job'\n",
    "\n",
    "i valori sono evidentemente categorici e quindi l'attributo viene mantenuto come stringa.\n",
    "\n",
    "sono presenti 4222 valori uguali a 'n/a'. Pensiamo si possa trattare di soggetti senza lavoro. Il valore √® quindi coerente e viene mantenuto cos√¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda row: row['Years_in_current_job'] == 'n/a').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Months_since_last_delinquent'\n",
    "\n",
    "consideriamo questo attributo come categorico perch√© 117 valori distinti non sono abbastanza a nostro avviso per definire l'attributo come numerico (non avrebbe senso ad esempio dividere i valori in Bins) e anche perch√© la maggior parte dei valori sono uguali a 'NA'.\n",
    "\n",
    "sono presenti infatti 53141 valori uguali a NA. la nostra interpretazione di questo valore √® che il soggetto in questione non ha mai commesso nessun reato (anche perch√© i valori 'NA' sono la maggior parte). trasformiamo i valori 'NA' in '-1', cio√® manteniamo l'attributo come stringa ma lo prepariamo nel caso si volesse trasformare in Intero per analizzare i suoi valori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53141"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda row: row['Months_since_last_delinquent'] == 'NA').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    if d['Months_since_last_delinquent'] == 'NA':\n",
    "        d['Months_since_last_delinquent'] = '-1'\n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Months_since_last_delinquent'] == '-1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Number_of_Open_Accounts'\n",
    "\n",
    "con 51 valori numerici distinti l'attributo viene considerato categorico. quindi rendiamo stringhe i suoi valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    d['Number_of_Open_Accounts'] = str(d['Number_of_Open_Accounts'])\n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Number_of_Credit_Problems'\n",
    "\n",
    "con 14 valori numerici distinti l'attributo viene considerato categorico. quindi rendiamo stringhe i suoi valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    d['Number_of_Credit_Problems'] = str(d['Number_of_Credit_Problems'])\n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Bankruptcies'\n",
    "\n",
    "con 8 valori numerici distinti l'attributo viene mantenuto categorico.\n",
    "\n",
    "sono presenti 204 valori dell'attributo 'Bankruptcies' uguali a NA. Poich√® il valore 0 √® presente si pensa possa trattarsi di missing values, quindi li trasformiamo in None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Bankruptcies'] == 'NA').count() + rdd.filter(lambda d: d['Bankruptcies'] is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    if d['Bankruptcies'] == 'NA':\n",
    "        d['Bankruptcies'] = None\n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Bankruptcies'] is None).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Tax_Liens'\n",
    "\n",
    "con 12 valori numerici distinti l'attributo viene mantenuto categorico.\n",
    "\n",
    "sono presenti 10 valori dell'attributo 'Tax_Liens' uguali a NA. Poich√® il valore 0 √® presente si pensa possa trattarsi di missing values, quindi li trasformiamo in None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Tax_Liens'] == 'NA').count() + rdd.filter(lambda d: d['Tax_Liens'] is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    if d['Tax_Liens'] == 'NA':\n",
    "        d['Tax_Liens'] = None\n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Tax_Liens'] is None).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gestione degli errori semantici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alcuni attributi hanno dei valori non coerenti con il significato che noi reputiamo possa avere l'attributo (essando sprovvisti di adeguata documentazione su di essi). Alcuni di questi errori sono stati scoperti in fasi pi√π avanzate del progetto (ex Data Understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il valore dell'attributo 'Current_Credit_Balance' non pu√≤ essere superiore al valore dell'attributo 'Maximum_Open_Credit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99819"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Current_Credit_Balance'] is not None and row['Maximum_Open_Credit'] is not None:\n",
    "        return row['Current_Credit_Balance'] < row['Maximum_Open_Credit']\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono 4526 valori dell'attributo 'Credit_Score' con valore superiore a 4000, che √® un valore troppo distante rispetto a quelli generici che assume questo attributo (si aggirano intorno a 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95293"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Credit_Score'] is not None:\n",
    "        return row['Credit_Score'] < 4000\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono diversi valori dell'attributo 'Maximum_Open_Credit' che sono pari a 99.999.999.  un numero eccessivamente pi√π alto rispetto a tutti gli altri valori e che per questo viene eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83904"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Current_Loan_Amount'] is not None:\n",
    "        return row['Current_Loan_Amount'] != 99999999\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eliminiamo le righe dove entrambi gli attributi 'Loan_ID' e 'Customer_ID' sono nulli, che corrispondono alle righe dove tutti i valori di tutti gli attributi sono nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83390"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.filter(lambda row: not ( (row['Loan_ID'] is None) and (row['Customer_ID'] is None) ))\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eliminiamo le righe duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73230"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.distinct()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mximum open credit di 100 000 000\n",
    "# annual income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problema: ci sono coppie di righe con tutti i valori duplicati eccetto per le due colonne 'Credit_Score' e 'Annual_Income', per le quali uno dei due valori √® presente e l'altro √® nullo.\n",
    "- Soluzione: si raggruppa per tutti gli attributi tranne quei due e poi si calcola la media di quei due. In questo modo se le uniche due righe uguali sono quelle con un valore nullo e uno non nullo per quegli attributi, lo media sar√† uguale al valore non nullo; se invece ci fossero altre righe ugauli ma con altri valori diversi non nulli per quegli attributi, viene effettivamente calcolata la media, il che √® auspicabile considerando che tutto il resto della riga √® uguale e quindi si tratta molto probabilmente dello stesso oggetto, duplicato per errore, di cui dunque prendiamo un valore medio tra quelli presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rdd.toDF()\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "columns_temp = [col for col in columns if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}, AVG(Credit_Score) AS Credit_Score, AVG(Annual_Income) AS Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY {0}\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- come si nota i Customer e Loan ID che si ripetevano nel dataset originale erano solo righe duplicate. il dataset pulito non presenta nessuna riga uguale negli ID. possiamo quindi eliminarli. Anche escludendo questi attributi tutte le righe rimangono distinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+\n",
      "|nbr_rows|nbr_customers|nbr_loans|\n",
      "+--------+-------------+---------+\n",
      "|   69057|        69057|    69057|\n",
      "+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows, COUNT(DISTINCT Customer_ID) AS nbr_customers, COUNT(DISTINCT Loan_ID) AS nbr_loans\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in columns if col != 'Customer_ID' and col != 'Loan_ID']\n",
    "\n",
    "sdf = sdf.select(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestione dei Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14838\n",
      "Annual_Income:      14838\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          1\n",
      "Bankruptcies:        138\n",
      "Tax_Liens:          7\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df=sdf, view_name = 'Bank_Loan_Dataset', columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filliamo i Missing values degli attributi \"Maximum_Open_Credit\", \"Bankruptcies\" e \"Tax_Liens\" usando la Media di diversi tipi di raggruppamenti.\n",
    "\n",
    "per farlo usiamo la sintassi dell'SQL analitico, creando nuove apposite colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    AVG(Maximum_Open_Credit) OVER(PARTITION BY Years_in_current_job,\n",
    "                                                Home_Ownership,\n",
    "                                                Number_of_Open_Accounts,\n",
    "                                                Years_of_Credit_History) AS toFill_Maximum_Open_Credit,\n",
    "    AVG(Bankruptcies) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                        Number_of_Credit_Problems) AS toFill_Bankruptcies,\n",
    "    AVG(Tax_Liens) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                    Number_of_Credit_Problems) AS toFill_Tax_Liens\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14838\n",
      "Annual_Income:      14838\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n"
     ]
    }
   ],
   "source": [
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Maximum_Open_Credit'] is None:\n",
    "        d['Maximum_Open_Credit'] = int(d['toFill_Maximum_Open_Credit'])\n",
    "    if d['Bankruptcies'] is None:\n",
    "        d['Bankruptcies'] = str(d['toFill_Bankruptcies'])\n",
    "    if d['Tax_Liens'] is None:\n",
    "        d['Tax_Liens'] = str(d['toFill_Tax_Liens'])\n",
    "    return Row(**d)\n",
    "\n",
    "sdf = sdf.rdd.map(fill_nulls).toDF().select(columns)\n",
    "\n",
    "nbr_nulls = get_nbr_nulls(spark_df=sdf, view_name = 'Bank_Loan_Dataset', columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filliamo i Missing Values degli attributi \"Credit_Score\" e \"Annual_Income\" dividendo il dataset in clusters e usando la Media dei valori nei clusters.\n",
    "\n",
    "Per effettuare il clustering dobbiamo considerare solo le colonne numeriche diverse da 'Credit_Score' e 'Annual_Income'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current_Loan_Amount',\n",
       " 'Monthly_Debt',\n",
       " 'Years_of_Credit_History',\n",
       " 'Current_Credit_Balance',\n",
       " 'Maximum_Open_Credit']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]\n",
    "\n",
    "columns_clustering = [col for col in columns_numerical if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "\n",
    "columns_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = sdf.select(columns_clustering).rdd.map(lambda row: Vectors.dense(row))\n",
    "\n",
    "X_scaled = StandardScaler(withMean=True, withStd=True).fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migliora modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KMeans.train(X_scaled, k=10, maxIterations=100, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_label(element):\n",
    "    d = element[0].asDict()\n",
    "    d['cluster_label'] = str(element[1])\n",
    "    return Row(**d)\n",
    "\n",
    "sdf = sdf.rdd.zip(model.predict(X_scaled)).map(append_label).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    AVG(Credit_Score) OVER(PARTITION BY cluster_label) AS toFill_Credit_Score,\n",
    "    AVG(Annual_Income) OVER(PARTITION BY cluster_label) AS toFill_Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:          0\n",
      "Annual_Income:          0\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n"
     ]
    }
   ],
   "source": [
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Credit_Score'] is None:\n",
    "        d['Credit_Score'] = float(d['toFill_Credit_Score'])\n",
    "    if d['Annual_Income'] is None:\n",
    "        d['Annual_Income'] = float(d['toFill_Annual_Income'])\n",
    "    return Row(**d)\n",
    "\n",
    "sdf = sdf.rdd.map(fill_nulls).toDF().select(columns)\n",
    "\n",
    "nbr_nulls = get_nbr_nulls(spark_df=sdf, view_name = 'Bank_Loan_Dataset', columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** sei qui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding of Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creiamo una nuova struttura dati tabulare dove mappiamo ciascun valore distinto degli attributi caegorici con un numero. Questa sar√† utile per sostituire √≤e stringhe degli attributi categorici "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|nbr_rows|\n",
      "+--------+\n",
      "|   69057|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Loan_Status|nbr_rows|\n",
      "+-----------+--------+\n",
      "| Fully Paid|   51088|\n",
      "|Charged Off|   17969|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Loan_Status| avg_Credit_Score|\n",
      "+-----------+-----------------+\n",
      "| Fully Paid|719.9645575699363|\n",
      "|Charged Off|710.2000954350247|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Credit_Score) AS avg_Credit_Score\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|Loan_Status|  avg_Credit_Score|\n",
      "+-----------+------------------+\n",
      "| Fully Paid|1410691.1453956058|\n",
      "|Charged Off|1253940.9389215843|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Annual_Income) AS avg_Credit_Score\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.write.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf = spark.read.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ricordati!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anche se non esistono valori distinti  COUNT(\\*)  pu√≤ differire da COUNT(DISTINCT \\*).\n",
    "\n",
    "perch√© il primo conta tutte le righe mentre il secondo conta tutte e sole le righe dove non √® presente neanche un NULL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(Credit_Score) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(DISTINCT *) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tentativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  tentativo non riuscito\\nrdd = sdf.rdd.map(lambda row: row.asDict())\\n\\nrdd.keyBy(lambda row: (row['Years_in_current_job'],\\n                             row['Home_Ownership'],\\n                             row['Number_of_Open_Accounts'],\\n                             row['Years_of_Credit_History'])).groupByKey()\\n                             \\ndef fill_null(d):\\n    if d['Maximum_Open_Credit'] == None:\\n        d['Maximum_Open_Credit'] = \\n    return d\\n\\nrdd.mapValues(fill_null)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  tentativo non riuscito per fillare i missing values\n",
    "rdd = sdf.rdd.map(lambda row: row.asDict())\n",
    "\n",
    "rdd.keyBy(lambda row: (row['Years_in_current_job'],\n",
    "                             row['Home_Ownership'],\n",
    "                             row['Number_of_Open_Accounts'],\n",
    "                             row['Years_of_Credit_History'])).groupByKey()\n",
    "                             \n",
    "def fill_null(d):\n",
    "    if d['Maximum_Open_Credit'] == None:\n",
    "        d['Maximum_Open_Credit'] = \n",
    "    return d\n",
    "\n",
    "rdd.mapValues(fill_null)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
