{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# librerie\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessione\n",
    "\n",
    "sc = SparkContext(appName=\"DDAM_Project\", master=\"local[*]\")\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DDAM_Project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: integer (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Annual_Income: integer (nullable = true)\n",
      " |-- Years_in_current_job: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: integer (nullable = true)\n",
      " |-- Number_of_Credit_Problems: integer (nullable = true)\n",
      " |-- Current_Credit_Balance: integer (nullable = true)\n",
      " |-- Maximum_Open_Credit: integer (nullable = true)\n",
      " |-- Bankruptcies: string (nullable = true)\n",
      " |-- Tax_Liens: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/credit_train.csv\", sep=\",\",\n",
    "                     inferSchema=True, header=True)\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "# rinominare le colonne sotituendo lo spazio con l'underscore\n",
    "for col in columns:\n",
    "    sdf = sdf.withColumnRenamed(col, col.replace(' ', '_'))\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbr_distincts(view_name, columns):\n",
    "    \"\"\"funzione per ottenere il numero di valori distinti di ciascun attributo.\n",
    "    il valore nullo non viene contato come valore distinto\"\"\"\n",
    "    \n",
    "    sdf.createOrReplaceTempView(view_name)\n",
    "\n",
    "    Project = []\n",
    "    for col in columns:\n",
    "        Project.append('COUNT(DISTINCT {0}) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "\n",
    "    return spark.sql(sql).first().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:      81999\n",
      "Customer_ID:      81999\n",
      "Loan_Status:          2\n",
      "Current_Loan_Amount:      22004\n",
      "Term:          2\n",
      "Credit_Score:        324\n",
      "Annual_Income:      36174\n",
      "Years_in_current_job:         12\n",
      "Home_Ownership:          4\n",
      "Purpose:         16\n",
      "Monthly_Debt:      65765\n",
      "Years_of_Credit_History:        506\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Current_Credit_Balance:      32730\n",
      "Maximum_Open_Credit:      44596\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "nbr_distincts = get_nbr_distincts(view_name = 'Bank_Loan_Dataset', columns = columns)\n",
    "\n",
    "for key, value in nbr_distincts.items():\n",
    "    print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nbr_nulls(view_name, columns):\n",
    "    \"\"\"funzione per ottenere il numero di valori nulli presenti in ogni attributo\"\"\"\n",
    "    \n",
    "    sdf.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    Project = []\n",
    "    for col in columns:\n",
    "        Project.append('SUM(CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)    \n",
    "        \n",
    "    return spark.sql(sql).first().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:        514\n",
      "Customer_ID:        514\n",
      "Loan_Status:        514\n",
      "Current_Loan_Amount:        514\n",
      "Term:        514\n",
      "Credit_Score:      19668\n",
      "Annual_Income:      19668\n",
      "Years_in_current_job:        514\n",
      "Home_Ownership:        514\n",
      "Purpose:        514\n",
      "Monthly_Debt:        514\n",
      "Years_of_Credit_History:        514\n",
      "Months_since_last_delinquent:        514\n",
      "Number_of_Open_Accounts:        514\n",
      "Number_of_Credit_Problems:        514\n",
      "Current_Credit_Balance:        514\n",
      "Maximum_Open_Credit:        516\n",
      "Bankruptcies:        514\n",
      "Tax_Liens:        514\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(view_name = 'Bank_Loan_Dataset', columns = columns)\n",
    "\n",
    "for key, value in nbr_nulls.items():\n",
    "    print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributi con Data Type incoerente\n",
    "\n",
    "controlliamo alcuni \"attributi problematici\", cio√® quelli con un numero di valori distinti basso (inferiore a 200) ma che hanno valori di tipo numerico. Se necessario dobbiamo modificare il loro data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years_in_current_job:         12\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "# attributi problematici:\n",
    "\n",
    "problematic_columns = []\n",
    "for key, value in nbr_distincts.items():\n",
    "    if value < 200 and key not in ['Loan_Status', 'Term', 'Home_Ownership', 'Purpose']:\n",
    "        problematic_columns.append(key)\n",
    "        print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Years_in_current_job|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|              1 year|\n",
      "|           10+ years|\n",
      "|             2 years|\n",
      "|             3 years|\n",
      "|             4 years|\n",
      "|             5 years|\n",
      "|             6 years|\n",
      "|             7 years|\n",
      "|             8 years|\n",
      "|             9 years|\n",
      "|            < 1 year|\n",
      "|                 n/a|\n",
      "+--------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|Months_since_last_delinquent|\n",
      "+----------------------------+\n",
      "|                        null|\n",
      "|                           0|\n",
      "|                           1|\n",
      "|                          10|\n",
      "|                         100|\n",
      "|                         104|\n",
      "|                         106|\n",
      "|                         107|\n",
      "|                         108|\n",
      "|                          11|\n",
      "|                         110|\n",
      "|                         114|\n",
      "|                         115|\n",
      "|                         118|\n",
      "|                          12|\n",
      "|                         120|\n",
      "|                         129|\n",
      "|                          13|\n",
      "|                         130|\n",
      "|                         131|\n",
      "|                         139|\n",
      "|                          14|\n",
      "|                         141|\n",
      "|                         143|\n",
      "|                         148|\n",
      "|                          15|\n",
      "|                         152|\n",
      "|                          16|\n",
      "|                          17|\n",
      "|                         176|\n",
      "|                          18|\n",
      "|                          19|\n",
      "|                           2|\n",
      "|                          20|\n",
      "|                          21|\n",
      "|                          22|\n",
      "|                          23|\n",
      "|                          24|\n",
      "|                          25|\n",
      "|                          26|\n",
      "|                          27|\n",
      "|                          28|\n",
      "|                          29|\n",
      "|                           3|\n",
      "|                          30|\n",
      "|                          31|\n",
      "|                          32|\n",
      "|                          33|\n",
      "|                          34|\n",
      "|                          35|\n",
      "|                          36|\n",
      "|                          37|\n",
      "|                          38|\n",
      "|                          39|\n",
      "|                           4|\n",
      "|                          40|\n",
      "|                          41|\n",
      "|                          42|\n",
      "|                          43|\n",
      "|                          44|\n",
      "|                          45|\n",
      "|                          46|\n",
      "|                          47|\n",
      "|                          48|\n",
      "|                          49|\n",
      "|                           5|\n",
      "|                          50|\n",
      "|                          51|\n",
      "|                          52|\n",
      "|                          53|\n",
      "|                          54|\n",
      "|                          55|\n",
      "|                          56|\n",
      "|                          57|\n",
      "|                          58|\n",
      "|                          59|\n",
      "|                           6|\n",
      "|                          60|\n",
      "|                          61|\n",
      "|                          62|\n",
      "|                          63|\n",
      "|                          64|\n",
      "|                          65|\n",
      "|                          66|\n",
      "|                          67|\n",
      "|                          68|\n",
      "|                          69|\n",
      "|                           7|\n",
      "|                          70|\n",
      "|                          71|\n",
      "|                          72|\n",
      "|                          73|\n",
      "|                          74|\n",
      "|                          75|\n",
      "|                          76|\n",
      "|                          77|\n",
      "|                          78|\n",
      "|                          79|\n",
      "|                           8|\n",
      "|                          80|\n",
      "|                          81|\n",
      "|                          82|\n",
      "|                          83|\n",
      "|                          84|\n",
      "|                          85|\n",
      "|                          86|\n",
      "|                          87|\n",
      "|                          88|\n",
      "|                          89|\n",
      "|                           9|\n",
      "|                          90|\n",
      "|                          91|\n",
      "|                          92|\n",
      "|                          93|\n",
      "|                          94|\n",
      "|                          96|\n",
      "|                          97|\n",
      "|                          NA|\n",
      "+----------------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Number_of_Open_Accounts|\n",
      "+-----------------------+\n",
      "|                   null|\n",
      "|                      0|\n",
      "|                      1|\n",
      "|                      2|\n",
      "|                      3|\n",
      "|                      4|\n",
      "|                      5|\n",
      "|                      6|\n",
      "|                      7|\n",
      "|                      8|\n",
      "|                      9|\n",
      "|                     10|\n",
      "|                     11|\n",
      "|                     12|\n",
      "|                     13|\n",
      "|                     14|\n",
      "|                     15|\n",
      "|                     16|\n",
      "|                     17|\n",
      "|                     18|\n",
      "|                     19|\n",
      "|                     20|\n",
      "|                     21|\n",
      "|                     22|\n",
      "|                     23|\n",
      "|                     24|\n",
      "|                     25|\n",
      "|                     26|\n",
      "|                     27|\n",
      "|                     28|\n",
      "|                     29|\n",
      "|                     30|\n",
      "|                     31|\n",
      "|                     32|\n",
      "|                     33|\n",
      "|                     34|\n",
      "|                     35|\n",
      "|                     36|\n",
      "|                     37|\n",
      "|                     38|\n",
      "|                     39|\n",
      "|                     40|\n",
      "|                     41|\n",
      "|                     42|\n",
      "|                     43|\n",
      "|                     44|\n",
      "|                     45|\n",
      "|                     47|\n",
      "|                     48|\n",
      "|                     52|\n",
      "|                     56|\n",
      "|                     76|\n",
      "+-----------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|Number_of_Credit_Problems|\n",
      "+-------------------------+\n",
      "|                     null|\n",
      "|                        0|\n",
      "|                        1|\n",
      "|                        2|\n",
      "|                        3|\n",
      "|                        4|\n",
      "|                        5|\n",
      "|                        6|\n",
      "|                        7|\n",
      "|                        8|\n",
      "|                        9|\n",
      "|                       10|\n",
      "|                       11|\n",
      "|                       12|\n",
      "|                       15|\n",
      "+-------------------------+\n",
      "\n",
      "+------------+\n",
      "|Bankruptcies|\n",
      "+------------+\n",
      "|        null|\n",
      "|           0|\n",
      "|           1|\n",
      "|           2|\n",
      "|           3|\n",
      "|           4|\n",
      "|           5|\n",
      "|           6|\n",
      "|           7|\n",
      "|          NA|\n",
      "+------------+\n",
      "\n",
      "+---------+\n",
      "|Tax_Liens|\n",
      "+---------+\n",
      "|     null|\n",
      "|        0|\n",
      "|        1|\n",
      "|       10|\n",
      "|       11|\n",
      "|       15|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        9|\n",
      "|       NA|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dei valori distinti degli attributi problematici\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "for col in problematic_columns:\n",
    "    sql = \"\"\"\n",
    "    SELECT DISTINCT {0}\n",
    "    FROM Bank_Loan_Dataset\n",
    "    ORDER BY {0}\n",
    "    \"\"\".format(col)\n",
    "\n",
    "    spark.sql(sql).show(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sdf.rdd\n",
    "\n",
    "# le Row sono tipi particolari di Tuple, quindi sono oggetti immutabili.\n",
    "# per sostituire i valori dunque trasformiamo le Row in Dictionaries.\n",
    "\n",
    "rdd = rdd.map(lambda row: row.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Years_in_current_job'\n",
    "\n",
    "i valori sono evidentemente categorici e quindi l'attributo viene mantenuto come stringa.\n",
    "\n",
    "sono presenti 4222 valori uguali a 'n/a'. Pensiamo si possa trattare di soggetti senza lavoro. Il valore √® quindi coerente e viene mantenuto cos√¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Years_in_current_job'] == 'n/a').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Months_since_last_delinquent'\n",
    "\n",
    "consideriamo questo attributo come categorico perch√© 117 valori distinti non sono abbastanza a nostro avviso per definire l'attributo come numerico (non avrebbe senso ad esempio dividere i valori in Bins) e anche perch√© la maggior parte dei valori sono uguali a 'NA'.\n",
    "\n",
    "sono presenti infatti 53141 valori uguali a NA. la nostra interpretazione di questo valore √® che il soggetto in questione non ha mai commesso nessun reato (anche perch√© i valori 'NA' sono la maggior parte). trasformiamo i valori 'NA' in '-1', cio√® manteniamo l'attributo come stringa ma lo prepariamo nel caso si volesse trasformare in Intero per analizzare i suoi valori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53141"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Months_since_last_delinquent'] == 'NA').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(d):\n",
    "    if d['Months_since_last_delinquent'] == 'NA':\n",
    "        d['Months_since_last_delinquent'] = '-1'\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Months_since_last_delinquent'] == '-1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Number_of_Open_Accounts'\n",
    "\n",
    "con 51 valori numerici distinti l'attributo viene considerato categorico. quindi modifichiamo i suoi valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(d):\n",
    "    d['Number_of_Open_Accounts'] = str(d['Number_of_Open_Accounts'])\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Number_of_Credit_Problems'\n",
    "\n",
    "con 14 valori numerici distinti l'attributo viene considerato categorico. quindi modifichiamo i suoi valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(d):\n",
    "    d['Number_of_Credit_Problems'] = str(d['Number_of_Credit_Problems'])\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Bankruptcies'\n",
    "\n",
    "con 8 valori numerici distinti l'attributo viene mantenuto categorico.\n",
    "\n",
    "sono presenti 204 valori dell'attributo 'Bankruptcies' uguali a NA. Poich√® il valore 0 √® presente si pensa possa trattarsi di missing values, quindi li trasformiamo in None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Bankruptcies'] == 'NA').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Bankruptcies'] is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(d):\n",
    "    if d['Bankruptcies'] == 'NA':\n",
    "        d['Bankruptcies'] = None\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Bankruptcies'] is None).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Tax_Liens'\n",
    "\n",
    "con 12 valori numerici distinti l'attributo viene mantenuto categorico.\n",
    "\n",
    "sono presenti 10 valori dell'attributo 'Tax_Liens' uguali a NA. Poich√® il valore 0 √® presente si pensa possa trattarsi di missing values, quindi li trasformiamo in None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Tax_Liens'] == 'NA').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda d: d['Tax_Liens'] is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(d):\n",
    "    if d['Tax_Liens'] == 'NA':\n",
    "        d['Tax_Liens'] = None\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(change_value)\n",
    "\n",
    "rdd.filter(lambda d: d['Tax_Liens'] is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Annual_Income=1167493, Bankruptcies='1', Credit_Score=709, Current_Credit_Balance=228190, Current_Loan_Amount=445412, Customer_ID='981165ec-3274-42f5-a3b4-d104041a9ca9', Home_Ownership='Home Mortgage', Loan_ID='14dd8831-6af5-400b-83ec-68e61888a048', Loan_Status='Fully Paid', Maximum_Open_Credit=416746, Monthly_Debt=5214.74, Months_since_last_delinquent='-1', Number_of_Credit_Problems='1', Number_of_Open_Accounts='6', Purpose='Home Improvements', Tax_Liens='0', Term='Short Term', Years_in_current_job='8 years', Years_of_Credit_History=17.2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trasformiamo nuovamente l'RDD di Dictionaries in un RDD di Rows\n",
    "\n",
    "rdd = rdd.map(lambda x: Row(**x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gestione degli errori semantici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alcuni attributi hanno dei valori non coerenti con il significato che noi reputiamo possa avere l'attributo (essando sprovvisti di adeguata documentazione su di essi). Alcuni di questi errori sono stati scoperti in fasi pi√π avanzate del progetto (ex Data Understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il valore dell'attributo 'Current_Credit_Balance' non pu√≤ essere superiore al valore dell'attributo 'Maximum_Open_Credit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99819"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Current_Credit_Balance'] is not None and row['Maximum_Open_Credit'] is not None:\n",
    "        return row['Current_Credit_Balance'] < row['Maximum_Open_Credit']\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono 4526 valori dell'attributo 'Credit_Score' con valore superiore a 4000, che √® un valore troppo distante rispetto a quelli generici che assume questo attributo (si aggirano intorno a 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Credit_Score'] is not None:\n",
    "        return row['Credit_Score'] < 4000\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono valori dell'attributo 'Maximum_Open_Credit' che sono pari a 99.999.999.  un numero eccessivamente pi√π alto rispetto a tutti gli altri valori e che per questo viene eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83904"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_errors(row):\n",
    "    if row['Current_Loan_Amount'] is not None:\n",
    "        return row['Current_Loan_Amount'] != 99999999\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(semantic_errors)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eliminiamo le righe dove entrambi gli attributi 'Loan_ID' e 'Customer_ID' sono nulli, che corrispondono alle righe dove tutti i valori di tutti gli attributi sono nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83390"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.filter(lambda row: not ( (row['Loan_ID'] is None) and (row['Customer_ID'] is None) ))\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eliminiamo le righe duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73230"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.distinct()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mximum open credit di 100 000 000\n",
    "# annual income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problema: ci sono coppie di righe con tutti i valori duplicati eccetto per le due colonne 'Credit_Score' e 'Annual_Income', per le quali uno dei due valori √® presente e l'altro √® nullo.\n",
    "- Soluzione: si raggruppa per tutti gli attributi tranne quei due e poi si calcola la media di quei due. In questo modo se le uniche due righe uguali sono quelle con un valore nullo e uno non nullo per quegli attributi, lo media sar√† uguale al valore non nullo; se invece ci fossero altre righe ugauli ma con altri valori diversi non nulli per quegli attributi, viene effettivamente calcolata la media, il che √® auspicabile considerando che tutto il resto della riga √® uguale e quindi si tratta molto probabilmente dello stesso oggetto, duplicato per errore, di cui dunque prendiamo un valore medio tra quelli presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rdd.toDF()\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "columns_temp = [col for col in columns if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}, AVG(Credit_Score) AS Credit_Score, AVG(Annual_Income) AS Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY {0}\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- come si nota i customers e loans ID che si ripetevano nel dataset originale erano solo righe duplicate. il dataset pulito non presenta nessuna riga uguale negli ID. possiamo quindi eliminarli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+\n",
      "|nbr_rows|nbr_customers|nbr_loans|\n",
      "+--------+-------------+---------+\n",
      "|   69057|        69057|    69057|\n",
      "+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows, COUNT(DISTINCT Customer_ID) AS nbr_customers, COUNT(DISTINCT Loan_ID) AS nbr_loans\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = [col for col in columns if col != 'Customer_ID' and col != 'Loan_ID']\n",
    "Project = ', '.join(columns)\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loan_Status',\n",
       " 'Term',\n",
       " 'Years_in_current_job',\n",
       " 'Home_Ownership',\n",
       " 'Purpose',\n",
       " 'Months_since_last_delinquent',\n",
       " 'Number_of_Open_Accounts',\n",
       " 'Number_of_Credit_Problems',\n",
       " 'Bankruptcies',\n",
       " 'Tax_Liens']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current_Loan_Amount',\n",
       " 'Credit_Score',\n",
       " 'Annual_Income',\n",
       " 'Monthly_Debt',\n",
       " 'Years_of_Credit_History',\n",
       " 'Current_Credit_Balance',\n",
       " 'Maximum_Open_Credit']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14838\n",
      "Annual_Income:      14838\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          1\n",
      "Bankruptcies:        138\n",
      "Tax_Liens:          7\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(view_name = 'Bank_Loan_Dataset', columns = columns)\n",
    "\n",
    "for key, value in nbr_nulls.items():\n",
    "    print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filliamo i Missing values degli attributi \"Maximum_Open_Credit\", \"Bankruptcies\" e \"Tax_Liens\" usando la Media di diversi tipi di raggruppamenti.\n",
    "\n",
    "per farlo usiamo la sintassi dell'SQL analitico, creando nuove apposite colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    AVG(Maximum_Open_Credit) OVER(PARTITION BY Years_in_current_job,\n",
    "                                                Home_Ownership,\n",
    "                                                Number_of_Open_Accounts,\n",
    "                                                Years_of_Credit_History) AS toFill_Maximum_Open_Credit,\n",
    "    AVG(Bankruptcies) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                        Number_of_Credit_Problems) AS toFill_Bankruptcies,\n",
    "    AVG(Tax_Liens) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                    Number_of_Credit_Problems) AS toFill_Tax_Liens\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sdf.rdd.map(lambda row: row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nulls(d):\n",
    "    if d['Maximum_Open_Credit'] is None:\n",
    "        d['Maximum_Open_Credit'] = int(d['toFill_Maximum_Open_Credit'])\n",
    "    if d['Bankruptcies'] is None:\n",
    "        d['Bankruptcies'] = str(d['toFill_Bankruptcies'])\n",
    "    if d['Tax_Liens'] is None:\n",
    "        d['Tax_Liens'] = str(d['toFill_Tax_Liens'])\n",
    "    return d\n",
    "\n",
    "rdd = rdd.map(fill_nulls).map(lambda x: Row(**x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rdd.toDF()\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "Project = ', '.join(columns)  # manteniamo solo le colonne che ci interessano\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status:          0\n",
      "Current_Loan_Amount:          0\n",
      "Term:          0\n",
      "Credit_Score:      14838\n",
      "Annual_Income:      14838\n",
      "Years_in_current_job:          0\n",
      "Home_Ownership:          0\n",
      "Purpose:          0\n",
      "Monthly_Debt:          0\n",
      "Years_of_Credit_History:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Current_Credit_Balance:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(view_name = 'Bank_Loan_Dataset', columns = columns)\n",
    "\n",
    "for key, value in nbr_nulls.items():\n",
    "    print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** sei qui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filliamo i Missing Values degli attributi \"Credit_Score\" e \"Annual_Income\" dividendo il dataset in clusters e usando la Media dei valori nei clusters dell'attributo da fillare.\n",
    "\n",
    "Per effettuare il clustering dobbiamo prima escludere le colonne categoriche e standardizzare il dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Current_Loan_Amount',\n",
       " 'Credit_Score',\n",
       " 'Annual_Income',\n",
       " 'Monthly_Debt',\n",
       " 'Years_of_Credit_History',\n",
       " 'Current_Credit_Balance',\n",
       " 'Maximum_Open_Credit']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project = ', '.join(columns_numerical)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf_numeric = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Current_Loan_Amount=387904, Credit_Score=723.0, Annual_Income=1409610.0, Monthly_Debt=3782.52, Years_of_Credit_History=22.1, Current_Credit_Balance=72238, Maximum_Open_Credit=344256)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_numeric = sdf_numeric.rdd\n",
    "rdd_numeric.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([387904.0, 723.0, 1409610.0, 3782.52, 22.1, 72238.0, 344256.0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = rdd_numeric.map(lambda row: Vectors.dense(row))\n",
    "X.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(X)\n",
    "X_scaled = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.4397, nan, nan, -1.2049, 0.5472, -0.6304, nan])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o332.trainKMeansModel.\n: java.lang.IllegalArgumentException: requirement failed\n\tat scala.Predef$.require(Predef.scala:212)\n\tat org.apache.spark.mllib.util.MLUtils$.fastSquaredDistance(MLUtils.scala:508)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure$.fastSquaredDistance(DistanceMeasure.scala:232)\n\tat org.apache.spark.mllib.clustering.EuclideanDistanceMeasure.isCenterConverged(DistanceMeasure.scala:190)\n\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$runAlgorithm$4.apply(KMeans.scala:336)\n\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$runAlgorithm$4.apply(KMeans.scala:334)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:334)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:251)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:233)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainKMeansModel(PythonMLLibAPI.scala:367)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a515c22006a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/clustering.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, runs, initializationMode, seed, initializationSteps, epsilon, initialModel)\u001b[0m\n\u001b[1;32m    354\u001b[0m         model = callMLlibFunc(\"trainKMeansModel\", rdd.map(_convert_to_vector), k, maxIterations,\n\u001b[1;32m    355\u001b[0m                               \u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializationSteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                               clusterInitialModel)\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusterCenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mKMeansModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed'"
     ]
    }
   ],
   "source": [
    "model = KMeans.train(X_scaled, k=10, maxIterations=100, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding of Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creiamo una nuova struttura dati tabulare dove mappiamo ciascun valore distinto degli attributi caegorici con un numero. Questa sar√† utile per sostituire √≤e stringhe degli attributi categorici "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|nbr_rows|\n",
      "+--------+\n",
      "|   69057|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Loan_Status|nbr_rows|\n",
      "+-----------+--------+\n",
      "| Fully Paid|   51088|\n",
      "|Charged Off|   17969|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Loan_Status| avg_Credit_Score|\n",
      "+-----------+-----------------+\n",
      "| Fully Paid|719.9645575699363|\n",
      "|Charged Off|710.2000954350247|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Credit_Score) AS avg_Credit_Score\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|Loan_Status|  avg_Credit_Score|\n",
      "+-----------+------------------+\n",
      "| Fully Paid|1410691.1453956058|\n",
      "|Charged Off|1253940.9389215843|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Loan_Status, AVG(Annual_Income) AS avg_Credit_Score\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY Loan_Status \"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.write.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf = spark.read.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ricordati!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anche se non esistono valori distinti  COUNT(\\*)  pu√≤ differire da COUNT(DISTINCT \\*).\n",
    "\n",
    "perch√© il primo conta tutte le righe mentre il secondo conta tutte e sole le righe dove non √® presente neanche un NULL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(Credit_Score) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(DISTINCT *) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tentativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  tentativo non riuscito\\nrdd = sdf.rdd.map(lambda row: row.asDict())\\n\\nrdd.keyBy(lambda row: (row['Years_in_current_job'],\\n                             row['Home_Ownership'],\\n                             row['Number_of_Open_Accounts'],\\n                             row['Years_of_Credit_History'])).groupByKey()\\n                             \\ndef fill_null(d):\\n    if d['Maximum_Open_Credit'] == None:\\n        d['Maximum_Open_Credit'] = \\n    return d\\n\\nrdd.mapValues(fill_null)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  tentativo non riuscito per fillare i missing values\n",
    "rdd = sdf.rdd.map(lambda row: row.asDict())\n",
    "\n",
    "rdd.keyBy(lambda row: (row['Years_in_current_job'],\n",
    "                             row['Home_Ownership'],\n",
    "                             row['Number_of_Open_Accounts'],\n",
    "                             row['Years_of_Credit_History'])).groupByKey()\n",
    "                             \n",
    "def fill_null(d):\n",
    "    if d['Maximum_Open_Credit'] == None:\n",
    "        d['Maximum_Open_Credit'] = \n",
    "    return d\n",
    "\n",
    "rdd.mapValues(fill_null)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
