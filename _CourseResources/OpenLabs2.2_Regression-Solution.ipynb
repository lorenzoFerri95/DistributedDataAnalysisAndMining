{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenLab 2.2 Regression\n",
    "\n",
    "In this Openlab we are going to **predict the price** of airbnbs based on a set of given features.\n",
    "\n",
    "Steps:\n",
    "1. Clean the data: transform properly the string data to *float*;\n",
    "2. Split the dataset into *train* and *test*;\n",
    "3. Normalize the numeric values using the *StandardScaler*;\n",
    "4. Train a *LinearRegressionWithSGD* model using the training set;\n",
    "5. Evaluate the prediction of model for the test set computing the *Mean Square Error*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7696"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a spark context\n",
    "conf = SparkConf().setAppName(\"pre-processing\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# load the dataset\n",
    "server = \"11\"\n",
    "rdd_airbnbs = sc.textFile(\"hdfs://kddrtserver{0}.isti.cnr.it:9000/hpsa/datasets/venice_airbnb_regression.csv\".format(server))\n",
    "rdd_airbnbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id;price;accommodates;bathrooms;bedrooms;beds;review_scores_rating;review_scores_accuracy;review_scores_cleanliness;review_scores_checkin;review_scores_communication;review_scores_location;review_scores_value;reviews_per_month',\n",
       " '6623;$225.00;4;2.0;2.0;4.0;99.0;10.0;10.0;10.0;10.0;10.0;10.0;1.3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_airbnbs.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the line\n",
    "def parseLine(line):\n",
    "    return [float(x.replace(\"$\", \"\").replace(\",\",\"\")) for x in line.split(';')]\n",
    "\n",
    "# remove the header\n",
    "header = rdd_airbnbs.first()\n",
    "\n",
    "# preprocessing\n",
    "cleaned_rdd = rdd_airbnbs.filter(lambda l: l != header) \\\n",
    "                  .map(parseLine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6623.0, 225.0, 4.0, 2.0, 2.0, 4.0, 99.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 1.3], [44527.0, 300.0, 6.0, 2.0, 3.0, 3.0, 96.0, 10.0, 9.0, 10.0, 10.0, 10.0, 9.0, 0.67], [44998.0, 200.0, 6.0, 2.0, 2.0, 4.0, 60.0, 6.0, 7.0, 7.0, 6.0, 9.0, 5.0, 0.05], [45036.0, 120.0, 2.0, 1.0, 1.0, 1.0, 78.0, 8.0, 8.0, 8.0, 9.0, 9.0, 7.0, 0.17], [46158.0, 200.0, 2.0, 1.0, 1.0, 1.0, 73.0, 7.0, 8.0, 8.0, 7.0, 9.0, 7.0, 0.21]]\n"
     ]
    }
   ],
   "source": [
    "# sample the train set\n",
    "rdd_train = cleaned_rdd.sample(withReplacement=False, fraction=0.7, seed=100)\n",
    "\n",
    "# obtain the test set\n",
    "train_ids = rdd_train.map(lambda x: x[0]).collect()\n",
    "rdd_test = cleaned_rdd.filter(lambda x: x[0] not in train_ids)\n",
    "\n",
    "# print\n",
    "print(rdd_test.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the split went well\n",
    "assert rdd_train.count() + rdd_test.count() == rdd_airbnbs.count() - 1, \"Wrong split!\" # minus 1 because of the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "# normalize using the mean and the std\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "std_scaler = scaler.fit(rdd_train)\n",
    "rdd_train_norm = std_scaler.transform(rdd_train)\n",
    "rdd_test_norm = std_scaler.transform(rdd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the model\n",
    "def getVectors(v):\n",
    "    # ignoring the id in the first tuple value\n",
    "    return LabeledPoint(v[1], v[2:len(v)])\n",
    "\n",
    "\n",
    "# adapting the train and test vectors for the model  \n",
    "rdd_train_vectors = rdd_train_norm.map(lambda x: getVectors(x))\n",
    "rdd_test_vectors = rdd_test_norm.map(lambda x: getVectors(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LinearRegressionWithSGD.train(rdd_train_vectors, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.2086, 0.0577, 0.2889, -0.0661, 0.1684, -0.0568, 0.0379, -0.0104, -0.0311, 0.094, -0.1075, -0.1299])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coeficients of the model\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Training\n",
      "Mean Squared Error = 0.7562160161873945\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_target_predic = rdd_train_vectors.map(lambda x: (x.label, model.predict(x.features)))\n",
    "SE = train_target_predic.map(lambda vp: (vp[0] - vp[1])**2)\n",
    "MSE = SE.reduce(lambda x, y: x + y) / train_target_predic.count()\n",
    "\n",
    "print(\"Evaluation - Training\")\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Test\n",
      "Mean Squared Error = 3.479052207082139\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "test_target_predic = rdd_test_vectors.map(lambda x: (x.label, model.predict(x.features)))\n",
    "SE = test_target_predic.map(lambda vp: (vp[0] - vp[1])**2)\n",
    "MSE = SE.reduce(lambda x, y: x + y) / test_target_predic.count()\n",
    "\n",
    "print(\"Evaluation - Test\")\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
