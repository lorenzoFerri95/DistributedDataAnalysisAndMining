{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# librerie\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "\n",
    "#####################################   mllib   #####################################\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessione\n",
    "\n",
    "sc = SparkContext(appName=\"DDAM_Project\", master=\"local[*]\")\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DDAM_Project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Debt_Income_Rate: double (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Installment_Rate: double (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- cluster_label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status_dataset\")\n",
    "\n",
    "columns = sdf.schema.names\n",
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on \"Loan_Status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Numeric Only Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=sdf[columns_numerical+['Loan_Status']]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=columns_numerical,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(data)\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Loan_Status\", outputCol=\"label\")\n",
    "indexed = indexer.fit(output).transform(output)\n",
    "\n",
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features',\n",
    "                              outputCol='std_features')\n",
    "\n",
    "sonar = indexed.select(['features', 'Loan_Status','label']) #sdf with one column being the vectorized features, another column \n",
    "\n",
    "# Prepare training and test set\n",
    "training, test = sonar.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[ lr])\n",
    "\n",
    "#This grid will have 6 x 2 x 3 = 36 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0,0.2,0.4,0.6,0.8,1]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.family, ['auto', 'binomial', 'multinomial' ]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),  # areaUnderROC\" (default)\n",
    "                          numFolds=3,\n",
    "                          parallelism=10) \n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)  # Make predictions on test. cvModel uses the best model found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.302937\n",
      "Accuracy = 0.697063\n",
      "Precision = 0.625775\n",
      "Recall = 0.697063\n",
      "F1 = 0.578406\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedPrecision\")\n",
    "\n",
    "precision = evaluator.evaluate(prediction)\n",
    "print(\"Precision = %g\" % (precision))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedRecall\")\n",
    "\n",
    "recall = evaluator.evaluate(prediction)\n",
    "print(\"Recall = %g\" % (recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "print(\"F1 = %g\" % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "pipeline = Pipeline(stages=[ standardizer,lsvc])\n",
    "\n",
    "# this grid will have 5 x 2 x 5 = 50 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lsvc.aggregationDepth, [2,3,4,7,10]) \\\n",
    "    .addGrid(lsvc.fitIntercept, [True, False]) \\\n",
    "    .addGrid(lsvc.regParam,[0.0,0.2,0.4,0.7,0.9]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=10)\n",
    "\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/ml/evaluation/MulticlassClassificationEvaluator.html\n",
    "Only one metric can be chosen in the multiclassclassificationevaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.302394\n",
      "Accuracy = 0.697606\n",
      "Precision = 0.486654\n",
      "Recall = 0.697606\n",
      "F1 = 0.573342\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedPrecision\")\n",
    "\n",
    "precision = evaluator.evaluate(prediction)\n",
    "print(\"Precision = %g\" % (precision))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedRecall\")\n",
    "\n",
    "recall = evaluator.evaluate(prediction)\n",
    "print(\"Recall = %g\" % (recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "print(\"F1 = %g\" % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Classifiers considering mixed categorical and numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for mixed input type classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf=sdf #encoded dataframe\n",
    "\n",
    "#encoding\n",
    "\n",
    "\n",
    "for col in columns_categorical:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col+'_Encoded')\n",
    "    edf = indexer.fit(edf).transform(edf)\n",
    "\n",
    "\n",
    "columnse = edf.schema.names #names of all columns in the encoded dataset\n",
    "columns_categoricale = [col.name for col in edf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "columns_numericale = [col for col in columnse if (col not in columns_categoricale and col!= 'Loan_Status_Encoded') ]\n",
    "\n",
    "\n",
    "data=edf[columns_numericale+['Loan_Status']]\n",
    "\n",
    "# vectorize the numerical and freshly encoded categorical features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=columns_numericale, \n",
    "    outputCol=\"features\", handleInvalid = \"skip\") #handleInvalid = \"skip\" is a current way to deal with missing values.\n",
    "output = assembler.transform(data)\n",
    "\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Loan_Status\", outputCol=\"label\").fit(output)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training, test) = output.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.first of DataFrame[Annual_Income: bigint, Bankruptcies: bigint, Credit_Score: bigint, Current_Credit_Balance: bigint, Current_Loan_Amount: bigint, Debt_Income_Rate: double, Installment_Rate: double, Maximum_Open_Credit: bigint, Monthly_Debt: double, Number_of_Credit_Problems: bigint, Number_of_Open_Accounts: bigint, Tax_Liens: bigint, Years_in_current_job: double, Years_of_Credit_History: double, cluster_label: bigint, Home_Ownership_Encoded: double, Months_since_last_delinquent_Encoded: double, Purpose_Encoded: double, Term_Encoded: double, Loan_Status: string, features: vector]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer,  ldt, labelConverter]) #featureIndexer,\n",
    "\n",
    "#This grid will have 6 x 6 x 2 x 2 = 144 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(ldt.maxBins, [112,150,180,200,300,500] )\\\n",
    "        .addGrid(ldt.maxDepth, [2,6,10,15,20,30] )\\\n",
    "        .addGrid(ldt.cacheNodeIds, [True, False]) \\\n",
    "        .addGrid(ldt.impurity, ['entropy', 'gini']) \\\n",
    "        .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=10)  \n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.400841\n",
      "Accuracy = 0.599159\n",
      "Precision = 0.605296\n",
      "Recall = 0.599159\n",
      "F1 = 0.602075\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedPrecision\")\n",
    "\n",
    "precision = evaluator.evaluate(prediction)\n",
    "print(\"Precision = %g\" % (precision))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedRecall\")\n",
    "\n",
    "recall = evaluator.evaluate(prediction)\n",
    "print(\"Recall = %g\" % (recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "print(\"F1 = %g\" % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "#feature 11 has 112 categorical values and  therefore maxbins needs \n",
    "#to be at least as large as the max number of cat values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer,  rf, labelConverter]) #featureIndexer,\n",
    "\n",
    "#This grid will have 6 x 2 x 4 x 5 = 180 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [5,10,15,20,30])\\\n",
    "    .addGrid(rf.maxDepth, [2,10,15])  \\\n",
    "    .build()\n",
    "\n",
    "#  .addGrid(rf.featureSubsetStrategy, ['1','5','6','10','14','17']) \n",
    "#    .addGrid(rf.impurity, ['entropy','gini'])  \\\n",
    "#    .addGrid(rf.maxBins, [112,200,300,400])  \\\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=10)  \n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"label\", \"features\", \"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|            features|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[1859.0,0.0,688.0...|[0.48694014628216...|       1.0|\n",
      "|  0.0|[2143.0,0.0,745.0...|[0.81457781425442...|       0.0|\n",
      "|  1.0|[2188.0,0.0,728.0...|[0.61733384301568...|       0.0|\n",
      "|  0.0|[2297.0,1.0,697.0...|[0.60014927148771...|       0.0|\n",
      "|  1.0|[2599.0,0.0,738.0...|[0.65021602604699...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.299031\n",
      "Accuracy = 0.700969\n",
      "Precision = 0.669123\n",
      "Recall = 0.700969\n",
      "F1 = 0.60747\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedPrecision\")\n",
    "\n",
    "precision = evaluator.evaluate(prediction)\n",
    "print(\"Precision = %g\" % (precision))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedRecall\")\n",
    "\n",
    "recall = evaluator.evaluate(prediction)\n",
    "print(\"Recall = %g\" % (recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "print(\"F1 = %g\" % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer,  nb, labelConverter]) #featureIndexer,\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(nb.smoothing,[0.4,0.7,0.9,1,1.1,1.2,1.3,1.5]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=10) \n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.561186\n",
      "Accuracy = 0.438814\n",
      "Precision = 0.618969\n",
      "Recall = 0.438814\n",
      "F1 = 0.430101\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedPrecision\")\n",
    "\n",
    "precision = evaluator.evaluate(prediction)\n",
    "print(\"Precision = %g\" % (precision))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"weightedRecall\")\n",
    "\n",
    "recall = evaluator.evaluate(prediction)\n",
    "print(\"Recall = %g\" % (recall))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "print(\"F1 = %g\" % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Classification with PCA:\n",
    "\n",
    "iterating over k, fitting the logistic regression model, and measuring the results.\n",
    "\n",
    "Here is where each set of k principal components is plugged into the Logistic Regression model to see how well the metrics come out relative to the unreduced dataset. Even though the results are impressive (we could cut the number of dimensions by half, namely from 14 down to 7, and lose only a bit over 1% in accuracy), the idea here was to demonstratte the use of PCA in the Apache Spark paradigm rather than actually reducing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numd=columns_numerical+ ['Loan_Status'] #numerical columns+dependent variable (Loan Status)\n",
    "\n",
    "datas=sdf[numd] #spark numerical + loan status\n",
    "\n",
    "datas = datas.withColumnRenamed(\"Loan_Status\",\"label\")\n",
    "\n",
    "#vectorizing the numerical columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=columns_numerical,\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(datas)\n",
    "\n",
    "#normalization\n",
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features',\n",
    "                              outputCol='std_features')\n",
    "model = standardizer.fit(output)\n",
    "output = model.transform(output)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\n",
    "indexed = indexer.fit(output).transform(output)\n",
    "\n",
    "sonar = indexed.select(['std_features', 'label', 'label_idx']) #extract only features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k number of components= 1\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.31068258695100054\n",
      "Area under roc curve = 0.5147377356450528\n",
      "Accuracy =  0.5326195899772209\n",
      "DenseMatrix([[8591., 6762.],\n",
      "             [3497., 3100.]])\n",
      "Class 0.0 precision = 0.710704831237591\n",
      "Class 0.0 recall = 0.5595649058815867\n",
      "Class 0.0 F1 Measure = 0.6261433621223716\n",
      "Class 1.0 precision = 0.314337862502535\n",
      "Class 1.0 recall = 0.469910565408519\n",
      "Class 1.0 F1 Measure = 0.3766936022844644\n",
      "Weighted recall = 0.5326195899772209\n",
      "Weighted precision = 0.5915780479690186\n",
      "Weighted F(1) Score = 0.5511720607259856\n",
      "Weighted F(0.5) Score = 0.5727994887250181\n",
      "Weighted false positive rate = 0.5031441186871153\n",
      "\n",
      "\n",
      "k number of components= 2\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.31853811117098574\n",
      "Area under roc curve = 0.51524314870289\n",
      "Accuracy =  0.5336223595609261\n",
      "DenseMatrix([[8450., 6557.],\n",
      "             [3555., 3120.]])\n",
      "Class 0.0 precision = 0.7038733860891295\n",
      "Class 0.0 recall = 0.5630705670687013\n",
      "Class 0.0 F1 Measure = 0.625647860210277\n",
      "Class 1.0 precision = 0.3224139712720885\n",
      "Class 1.0 recall = 0.46741573033707867\n",
      "Class 1.0 F1 Measure = 0.38160469667318986\n",
      "Weighted recall = 0.5336223595609262\n",
      "Weighted precision = 0.586437651659476\n",
      "Weighted F(1) Score = 0.5505169628479462\n",
      "Weighted F(0.5) Score = 0.5697988360598545\n",
      "Weighted false positive rate = 0.5031360621551462\n",
      "\n",
      "\n",
      "k number of components= 3\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.31081077566679916\n",
      "Area under roc curve = 0.5153574513336302\n",
      "Accuracy =  0.5324604720699497\n",
      "DenseMatrix([[8621., 6824.],\n",
      "             [3496., 3132.]])\n",
      "Class 0.0 precision = 0.7114797392093752\n",
      "Class 0.0 recall = 0.5581741663968922\n",
      "Class 0.0 F1 Measure = 0.6255714389376679\n",
      "Class 1.0 precision = 0.31458417034953795\n",
      "Class 1.0 recall = 0.47254073627036813\n",
      "Class 1.0 F1 Measure = 0.3777134587554269\n",
      "Weighted recall = 0.5324604720699497\n",
      "Weighted precision = 0.5923013841872667\n",
      "Weighted F(1) Score = 0.5511455479102636\n",
      "Weighted F(0.5) Score = 0.5731461416877665\n",
      "Weighted false positive rate = 0.5017455694026893\n",
      "\n",
      "\n",
      "k number of components= 4\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.3529696716682561\n",
      "Area under roc curve = 0.5705455351346618\n",
      "Accuracy =  0.5694263510174745\n",
      "DenseMatrix([[8801., 6702.],\n",
      "             [2883., 3875.]])\n",
      "Class 0.0 precision = 0.7532523108524478\n",
      "Class 0.0 recall = 0.5676965748564794\n",
      "Class 0.0 F1 Measure = 0.647441792032957\n",
      "Class 1.0 precision = 0.3663609719202042\n",
      "Class 1.0 recall = 0.573394495412844\n",
      "Class 1.0 F1 Measure = 0.4470723968849149\n",
      "Weighted recall = 0.5694263510174745\n",
      "Weighted precision = 0.6357997405050195\n",
      "Weighted F(1) Score = 0.5866136004687654\n",
      "Weighted F(0.5) Score = 0.6122680860414407\n",
      "Weighted false positive rate = 0.4283352807481511\n",
      "\n",
      "\n",
      "k number of components= 5\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.3556777138059733\n",
      "Area under roc curve = 0.572301378666463\n",
      "Accuracy =  0.5686247723132969\n",
      "DenseMatrix([[8587., 6669.],\n",
      "             [2804., 3900.]])\n",
      "Class 0.0 precision = 0.7538407514704591\n",
      "Class 0.0 recall = 0.562860513896172\n",
      "Class 0.0 F1 Measure = 0.6445003189852517\n",
      "Class 1.0 precision = 0.36900369003690037\n",
      "Class 1.0 recall = 0.5817422434367542\n",
      "Class 1.0 F1 Measure = 0.45157181728709544\n",
      "Weighted recall = 0.5686247723132969\n",
      "Weighted precision = 0.6363567961038572\n",
      "Weighted F(1) Score = 0.5856026561717527\n",
      "Weighted F(0.5) Score = 0.6119654130779628\n",
      "Weighted false positive rate = 0.42402201498037073\n",
      "\n",
      "\n",
      "k number of components= 6\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.35410790506902884\n",
      "Area under roc curve = 0.5707067473966251\n",
      "Accuracy =  0.5678695731819015\n",
      "DenseMatrix([[8553., 6627.],\n",
      "             [2809., 3847.]])\n",
      "Class 0.0 precision = 0.7527723992254884\n",
      "Class 0.0 recall = 0.5634387351778656\n",
      "Class 0.0 F1 Measure = 0.6444879813126365\n",
      "Class 1.0 precision = 0.3672904334542677\n",
      "Class 1.0 recall = 0.5779747596153846\n",
      "Class 1.0 F1 Measure = 0.4491535318155283\n",
      "Weighted recall = 0.5678695731819015\n",
      "Weighted precision = 0.6352706606207419\n",
      "Weighted F(1) Score = 0.5849465773992479\n",
      "Weighted F(0.5) Score = 0.6111193487098148\n",
      "Weighted false positive rate = 0.4264560783886512\n",
      "\n",
      "\n",
      "k number of components= 7\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.36305967963295993\n",
      "Area under roc curve = 0.5793236493678646\n",
      "Accuracy =  0.5761208885671628\n",
      "DenseMatrix([[8747., 6571.],\n",
      "             [2798., 3987.]])\n",
      "Class 0.0 precision = 0.7576440017323517\n",
      "Class 0.0 recall = 0.5710275492884188\n",
      "Class 0.0 F1 Measure = 0.6512303167926143\n",
      "Class 1.0 precision = 0.37762833870051143\n",
      "Class 1.0 recall = 0.5876197494473102\n",
      "Class 1.0 F1 Measure = 0.45978204462895694\n",
      "Weighted recall = 0.5761208885671628\n",
      "Weighted precision = 0.6409898700004132\n",
      "Weighted F(1) Score = 0.5924610761180265\n",
      "Weighted F(0.5) Score = 0.617698922139097\n",
      "Weighted false positive rate = 0.4174735898314337\n",
      "\n",
      "\n",
      "k number of components= 8\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.3555765507340343\n",
      "Area under roc curve = 0.5778435819301868\n",
      "Accuracy =  0.5732481189375397\n",
      "DenseMatrix([[8728., 6685.],\n",
      "             [2730., 3919.]])\n",
      "Class 0.0 precision = 0.761738523302496\n",
      "Class 0.0 recall = 0.5662752222150133\n",
      "Class 0.0 F1 Measure = 0.6496222693610211\n",
      "Class 1.0 precision = 0.3695775179177669\n",
      "Class 1.0 recall = 0.5894119416453603\n",
      "Class 1.0 F1 Measure = 0.4542978032805889\n",
      "Weighted recall = 0.5732481189375397\n",
      "Weighted precision = 0.64354984934714\n",
      "Weighted F(1) Score = 0.5907557851361641\n",
      "Weighted F(0.5) Score = 0.6181624883380237\n",
      "Weighted false positive rate = 0.4175609550771662\n",
      "\n",
      "\n",
      "k number of components= 9\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.35864296421185926\n",
      "Area under roc curve = 0.5809058398724035\n",
      "Accuracy =  0.5824763471822295\n",
      "DenseMatrix([[8947., 6351.],\n",
      "             [2784., 3797.]])\n",
      "Class 0.0 precision = 0.7626800784246868\n",
      "Class 0.0 recall = 0.5848476925088247\n",
      "Class 0.0 F1 Measure = 0.6620296718339561\n",
      "Class 1.0 precision = 0.3741623965313362\n",
      "Class 1.0 recall = 0.5769639872359824\n",
      "Class 1.0 F1 Measure = 0.4539422559626995\n",
      "Weighted recall = 0.5824763471822296\n",
      "Weighted precision = 0.6458175680476065\n",
      "Weighted F(1) Score = 0.5994389097402252\n",
      "Weighted F(0.5) Score = 0.6237567226122408\n",
      "Weighted false positive rate = 0.4206646674374225\n",
      "\n",
      "\n",
      "k number of components= 10\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.3593007164943347\n",
      "Area under roc curve = 0.5819142059890348\n",
      "Accuracy =  0.5824903439396726\n",
      "DenseMatrix([[8870., 6335.],\n",
      "             [2745., 3798.]])\n",
      "Class 0.0 precision = 0.7636676711149376\n",
      "Class 0.0 recall = 0.5833607365998027\n",
      "Class 0.0 F1 Measure = 0.6614466815809098\n",
      "Class 1.0 precision = 0.3748149610184546\n",
      "Class 1.0 recall = 0.5804676753782668\n",
      "Class 1.0 F1 Measure = 0.45550491724634207\n",
      "Weighted recall = 0.5824903439396726\n",
      "Weighted precision = 0.6466792913944442\n",
      "Weighted F(1) Score = 0.5994880203687949\n",
      "Weighted F(0.5) Score = 0.6241955919159387\n",
      "Weighted false positive rate = 0.41866193196160306\n",
      "\n",
      "\n",
      "k number of components= 11\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.36448973145832597\n",
      "Area under roc curve = 0.5845911988734962\n",
      "Accuracy =  0.5832496118366974\n",
      "DenseMatrix([[8857., 6383.],\n",
      "             [2743., 3915.]])\n",
      "Class 0.0 precision = 0.7635344827586207\n",
      "Class 0.0 recall = 0.5811679790026246\n",
      "Class 0.0 F1 Measure = 0.6599850968703427\n",
      "Class 1.0 precision = 0.3801709069722276\n",
      "Class 1.0 recall = 0.5880144187443677\n",
      "Class 1.0 F1 Measure = 0.4617834394904458\n",
      "Weighted recall = 0.5832496118366974\n",
      "Weighted precision = 0.6469743088803759\n",
      "Weighted F(1) Score = 0.5997226694872322\n",
      "Weighted F(0.5) Score = 0.6243876830887849\n",
      "Weighted false positive rate = 0.4140672140897051\n",
      "\n",
      "\n",
      "k number of components= 12\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.3554722098725067\n",
      "Area under roc curve = 0.5792599153259574\n",
      "Accuracy =  0.5814811441063946\n",
      "DenseMatrix([[9001., 6391.],\n",
      "             [2798., 3766.]])\n",
      "Class 0.0 precision = 0.7628612594287651\n",
      "Class 0.0 recall = 0.5847843035343036\n",
      "Class 0.0 F1 Measure = 0.6620572983707845\n",
      "Class 1.0 precision = 0.37077877325982084\n",
      "Class 1.0 recall = 0.5737355271176112\n",
      "Class 1.0 F1 Measure = 0.4504515280186592\n",
      "Weighted recall = 0.5814811441063946\n",
      "Weighted precision = 0.6456436679178821\n",
      "Weighted F(1) Score = 0.5987953072707959\n",
      "Weighted F(0.5) Score = 0.6233821414570861\n",
      "Weighted false positive rate = 0.42296131345447985\n",
      "\n",
      "\n",
      "k number of components= 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Area under precision recall curve = 0.36386067864656957\n",
      "Area under roc curve = 0.587792470061391\n",
      "Accuracy =  0.5852024070021882\n",
      "DenseMatrix([[8908., 6417.],\n",
      "             [2682., 3929.]])\n",
      "Class 0.0 precision = 0.7685936151855047\n",
      "Class 0.0 recall = 0.5812724306688417\n",
      "Class 0.0 F1 Measure = 0.661935723574215\n",
      "Class 1.0 precision = 0.37976029383336557\n",
      "Class 1.0 recall = 0.5943125094539404\n",
      "Class 1.0 F1 Measure = 0.46340744235419\n",
      "Weighted recall = 0.5852024070021882\n",
      "Weighted precision = 0.6514082993823048\n",
      "Weighted F(1) Score = 0.6021039189085702\n",
      "Weighted F(0.5) Score = 0.6278024024066586\n",
      "Weighted false positive rate = 0.40961746687940603\n",
      "\n",
      "\n",
      "k number of components= 14\n",
      "Summary Stats\n",
      "Area under precision recall curve = 0.36622127643412605\n",
      "Area under roc curve = 0.5837297357319509\n",
      "Accuracy =  0.5871868321738735\n",
      "DenseMatrix([[9055., 6225.],\n",
      "             [2854., 3859.]])\n",
      "Class 0.0 precision = 0.7603493156436308\n",
      "Class 0.0 recall = 0.5926047120418848\n",
      "Class 0.0 F1 Measure = 0.6660781933870314\n",
      "Class 1.0 precision = 0.3826854422848076\n",
      "Class 1.0 recall = 0.574854759422017\n",
      "Class 1.0 F1 Measure = 0.4594868131213907\n",
      "Weighted recall = 0.5871868321738736\n",
      "Weighted precision = 0.6450736560311278\n",
      "Weighted F(1) Score = 0.6030195867520455\n",
      "Weighted F(0.5) Score = 0.625138705535714\n",
      "Weighted false positive rate = 0.41972736070997174\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range (1,(len(columns_numerical))):\n",
    "    pca = PCA(k=k, inputCol=\"std_features\", outputCol=\"pca\")\n",
    "    model = pca.fit(sonar)\n",
    "    transformed = model.transform(sonar)\n",
    "\n",
    "    #data=transformed.select('pca', 'label_idx')\n",
    "\n",
    "    data = transformed.rdd.map(lambda x: LabeledPoint(x[2], MLLibVectors.fromML(x[3]))) \n",
    "\n",
    "    train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Run training algorithm to build the model\n",
    "    model = LogisticRegressionWithLBFGS.train(train, numClasses=2)\n",
    "\n",
    "    # Compute raw scores on the test set\n",
    "    predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    binary= BinaryClassificationMetrics(predictionAndLabels)\n",
    "    # Overall statistics\n",
    "    print('k number of components=',k)\n",
    "    \n",
    "    accuracy=metrics.accuracy \n",
    "    cm=metrics.confusionMatrix()\n",
    "    aupr=binary.areaUnderPR\n",
    "    auc=binary.areaUnderROC\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Area under precision recall curve = %s\" % aupr)\n",
    "    print(\"Area under roc curve = %s\" % auc)\n",
    "    print('Accuracy = ', accuracy)\n",
    "    \n",
    "    #precision, recall and f1 are all depracated since 2.2.2. version of Spark Apache. Only accuracy is available. \n",
    "    print(cm)\n",
    "\n",
    "    # Statistics by class\n",
    "    labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "    for label in sorted(labels):\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "    # Weighted stats\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed below, and as was observed previously with PCA, the LR with MLlib didn't yield impressive results in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar=sonar.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 0.41671639050810116 Accuracy =  0.5832836094918988\n"
     ]
    }
   ],
   "source": [
    "data = sonar.map(lambda x: LabeledPoint(x[2], MLLibVectors.fromML(x[0]))) \n",
    "#converting a ml vector to mll vectr\n",
    "\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(train)\n",
    "\n",
    "y_yhat = test.map(lambda x: (x.label, model.predict(x.features)))\n",
    "err = y_yhat.filter(lambda x: x[0] != x[1]).count() / float(test.count())\n",
    "print(\"Error = \" + str(err), 'Accuracy = ', 1-err )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on \"Credit_Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Annual_Income=1859, Bankruptcies=0, Credit_Score=688, Current_Credit_Balance=1436, Current_Loan_Amount=684, Debt_Income_Rate=27.401, Home_Ownership='Own Home', Installment_Rate=6.206, Loan_Status='Fully Paid', Maximum_Open_Credit=2739, Monthly_Debt=42.4487, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=3, Purpose='Other', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.0, Years_of_Credit_History=21.3, cluster_label=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training, test = sdf.randomSplit([0.7, 0.3])\n",
    "\n",
    "training.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set \"Charged_Off\" percentage over total: 30.72883368344141\n",
      "test set \"Fully_Paid\" percentage over total: 69.27116631655859\n"
     ]
    }
   ],
   "source": [
    "perc_Loan_Status = test.groupby('Loan_Status').count().collect()\n",
    "perc_Charged_Off = 100 * perc_Loan_Status[1]['count']/(perc_Loan_Status[0]['count'] + perc_Loan_Status[1]['count'])\n",
    "perc_Fully_Paid = 100 * perc_Loan_Status[0]['count']/(perc_Loan_Status[0]['count'] + perc_Loan_Status[1]['count'])\n",
    "\n",
    "print('test set \"Charged_Off\" percentage over total:', perc_Charged_Off)\n",
    "print('test set \"Fully_Paid\" percentage over total:', perc_Fully_Paid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_data_preparation_stages(spark_df, model_type, target_col, columns_toExclude, encode=True, scale=False):\n",
    "    \n",
    "    columns_categorical = [col.name for col in spark_df.schema.fields if isinstance(col.dataType, StringType)]\n",
    "    \n",
    "    columns_toExclude += columns_categorical + [target_col]\n",
    "    \n",
    "    ml_features = [col for col in spark_df.columns if col not in columns_toExclude]\n",
    "    \n",
    "    stages = []\n",
    "    \n",
    "    if encode:\n",
    "        for col in columns_categorical:\n",
    "            encoder = StringIndexer(inputCol = col, outputCol = col + \"_encoded\", stringOrderType = 'alphabetDesc')\n",
    "            stages += [encoder]\n",
    "            ml_features += [encoder.getOutputCol()]\n",
    "            \n",
    "    assembler = VectorAssembler(inputCols = ml_features, outputCol = \"features\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    if scale:\n",
    "        scaler = StandardScaler(inputCol = \"features\", outputCol = \"scaledFeatures\", withStd=True, withMean=False)\n",
    "        stages += [scaler]\n",
    "    \n",
    "    if model_type == 'classification':\n",
    "        label_encoder = StringIndexer(inputCol = target_col, outputCol = \"label\")\n",
    "        stages += [label_encoder]\n",
    "    \n",
    "    return stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_b8f3a25cea7a, StandardScaler_fdb29b59fa19]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_DP_stages = get_ml_data_preparation_stages(spark_df = sdf, model_type = 'regression', target_col = 'Credit_Score',\n",
    "                                              columns_toExclude = ['Loan_Status'], encode = False, scale = True)\n",
    "LR_DP_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "Row(Annual_Income=1859, Bankruptcies=0, Credit_Score=688, Current_Credit_Balance=1436, Current_Loan_Amount=684, Debt_Income_Rate=27.401, Home_Ownership='Own Home', Installment_Rate=6.206, Loan_Status='Fully Paid', Maximum_Open_Credit=2739, Monthly_Debt=42.4487, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=3, Purpose='Other', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.0, Years_of_Credit_History=21.3, cluster_label=1)\n",
      "\n",
      "training_prepared:\n",
      "Row(Annual_Income=1859, Bankruptcies=0, Credit_Score=688, Current_Credit_Balance=1436, Current_Loan_Amount=684, Debt_Income_Rate=27.401, Home_Ownership='Own Home', Installment_Rate=6.206, Loan_Status='Fully Paid', Maximum_Open_Credit=2739, Monthly_Debt=42.4487, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=3, Purpose='Other', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.0, Years_of_Credit_History=21.3, cluster_label=1, features=DenseVector([1859.0, 0.0, 1436.0, 684.0, 27.401, 6.206, 2739.0, 42.4487, 0.0, 3.0, 0.0, 0.0, 21.3, 1.0]), scaledFeatures=DenseVector([0.1249, 0.0, 0.278, 0.2614, 3.0592, 0.6978, 0.0205, 0.2464, 0.0, 0.6015, 0.0, 0.0, 3.0882, 4.6842]))\n",
      "\n",
      "test:\n",
      "Row(Annual_Income=2654, Bankruptcies=0, Credit_Score=747, Current_Credit_Balance=1032, Current_Loan_Amount=922, Debt_Income_Rate=31.0006, Home_Ownership='Rent', Installment_Rate=7.4363, Loan_Status='Fully Paid', Maximum_Open_Credit=2459, Monthly_Debt=68.5629, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=8, Purpose='Debt Consolidation', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.5, Years_of_Credit_History=10.1, cluster_label=1)\n",
      "\n",
      "test_prepared:\n",
      "Row(Annual_Income=2654, Bankruptcies=0, Credit_Score=747, Current_Credit_Balance=1032, Current_Loan_Amount=922, Debt_Income_Rate=31.0006, Home_Ownership='Rent', Installment_Rate=7.4363, Loan_Status='Fully Paid', Maximum_Open_Credit=2459, Monthly_Debt=68.5629, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=8, Purpose='Debt Consolidation', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.5, Years_of_Credit_History=10.1, cluster_label=1, features=DenseVector([2654.0, 0.0, 1032.0, 922.0, 31.0006, 7.4363, 2459.0, 68.5629, 0.0, 8.0, 0.0, 0.5, 10.1, 1.0]), scaledFeatures=DenseVector([0.1783, 0.0, 0.1998, 0.3524, 3.4611, 0.8361, 0.0184, 0.398, 0.0, 1.6039, 0.0, 0.1355, 1.4644, 4.6842]))\n"
     ]
    }
   ],
   "source": [
    "LR_DP_pipeline = Pipeline(stages = LR_DP_stages).fit(sdf)\n",
    "\n",
    "training_prepared = LR_DP_pipeline.transform(training)\n",
    "test_prepared = LR_DP_pipeline.transform(test)\n",
    "\n",
    "print('training:')\n",
    "print(training.rdd.first())\n",
    "print('\\ntraining_prepared:')\n",
    "print(training_prepared.rdd.first())\n",
    "print('\\ntest:')\n",
    "print(test.rdd.first())\n",
    "print('\\ntest_prepared:')\n",
    "print(test_prepared.rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(LinearRegression().explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pipeline:\n",
      "[PCA_36e756fa74f7, LinearRegression_5fa949d9048d]\n",
      "\n",
      "Best PCA Parameters:\n",
      "{Param(parent='PCA_36e756fa74f7', name='outputCol', doc='output column name'): 'pcaFeatures', Param(parent='PCA_36e756fa74f7', name='inputCol', doc='input column name'): 'scaledFeatures', Param(parent='PCA_36e756fa74f7', name='k', doc='the number of principal components (> 0)'): 13}\n",
      "\n",
      "Best Linear Regression Parameters:\n",
      "{Param(parent='LinearRegression_5fa949d9048d', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_5fa949d9048d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LinearRegression_5fa949d9048d', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_5fa949d9048d', name='featuresCol', doc='features column name'): 'pcaFeatures', Param(parent='LinearRegression_5fa949d9048d', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_5fa949d9048d', name='labelCol', doc='label column name'): 'label', Param(parent='LinearRegression_5fa949d9048d', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_5fa949d9048d', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_5fa949d9048d', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LinearRegression_5fa949d9048d', name='regParam', doc='regularization parameter (>= 0)'): 0.5, Param(parent='LinearRegression_5fa949d9048d', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_5fa949d9048d', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_5fa949d9048d', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "training_prepared = training_prepared.withColumnRenamed('Credit_Score', 'label')\n",
    "\n",
    "features_len = len(training_prepared.rdd.first()['features'])\n",
    "\n",
    "pca = PCA(inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "linear_regression = LinearRegression(featuresCol=\"pcaFeatures\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "stages = [pca, linear_regression]\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(pca.k, [1, 2, features_len-3, features_len-1, features_len])\\\n",
    "        .addGrid(linear_regression.loss, ['squaredError', 'huber'])\\\n",
    "        .addGrid(linear_regression.regParam, [0, 0.5, 1])\\\n",
    "        .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = Pipeline(stages = stages),\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = RegressionEvaluator(),\n",
    "                          numFolds = 3,\n",
    "                          parallelism = 10)\n",
    "\n",
    "Best_LR_pipeline = crossval.fit(training_prepared)\n",
    "\n",
    "print('Best Pipeline:')\n",
    "print(Best_LR_pipeline.bestModel.stages)\n",
    "print('\\nBest PCA Parameters:')\n",
    "print(Best_LR_pipeline.bestModel.stages[0].extractParamMap())\n",
    "print('\\nBest Linear Regression Parameters:')\n",
    "print(Best_LR_pipeline.bestModel.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercetta della Regressione Lienare:\n",
      "719.2276872313762\n",
      "\n",
      "Coefficienti della Regressione Lienare:\n",
      "[1.3210834297056928,0.5679753198064444,-1.1562795588198562,0.6816839237518202,4.1164309344350185,-2.063111474211884,1.48121538271208,1.872291683709596,1.060521199869502,1.3999631013954168,1.249060127279574,6.241891611698334,1.4343659516247973]\n"
     ]
    }
   ],
   "source": [
    "print('Intercetta della Regressione Lienare:')\n",
    "print(Best_LR_pipeline.bestModel.stages[1].intercept)\n",
    "print('\\nCoefficienti della Regressione Lienare:')\n",
    "print(Best_LR_pipeline.bestModel.stages[1].coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determination Coefficient (R^2) on test data = 0.0666471\n",
      "Mean Squared Error (MSE) on test data = 647.419\n",
      "Root Mean Squared Error (RMSE) on test data = 25.4444\n",
      "Mean Absolute Error (MAE) on test data = 18.9679\n"
     ]
    }
   ],
   "source": [
    "prediction = Best_LR_pipeline.transform(test_prepared)\n",
    "\n",
    "r2 = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(prediction)\n",
    "\n",
    "mse = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"mse\").evaluate(prediction)\n",
    "\n",
    "rmse = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(prediction)\n",
    "\n",
    "mae = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(prediction)\n",
    "\n",
    "print(\"Determination Coefficient (R^2) on test data = %g\" % r2)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mse)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_32abee704304,\n",
       " StringIndexer_730f95e41f00,\n",
       " StringIndexer_26514bdf8c98,\n",
       " StringIndexer_0ed720463535,\n",
       " StringIndexer_afe68a4427b7,\n",
       " VectorAssembler_e2747e071125]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_DP_stages = get_ml_data_preparation_stages(spark_df = sdf, model_type = 'regression', target_col = 'Credit_Score',\n",
    "                                              columns_toExclude = ['Loan_Status'], encode = True, scale = False)\n",
    "RF_DP_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "Row(Annual_Income=1859, Bankruptcies=0, Credit_Score=688, Current_Credit_Balance=1436, Current_Loan_Amount=684, Debt_Income_Rate=27.401, Home_Ownership='Own Home', Installment_Rate=6.206, Loan_Status='Fully Paid', Maximum_Open_Credit=2739, Monthly_Debt=42.4487, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=3, Purpose='Other', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.0, Years_of_Credit_History=21.3, cluster_label=1)\n",
      "\n",
      "training_prepared:\n",
      "Row(Annual_Income=1859, Bankruptcies=0, Credit_Score=688, Current_Credit_Balance=1436, Current_Loan_Amount=684, Debt_Income_Rate=27.401, Home_Ownership='Own Home', Installment_Rate=6.206, Loan_Status='Fully Paid', Maximum_Open_Credit=2739, Monthly_Debt=42.4487, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=3, Purpose='Other', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.0, Years_of_Credit_History=21.3, cluster_label=1, Home_Ownership_encoded=1.0, Loan_Status_encoded=0.0, Months_since_last_delinquent_encoded=4.0, Purpose_encoded=8.0, Term_encoded=0.0, features=DenseVector([1859.0, 0.0, 1436.0, 684.0, 27.401, 6.206, 2739.0, 42.4487, 0.0, 3.0, 0.0, 0.0, 21.3, 1.0, 1.0, 0.0, 4.0, 8.0, 0.0]))\n",
      "\n",
      "test:\n",
      "Row(Annual_Income=2654, Bankruptcies=0, Credit_Score=747, Current_Credit_Balance=1032, Current_Loan_Amount=922, Debt_Income_Rate=31.0006, Home_Ownership='Rent', Installment_Rate=7.4363, Loan_Status='Fully Paid', Maximum_Open_Credit=2459, Monthly_Debt=68.5629, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=8, Purpose='Debt Consolidation', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.5, Years_of_Credit_History=10.1, cluster_label=1)\n",
      "\n",
      "test_prepared:\n",
      "Row(Annual_Income=2654, Bankruptcies=0, Credit_Score=747, Current_Credit_Balance=1032, Current_Loan_Amount=922, Debt_Income_Rate=31.0006, Home_Ownership='Rent', Installment_Rate=7.4363, Loan_Status='Fully Paid', Maximum_Open_Credit=2459, Monthly_Debt=68.5629, Months_since_last_delinquent='Never committed', Number_of_Credit_Problems=0, Number_of_Open_Accounts=8, Purpose='Debt Consolidation', Tax_Liens=0, Term='Short Term', Years_in_current_job=0.5, Years_of_Credit_History=10.1, cluster_label=1, Home_Ownership_encoded=0.0, Loan_Status_encoded=0.0, Months_since_last_delinquent_encoded=4.0, Purpose_encoded=12.0, Term_encoded=0.0, features=DenseVector([2654.0, 0.0, 1032.0, 922.0, 31.0006, 7.4363, 2459.0, 68.5629, 0.0, 8.0, 0.0, 0.5, 10.1, 1.0, 0.0, 0.0, 4.0, 12.0, 0.0]))\n"
     ]
    }
   ],
   "source": [
    "RF_DP_pipeline = Pipeline(stages = RF_DP_stages).fit(sdf)\n",
    "\n",
    "# RF_DP_pipeline.save('RF_DP_pipeline')\n",
    "# RF_DP_pipeline = PipelineModel.load('RF_DP_pipeline')\n",
    "\n",
    "training_prepared = RF_DP_pipeline.transform(training)\n",
    "test_prepared = RF_DP_pipeline.transform(test)\n",
    "\n",
    "print('training:')\n",
    "print(training.rdd.first())\n",
    "print('\\ntraining_prepared:')\n",
    "print(training_prepared.rdd.first())\n",
    "print('\\ntest:')\n",
    "print(test.rdd.first())\n",
    "print('\\ntest_prepared:')\n",
    "print(test_prepared.rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1081762523111376242)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(RandomForestRegressor().explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pipeline:\n",
      "[PCA_2e237e96c506, RandomForestRegressionModel (uid=RandomForestRegressor_5feca2eb162c) with 30 trees]\n",
      "\n",
      "Best PCA Parameters:\n",
      "{Param(parent='PCA_2e237e96c506', name='outputCol', doc='output column name'): 'pcaFeatures', Param(parent='PCA_2e237e96c506', name='inputCol', doc='input column name'): 'features', Param(parent='PCA_2e237e96c506', name='k', doc='the number of principal components (> 0)'): 18}\n",
      "\n",
      "Best Random Forest Parameters:\n",
      "{Param(parent='RandomForestRegressor_5feca2eb162c', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, Param(parent='RandomForestRegressor_5feca2eb162c', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, Param(parent='RandomForestRegressor_5feca2eb162c', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto', Param(parent='RandomForestRegressor_5feca2eb162c', name='featuresCol', doc='features column name'): 'pcaFeatures', Param(parent='RandomForestRegressor_5feca2eb162c', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='RandomForestRegressor_5feca2eb162c', name='labelCol', doc='label column name'): 'label', Param(parent='RandomForestRegressor_5feca2eb162c', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.'): 32, Param(parent='RandomForestRegressor_5feca2eb162c', name='maxDepth', doc='Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10, Param(parent='RandomForestRegressor_5feca2eb162c', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, Param(parent='RandomForestRegressor_5feca2eb162c', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestRegressor_5feca2eb162c', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.'): 1, Param(parent='RandomForestRegressor_5feca2eb162c', name='numTrees', doc='Number of trees to train (at least 1)'): 30, Param(parent='RandomForestRegressor_5feca2eb162c', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='RandomForestRegressor_5feca2eb162c', name='seed', doc='random seed'): 1081762523111376242, Param(parent='RandomForestRegressor_5feca2eb162c', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "training_prepared = training_prepared.withColumnRenamed('Credit_Score', 'label')\n",
    "\n",
    "features_len = len(training_prepared.rdd.first()['features'])\n",
    "\n",
    "pca = PCA(inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "random_forest = RandomForestRegressor(featuresCol=\"pcaFeatures\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "stages = [pca, random_forest]\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(pca.k, [1, 2, features_len-3, features_len-1, features_len] )\\\n",
    "        .addGrid(random_forest.numTrees, [5, 10, 20, 30])\\\n",
    "        .addGrid(random_forest.maxDepth, [1, 3, 5, 10])\\\n",
    "        .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = Pipeline(stages = stages),\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = RegressionEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          parallelism=10)\n",
    "\n",
    "Best_RF_Pipeline = crossval.fit(training_prepared)\n",
    "\n",
    "print('Best Pipeline:')\n",
    "print(Best_RF_Pipeline.bestModel.stages)\n",
    "print('\\nBest PCA Parameters:')\n",
    "print(Best_RF_Pipeline.bestModel.stages[0].extractParamMap())\n",
    "print('\\nBest Random Forest Parameters:')\n",
    "print(Best_RF_Pipeline.bestModel.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Tree:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best Random Forest Tree:')\n",
    "print(Best_RF_Pipeline.bestModel.stages[1].toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determination Coefficient (R^2) on test data = 0.286025\n",
      "Mean Squared Error (MSE) on test data = 495.248\n",
      "Root Mean Squared Error (RMSE) on test data = 22.2542\n",
      "Mean Absolute Error (MAE) on test data = 16.7105\n"
     ]
    }
   ],
   "source": [
    "prediction = Best_RF_Pipeline.transform(test_prepared)\n",
    "\n",
    "r2 = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(prediction)\n",
    "\n",
    "mse = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"mse\").evaluate(prediction)\n",
    "\n",
    "rmse = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(prediction)\n",
    "\n",
    "mae = RegressionEvaluator(labelCol=\"Credit_Score\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(prediction)\n",
    "\n",
    "print(\"Determination Coefficient (R^2) on test data = %g\" % r2)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mse)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best_RF_Pipeline.save('Best_Regression_Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Importance for Regression\n",
    "\n",
    "Fittiamo nuovamente il migior modello di regressione ottenuto, ma senza eseguire la PCA, in modo da poter ottenere l'importanza delle Features originali. La feature importance della Random Forest precedente fornirebbe solo l'importanza delle Componenti Principali, che non è quindi rilevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(featuresCol=\"features\",\n",
    "                                      labelCol=\"label\",\n",
    "                                      predictionCol=\"prediction\")\n",
    "Best_Random_Forest_Regressor = random_forest.fit(training_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Term_encoded', 0.6400198774247271),\n",
       " ('Maximum_Open_Credit', 0.09780219059924301),\n",
       " ('Current_Loan_Amount', 0.08768526434166263),\n",
       " ('Purpose_encoded', 0.05953197356751779),\n",
       " ('Loan_Status_encoded', 0.023931316491823756),\n",
       " ('Years_of_Credit_History', 0.023096143221317107),\n",
       " ('Debt_Income_Rate', 0.017426426159552366),\n",
       " ('Months_since_last_delinquent_encoded', 0.017277096930852543),\n",
       " ('Home_Ownership_encoded', 0.006584514953259562),\n",
       " ('Annual_Income', 0.006578341665088518),\n",
       " ('Installment_Rate', 0.0051843573315337255),\n",
       " ('Current_Credit_Balance', 0.005087746905857592),\n",
       " ('Monthly_Debt', 0.0041511295552908575),\n",
       " ('Number_of_Open_Accounts', 0.0034396579369030196),\n",
       " ('Number_of_Credit_Problems', 0.0015007655616440393),\n",
       " ('Years_in_current_job', 0.0003131999664563091),\n",
       " ('Tax_Liens', 0.0002123704187862363),\n",
       " ('Bankruptcies', 0.0001776269684840113),\n",
       " ('cluster_label', 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_importance = list(zip(RF_DP_stages[-1].getInputCols(), Best_Random_Forest_Regressor.featureImportances))\n",
    "features_importance.sort(key = lambda x: x[1], reverse = True)\n",
    "features_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Mining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
