{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# librerie\n",
    "\n",
    "from numpy.random import choice as sample\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "######################## ml ###############################\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "######################## MLlib ###############################\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors as Vectors_mllib\n",
    "from pyspark.mllib.feature import StandardScaler as StandardScaler_mllib\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans as KMeans_mllib\n",
    "from pyspark.mllib.clustering import BisectingKMeans as BisectingKMeans_mllib\n",
    "from pyspark.mllib.clustering import GaussianMixture as GaussianMixture_mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessione\n",
    "\n",
    "sc = SparkContext(appName=\"DDAM_Project\", master=\"local[*]\")\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DDAM_Project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Current_Loan_Amount: integer (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Credit_Score: integer (nullable = true)\n",
      " |-- Annual_Income: integer (nullable = true)\n",
      " |-- Years_in_current_job: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Open_Accounts: integer (nullable = true)\n",
      " |-- Number_of_Credit_Problems: integer (nullable = true)\n",
      " |-- Current_Credit_Balance: integer (nullable = true)\n",
      " |-- Maximum_Open_Credit: integer (nullable = true)\n",
      " |-- Bankruptcies: string (nullable = true)\n",
      " |-- Tax_Liens: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/credit_train.csv\", sep=\",\",\n",
    "                     inferSchema=True, header=True)\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "# rinominare le colonne sotituendo lo spazio con l'underscore\n",
    "for col in columns:\n",
    "    sdf = sdf.withColumnRenamed(col, col.replace(' ', '_'))\n",
    "\n",
    "columns = sdf.schema.names\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nbr_nulls(spark_df, view_name, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori nulli presenti in ogni attributo\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    columns_temp = spark_df.schema.names\n",
    "    \n",
    "    Project = []\n",
    "    for col in columns_temp:\n",
    "        Project.append('SUM(CASE WHEN {0} IS NULL THEN 1 ELSE 0 END) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_nulls = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_nulls.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "        \n",
    "    return nbr_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:        514\n",
      "Customer_ID:        514\n",
      "Loan_Status:        514\n",
      "Current_Loan_Amount:        514\n",
      "Term:        514\n",
      "Credit_Score:      19668\n",
      "Annual_Income:      19668\n",
      "Years_in_current_job:        514\n",
      "Home_Ownership:        514\n",
      "Purpose:        514\n",
      "Monthly_Debt:        514\n",
      "Years_of_Credit_History:        514\n",
      "Months_since_last_delinquent:        514\n",
      "Number_of_Open_Accounts:        514\n",
      "Number_of_Credit_Problems:        514\n",
      "Current_Credit_Balance:        514\n",
      "Maximum_Open_Credit:        516\n",
      "Bankruptcies:        514\n",
      "Tax_Liens:        514\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbr_distincts(spark_df, view_name, print_result = True):\n",
    "    \"\"\"funzione per ottenere il numero di valori distinti di ciascun attributo.\n",
    "    il valore nullo non viene contato come valore distinto\"\"\"\n",
    "    \n",
    "    spark_df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    columns_temp = spark_df.schema.names\n",
    "\n",
    "    Project = []\n",
    "    for col in columns_temp:\n",
    "        Project.append('COUNT(DISTINCT {0}) AS {0}'.format(col))\n",
    "    Project = ', '.join(Project)\n",
    "\n",
    "    sql = \"\"\"\\\n",
    "    SELECT {0}\n",
    "    FROM {1}\\\n",
    "    \"\"\".format(Project, view_name)\n",
    "    \n",
    "    nbr_distincts = spark.sql(sql).first().asDict()\n",
    "    \n",
    "    if print_result:\n",
    "        for key, value in nbr_distincts.items():\n",
    "            print(key + ':', '{:>10}'.format(value))\n",
    "\n",
    "    return nbr_distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID:      81999\n",
      "Customer_ID:      81999\n",
      "Loan_Status:          2\n",
      "Current_Loan_Amount:      22004\n",
      "Term:          2\n",
      "Credit_Score:        324\n",
      "Annual_Income:      36174\n",
      "Years_in_current_job:         12\n",
      "Home_Ownership:          4\n",
      "Purpose:         16\n",
      "Monthly_Debt:      65765\n",
      "Years_of_Credit_History:        506\n",
      "Months_since_last_delinquent:        117\n",
      "Number_of_Open_Accounts:         51\n",
      "Number_of_Credit_Problems:         14\n",
      "Current_Credit_Balance:      32730\n",
      "Maximum_Open_Credit:      44596\n",
      "Bankruptcies:          9\n",
      "Tax_Liens:         13\n"
     ]
    }
   ],
   "source": [
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modifiche agli attributi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndi default questa trasformazione genera un RDD di Row().\\nle Row sono tipi particolari di Tuple e sono quindi oggetti immutabili.\\nin tutto il notebook per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni) in RDD di Dictionaries.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sdf.rdd\n",
    "\n",
    "'''\n",
    "di default questa trasformazione genera un RDD di Row().\n",
    "le Row sono tipi particolari di Tuple e sono quindi oggetti immutabili.\n",
    "in tutto il notebook per modificare gli RDD li trasformiamo temporaneamente (dentro le funzioni) in RDD di Dictionaries.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gli attributi che riguardano somme di denaro sono quantità denominate in valuta russa (Rubli). Per rendere più comprensibile il loro significato li convertiamo in Euro dividendo tutti i loro valori per il tasso di cambio EUR/RUB medio arrotondato dell'anno 2016 (anno a cui si riferiscono i dati del dataset): 70.\n",
    "\n",
    "questa modifica non impatta assolutamente nessuna analisi perché tutte le quantità monetarie vengono trasformate alla stessa maniera e quindi le proporzioni vengono mantenute. Sarà sempre possibile trasformare facilmente tutto di nuovo in Rubli qualora sia necessario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Current_Loan_Amount'] is not None:\n",
    "        d['Current_Loan_Amount'] = round(d['Current_Loan_Amount']/70)\n",
    "    \n",
    "    if d['Annual_Income'] is not None:    \n",
    "        d['Annual_Income'] = round(d['Annual_Income']/70)\n",
    "    \n",
    "    if d['Monthly_Debt'] is not None:\n",
    "        d['Monthly_Debt'] = round(float(d['Monthly_Debt']/70), 4)\n",
    "        \n",
    "    if d['Current_Credit_Balance'] is not None:\n",
    "        d['Current_Credit_Balance'] = round(d['Current_Credit_Balance']/70)\n",
    "        \n",
    "    if d['Maximum_Open_Credit'] is not None:\n",
    "        d['Maximum_Open_Credit'] = round(d['Maximum_Open_Credit']/70)\n",
    "    \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ci sono alcuni attributi che hanno un Data Type incoerente con il loro significato: sono letti come stringhe ma in realtà la loro semantica ci suggerisce di trasformarli in numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|Years_in_current_job|count|\n",
      "+--------------------+-----+\n",
      "|           10+ years|31121|\n",
      "|             2 years| 9134|\n",
      "|             3 years| 8169|\n",
      "|            < 1 year| 8164|\n",
      "|             5 years| 6787|\n",
      "|              1 year| 6460|\n",
      "|             4 years| 6143|\n",
      "|             6 years| 5686|\n",
      "|             7 years| 5577|\n",
      "|             8 years| 4582|\n",
      "|                 n/a| 4222|\n",
      "|             9 years| 3955|\n",
      "|                null|  514|\n",
      "+--------------------+-----+\n",
      "\n",
      "+----------------------------+-----+\n",
      "|Months_since_last_delinquent|count|\n",
      "+----------------------------+-----+\n",
      "|                          NA|53141|\n",
      "|                          13|  922|\n",
      "|                          12|  902|\n",
      "|                          14|  877|\n",
      "|                          15|  865|\n",
      "|                          10|  861|\n",
      "|                           8|  856|\n",
      "|                           9|  849|\n",
      "|                          18|  847|\n",
      "|                          16|  837|\n",
      "|                           6|  836|\n",
      "|                           7|  825|\n",
      "|                          21|  812|\n",
      "|                          17|  809|\n",
      "|                          19|  791|\n",
      "|                          11|  779|\n",
      "|                          22|  775|\n",
      "|                          23|  773|\n",
      "|                          20|  750|\n",
      "|                          28|  746|\n",
      "|                          25|  738|\n",
      "|                          30|  726|\n",
      "|                          27|  723|\n",
      "|                          26|  723|\n",
      "|                          24|  715|\n",
      "|                           5|  703|\n",
      "|                          31|  697|\n",
      "|                          38|  697|\n",
      "|                          29|  686|\n",
      "|                          33|  679|\n",
      "|                          40|  672|\n",
      "|                          32|  668|\n",
      "|                          34|  648|\n",
      "|                          36|  644|\n",
      "|                          39|  637|\n",
      "|                          41|  624|\n",
      "|                          44|  623|\n",
      "|                          45|  621|\n",
      "|                          35|  616|\n",
      "|                          42|  611|\n",
      "|                          43|  605|\n",
      "|                          37|  592|\n",
      "|                          48|  583|\n",
      "|                          47|  572|\n",
      "|                          46|  566|\n",
      "|                          49|  514|\n",
      "|                        null|  514|\n",
      "|                           4|  513|\n",
      "|                           3|  445|\n",
      "|                          53|  444|\n",
      "|                          61|  444|\n",
      "|                          51|  437|\n",
      "|                          59|  427|\n",
      "|                          54|  419|\n",
      "|                           2|  418|\n",
      "|                          60|  417|\n",
      "|                          68|  415|\n",
      "|                          57|  414|\n",
      "|                          63|  406|\n",
      "|                          58|  405|\n",
      "|                          52|  403|\n",
      "|                          55|  402|\n",
      "|                          69|  401|\n",
      "|                          50|  400|\n",
      "|                          56|  391|\n",
      "|                          65|  389|\n",
      "|                          62|  389|\n",
      "|                          64|  388|\n",
      "|                          66|  381|\n",
      "|                          73|  378|\n",
      "|                          71|  377|\n",
      "|                          67|  365|\n",
      "|                          72|  363|\n",
      "|                          74|  359|\n",
      "|                          70|  353|\n",
      "|                          75|  343|\n",
      "|                          78|  342|\n",
      "|                          77|  331|\n",
      "|                          76|  318|\n",
      "|                          80|  309|\n",
      "|                          79|  297|\n",
      "|                           1|  289|\n",
      "|                          81|  287|\n",
      "|                           0|  216|\n",
      "|                          82|  110|\n",
      "|                          83|   17|\n",
      "|                          85|   11|\n",
      "|                          84|    6|\n",
      "|                          87|    4|\n",
      "|                          97|    3|\n",
      "|                          96|    2|\n",
      "|                         120|    2|\n",
      "|                          92|    2|\n",
      "|                         176|    2|\n",
      "|                          88|    2|\n",
      "|                          94|    2|\n",
      "|                         108|    2|\n",
      "|                          89|    2|\n",
      "|                          86|    2|\n",
      "|                          91|    2|\n",
      "|                         139|    1|\n",
      "|                         100|    1|\n",
      "|                         110|    1|\n",
      "|                         115|    1|\n",
      "|                         131|    1|\n",
      "|                         107|    1|\n",
      "|                          90|    1|\n",
      "|                         114|    1|\n",
      "|                         106|    1|\n",
      "|                         130|    1|\n",
      "|                          93|    1|\n",
      "|                         104|    1|\n",
      "|                         148|    1|\n",
      "|                         152|    1|\n",
      "|                         118|    1|\n",
      "|                         129|    1|\n",
      "|                         141|    1|\n",
      "|                         143|    1|\n",
      "+----------------------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|Bankruptcies|count|\n",
      "+------------+-----+\n",
      "|           0|88774|\n",
      "|           1|10475|\n",
      "|        null|  514|\n",
      "|           2|  417|\n",
      "|          NA|  204|\n",
      "|           3|   93|\n",
      "|           4|   27|\n",
      "|           5|    7|\n",
      "|           6|    2|\n",
      "|           7|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|Tax_Liens|count|\n",
      "+---------+-----+\n",
      "|        0|98062|\n",
      "|        1| 1343|\n",
      "|     null|  514|\n",
      "|        2|  374|\n",
      "|        3|  111|\n",
      "|        4|   58|\n",
      "|        5|   16|\n",
      "|        6|   12|\n",
      "|       NA|   10|\n",
      "|        7|    7|\n",
      "|        9|    3|\n",
      "|       11|    2|\n",
      "|       15|    1|\n",
      "|       10|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dei valori distinti degli attributi con data type incoerente\n",
    "\n",
    "problematic_columns = ['Years_in_current_job', 'Months_since_last_delinquent', 'Bankruptcies', 'Tax_Liens']\n",
    "\n",
    "for col in problematic_columns:\n",
    "    sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Years_in_current_job'\n",
    "\n",
    "lo rendiamo numerico in questo modo:\n",
    "\n",
    "le stringhe 'years' e 'year' vengono escluse da ogni valore.\n",
    "\n",
    "sono presenti 4222 valori uguali a 'n/a'. Pensiamo si possa trattare di soggetti per cui non si può dire qunati sono stati impiegati nel lavoro corrente perché sono attualmente senza occupazione. Il valore viene quindi sostituito con 0.\n",
    "\n",
    "10+ viene trasformato in 10 perché 10+ non ha valenza semantica non conoscendo la distribuzione dei valori specifici per questa categoria. Questa trasformazione non comporta problematiche per algoritmi di machine learning come il Decision Tree perché l'ordinamento dei valori numerici è preservato (basterà tenere presente che un eventuale split sul valore 10 sarebbe in realtà riferito a valori anche maggiori di 10). L'unica problematica apparente potrebbe manifestarsi per algoritmi basati sulle distanze (es. Clustering, PCA, KNN ecc...) perché una certa distanza dal valore 10 potrebbe in realtà essere una distanza molto maggiore. Ma non ci preoccupiamo di questo perché le ennuple coinvolte sono relativamente poche e l'errore non avrebbe un impatto rilevante sul calcolo della distanza multidimensionale. E anche soprattutto perché arrivati a 10 anni di lavoro in una posizione si ritiene che il fattore lavorativo non sia ormai più un problema e quindi un'ipotetica \"distanza\" ad esempio tra 10 e 25 anni è tutto sommato meno rilevante di una distanza, anche minore, ma ad esempio tra 1 e 10.\n",
    "\n",
    "< 1 viene trasformato nella media dei valori tra 0 e 1, cioè 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Years_in_current_job'] is not None:\n",
    "        d['Years_in_current_job'] = str(d['Years_in_current_job']).replace(' years', '')\n",
    "        d['Years_in_current_job'] = str(d['Years_in_current_job']).replace(' year', '')\n",
    "        \n",
    "        if d['Years_in_current_job'] == 'n/a':\n",
    "            d['Years_in_current_job'] = 0\n",
    "        if d['Years_in_current_job'] == '10+':\n",
    "            d['Years_in_current_job'] = 10\n",
    "        if d['Years_in_current_job'] == '< 1':\n",
    "            d['Years_in_current_job'] = 0.5\n",
    "        \n",
    "        d['Years_in_current_job'] = float(d['Years_in_current_job'])\n",
    "    \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Months_since_last_delinquent'\n",
    "\n",
    "sono presenti 53141 valori uguali a 'NA' (la maggior parte). la nostra interpretazione di questo valore è che il soggetto in questione non ha mai commesso nessun reato (interpretazione coerente con il fatto che i valori 'NA' sono la maggior parte).\n",
    "\n",
    "il conteggio degli altri valori distinti è basso, decidiamo quindi di rendere l'attributo categorico raccogliendo i valori in questi range:\n",
    "\n",
    "[0, 12); [12, 48); [48, 96); [96, +inf); 'Never committed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Months_since_last_delinquent'] is not None:\n",
    "        if d['Months_since_last_delinquent'] == 'NA':\n",
    "            d['Months_since_last_delinquent'] = 'Never committed'\n",
    "        elif int(d['Months_since_last_delinquent']) < 12:\n",
    "            d['Months_since_last_delinquent'] = '[0, 12)'\n",
    "        elif int(d['Months_since_last_delinquent']) < 48:\n",
    "            d['Months_since_last_delinquent'] = '[12, 48)'\n",
    "        elif int(d['Months_since_last_delinquent']) < 96:\n",
    "            d['Months_since_last_delinquent'] = '[48, 96)'\n",
    "        else:\n",
    "            d['Months_since_last_delinquent'] = '[96, +inf)'\n",
    "            \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Bankruptcies'\n",
    "\n",
    "l'attributo viene letto come stringa perché sono presenti 204 valori uguali a 'NA'. Poichè il valore 0 è presente si pensa possa trattarsi di missing values, li trasformiamo quindi in None e rendiamo numerico l'attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "    \n",
    "    if d['Bankruptcies'] is not None:\n",
    "        if d['Bankruptcies'] == 'NA':\n",
    "            d['Bankruptcies'] = None\n",
    "        else:\n",
    "            d['Bankruptcies'] = int(d['Bankruptcies'])\n",
    "        \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Tax_Liens'\n",
    "\n",
    "l'attributo viene letto come stringa perché sono presenti 10 valori uguali a 'NA'. Poichè il valore 0 è presente si pensa possa trattarsi di missing values, li trasformiamo quindi in None e rendiamo numerico l'attributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    d = row.asDict()\n",
    "\n",
    "    if d['Tax_Liens'] is not None:\n",
    "        if d['Tax_Liens'] == 'NA':\n",
    "            d['Tax_Liens'] = None\n",
    "        else:\n",
    "            d['Tax_Liens'] = int(d['Tax_Liens'])\n",
    "        \n",
    "    return Row(**d)\n",
    "\n",
    "rdd = rdd.map(change_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = rdd.toDF()\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|Years_in_current_job|count|\n",
      "+--------------------+-----+\n",
      "|                10.0|31121|\n",
      "|                 2.0| 9134|\n",
      "|                 3.0| 8169|\n",
      "|                 0.5| 8164|\n",
      "|                 5.0| 6787|\n",
      "|                 1.0| 6460|\n",
      "|                 4.0| 6143|\n",
      "|                 6.0| 5686|\n",
      "|                 7.0| 5577|\n",
      "|                 8.0| 4582|\n",
      "|                 0.0| 4222|\n",
      "|                 9.0| 3955|\n",
      "|                null|  514|\n",
      "+--------------------+-----+\n",
      "\n",
      "+----------------------------+-----+\n",
      "|Months_since_last_delinquent|count|\n",
      "+----------------------------+-----+\n",
      "|             Never committed|53141|\n",
      "|                    [12, 48)|25789|\n",
      "|                    [48, 96)|13453|\n",
      "|                     [0, 12)| 7590|\n",
      "|                        null|  514|\n",
      "|                  [96, +inf)|   27|\n",
      "+----------------------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|Bankruptcies|count|\n",
      "+------------+-----+\n",
      "|           0|88774|\n",
      "|           1|10475|\n",
      "|        null|  718|\n",
      "|           2|  417|\n",
      "|           3|   93|\n",
      "|           4|   27|\n",
      "|           5|    7|\n",
      "|           6|    2|\n",
      "|           7|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|Tax_Liens|count|\n",
      "+---------+-----+\n",
      "|        0|98062|\n",
      "|        1| 1343|\n",
      "|     null|  524|\n",
      "|        2|  374|\n",
      "|        3|  111|\n",
      "|        4|   58|\n",
      "|        5|   16|\n",
      "|        6|   12|\n",
      "|        7|    7|\n",
      "|        9|    3|\n",
      "|       11|    2|\n",
      "|       10|    1|\n",
      "|       15|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check delle trasformazioni\n",
    "for col in problematic_columns:\n",
    "    sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestione degli errori\n",
    "\n",
    "alcuni attributi hanno valori errati o non coerenti con il loro presunto significato. Alcuni di questi errori sono stati scoperti in fasi più avanzate del progetto (ex Data Understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Credit_Score'\n",
    "\n",
    "ci sono 4551 valori dell'attributo 'Credit_Score' con valore superiore a 5000, che è un valore troppo distante rispetto a quelli generici che assume questo attributo (da 500 a 800). Controllando i valori distinti si è scoperto che quei valori sono errati: c'è uno '0' di troppo in fondo al numero, che eliminamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4551"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def problematic_values_count(row):\n",
    "    if row['Credit_Score'] is not None:\n",
    "        return row['Credit_Score'] > 5000\n",
    "\n",
    "rdd.filter(problematic_values_count).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Credit_Score|count|\n",
      "+------------+-----+\n",
      "|        null|19668|\n",
      "|         747| 1825|\n",
      "|         740| 1746|\n",
      "|         746| 1742|\n",
      "|         741| 1732|\n",
      "|         742| 1723|\n",
      "|         739| 1624|\n",
      "|         745| 1612|\n",
      "|         748| 1598|\n",
      "|         743| 1555|\n",
      "|         725| 1548|\n",
      "|         724| 1522|\n",
      "|         738| 1495|\n",
      "|         744| 1485|\n",
      "|         721| 1465|\n",
      "|         723| 1421|\n",
      "|         737| 1405|\n",
      "|         722| 1387|\n",
      "|         718| 1261|\n",
      "|         750| 1234|\n",
      "|         717| 1218|\n",
      "|         720| 1216|\n",
      "|         736| 1156|\n",
      "|         734| 1147|\n",
      "|         735| 1134|\n",
      "|         719| 1118|\n",
      "|         732| 1084|\n",
      "|         733| 1073|\n",
      "|         715| 1070|\n",
      "|         716| 1061|\n",
      "|         714| 1046|\n",
      "|         713| 1017|\n",
      "|         731| 1010|\n",
      "|         712| 1006|\n",
      "|         730|  984|\n",
      "|         728|  931|\n",
      "|         729|  925|\n",
      "|         708|  902|\n",
      "|         709|  865|\n",
      "|         710|  857|\n",
      "|         726|  853|\n",
      "|         749|  827|\n",
      "|         707|  814|\n",
      "|         727|  806|\n",
      "|         711|  791|\n",
      "|         706|  791|\n",
      "|         705|  727|\n",
      "|         751|  723|\n",
      "|         703|  717|\n",
      "|         704|  716|\n",
      "|         699|  660|\n",
      "|         702|  644|\n",
      "|         701|  620|\n",
      "|         700|  612|\n",
      "|         698|  595|\n",
      "|         697|  552|\n",
      "|         693|  509|\n",
      "|         694|  506|\n",
      "|         695|  506|\n",
      "|         692|  504|\n",
      "|         696|  483|\n",
      "|         691|  386|\n",
      "|         689|  384|\n",
      "|         686|  369|\n",
      "|         685|  369|\n",
      "|         687|  367|\n",
      "|         683|  365|\n",
      "|         680|  346|\n",
      "|         688|  338|\n",
      "|         684|  327|\n",
      "|         690|  320|\n",
      "|         678|  298|\n",
      "|         682|  294|\n",
      "|         679|  285|\n",
      "|         676|  284|\n",
      "|         681|  277|\n",
      "|         675|  262|\n",
      "|         674|  262|\n",
      "|         677|  234|\n",
      "|         669|  230|\n",
      "|         668|  227|\n",
      "|         670|  227|\n",
      "|         672|  219|\n",
      "|         673|  212|\n",
      "|         671|  208|\n",
      "|         663|  182|\n",
      "|         667|  176|\n",
      "|         664|  173|\n",
      "|         656|  171|\n",
      "|         661|  168|\n",
      "|         666|  167|\n",
      "|         665|  161|\n",
      "|         659|  148|\n",
      "|         655|  147|\n",
      "|         660|  144|\n",
      "|         662|  137|\n",
      "|         654|  129|\n",
      "|         649|  126|\n",
      "|         657|  123|\n",
      "|         652|  122|\n",
      "|         658|  118|\n",
      "|         653|  118|\n",
      "|        7390|  108|\n",
      "|        7370|  106|\n",
      "|         651|  101|\n",
      "|         645|   99|\n",
      "|         650|   98|\n",
      "|        7400|   97|\n",
      "|        7380|   93|\n",
      "|         640|   88|\n",
      "|        7330|   88|\n",
      "|        7410|   87|\n",
      "|         643|   87|\n",
      "|        7340|   87|\n",
      "|        7360|   85|\n",
      "|        7240|   84|\n",
      "|        7420|   84|\n",
      "|        7320|   83|\n",
      "|         642|   81|\n",
      "|        7280|   77|\n",
      "|         641|   77|\n",
      "|         648|   76|\n",
      "|        7350|   76|\n",
      "|        7460|   76|\n",
      "|         646|   75|\n",
      "|        7250|   75|\n",
      "|        7170|   74|\n",
      "|        7270|   74|\n",
      "|        7300|   73|\n",
      "|        7310|   73|\n",
      "|         644|   73|\n",
      "|         635|   72|\n",
      "|         638|   71|\n",
      "|         647|   71|\n",
      "|        7430|   70|\n",
      "|        7190|   68|\n",
      "|        7160|   68|\n",
      "|         632|   67|\n",
      "|        7260|   64|\n",
      "|        7230|   63|\n",
      "|        7200|   63|\n",
      "|        7140|   62|\n",
      "|         636|   62|\n",
      "|        7290|   60|\n",
      "|        7220|   59|\n",
      "|        7440|   58|\n",
      "|         633|   58|\n",
      "|        7090|   58|\n",
      "|        7210|   57|\n",
      "|        7180|   57|\n",
      "|        7450|   55|\n",
      "|        7100|   54|\n",
      "|        7130|   53|\n",
      "|        7070|   52|\n",
      "|         624|   52|\n",
      "|        7120|   51|\n",
      "|        7470|   51|\n",
      "|         622|   50|\n",
      "|        7080|   50|\n",
      "|        7040|   49|\n",
      "|         637|   47|\n",
      "|        7010|   46|\n",
      "|        6990|   46|\n",
      "|        7150|   45|\n",
      "|        7110|   45|\n",
      "|         634|   45|\n",
      "|        7060|   45|\n",
      "|        7050|   44|\n",
      "|         628|   44|\n",
      "|         627|   43|\n",
      "|        7480|   43|\n",
      "|         619|   42|\n",
      "|         639|   42|\n",
      "|         621|   41|\n",
      "|        6950|   39|\n",
      "|        7030|   39|\n",
      "|         626|   39|\n",
      "|        6960|   38|\n",
      "|        6940|   38|\n",
      "|        7020|   37|\n",
      "|        6930|   36|\n",
      "|        6980|   36|\n",
      "|         625|   36|\n",
      "|        6840|   35|\n",
      "|         630|   35|\n",
      "|        7000|   35|\n",
      "|         616|   34|\n",
      "|         614|   33|\n",
      "|         631|   33|\n",
      "|        6970|   32|\n",
      "|         612|   32|\n",
      "|         618|   32|\n",
      "|         617|   31|\n",
      "|        6810|   30|\n",
      "|         615|   30|\n",
      "|         629|   30|\n",
      "|         611|   29|\n",
      "|        6920|   29|\n",
      "|        6890|   29|\n",
      "|         601|   29|\n",
      "|        6860|   28|\n",
      "|         623|   28|\n",
      "|         610|   28|\n",
      "|        6870|   27|\n",
      "|         620|   26|\n",
      "|         609|   26|\n",
      "|        6900|   25|\n",
      "|         600|   25|\n",
      "|        6650|   25|\n",
      "|        6800|   25|\n",
      "|        6850|   25|\n",
      "|        6830|   24|\n",
      "|        6880|   24|\n",
      "|        7500|   24|\n",
      "|        6740|   24|\n",
      "|         605|   23|\n",
      "|         613|   23|\n",
      "|        7490|   23|\n",
      "|         597|   23|\n",
      "|        6910|   22|\n",
      "|        6750|   22|\n",
      "|        6820|   21|\n",
      "|        6630|   21|\n",
      "|         606|   20|\n",
      "|         588|   20|\n",
      "|        6780|   20|\n",
      "|        6660|   19|\n",
      "|         595|   19|\n",
      "|         603|   19|\n",
      "|         608|   19|\n",
      "|        6790|   18|\n",
      "|        6680|   17|\n",
      "|        6720|   17|\n",
      "|        6670|   17|\n",
      "|        6700|   17|\n",
      "|         602|   16|\n",
      "|        6770|   16|\n",
      "|        6690|   16|\n",
      "|        6730|   16|\n",
      "|         596|   15|\n",
      "|         599|   15|\n",
      "|        6520|   15|\n",
      "|        6710|   14|\n",
      "|        6450|   13|\n",
      "|         604|   13|\n",
      "|        6500|   13|\n",
      "|        6620|   13|\n",
      "|        6580|   13|\n",
      "|        6610|   13|\n",
      "|         607|   13|\n",
      "|        6560|   12|\n",
      "|         585|   12|\n",
      "|        6590|   12|\n",
      "|        6760|   12|\n",
      "|        6530|   12|\n",
      "|        6410|   12|\n",
      "|        6600|   11|\n",
      "|        6570|   11|\n",
      "|         587|   11|\n",
      "|         594|   10|\n",
      "|        6400|    9|\n",
      "|        7510|    9|\n",
      "|        6380|    9|\n",
      "|         591|    9|\n",
      "|         598|    9|\n",
      "|        6640|    9|\n",
      "|        6510|    9|\n",
      "|         590|    8|\n",
      "|        6550|    8|\n",
      "|        6240|    8|\n",
      "|        6260|    7|\n",
      "|        6390|    7|\n",
      "|         586|    7|\n",
      "|         593|    7|\n",
      "|        6220|    7|\n",
      "|        6490|    7|\n",
      "|        6440|    7|\n",
      "|        6540|    7|\n",
      "|        6320|    7|\n",
      "|        6480|    6|\n",
      "|        6470|    6|\n",
      "|        6230|    6|\n",
      "|         589|    6|\n",
      "|        6420|    6|\n",
      "|        6350|    6|\n",
      "|        6300|    6|\n",
      "|        6160|    6|\n",
      "|        6330|    6|\n",
      "|        6180|    5|\n",
      "|        6290|    5|\n",
      "|         592|    4|\n",
      "|        6090|    4|\n",
      "|        6340|    4|\n",
      "|        6430|    4|\n",
      "|        6250|    4|\n",
      "|        6270|    4|\n",
      "|        6360|    4|\n",
      "|        6310|    4|\n",
      "|        6110|    4|\n",
      "|        6170|    4|\n",
      "|        6210|    3|\n",
      "|        6130|    3|\n",
      "|        5950|    3|\n",
      "|        5940|    3|\n",
      "|        6150|    3|\n",
      "|        6280|    3|\n",
      "|        6460|    3|\n",
      "|        6190|    2|\n",
      "|        6100|    2|\n",
      "|        6060|    2|\n",
      "|        6140|    2|\n",
      "|        6120|    2|\n",
      "|        5850|    2|\n",
      "|        6080|    2|\n",
      "|        6370|    2|\n",
      "|        6200|    2|\n",
      "|        5930|    2|\n",
      "|        6070|    1|\n",
      "|        6010|    1|\n",
      "|        6050|    1|\n",
      "|        5890|    1|\n",
      "|        5860|    1|\n",
      "|        5960|    1|\n",
      "|        5920|    1|\n",
      "|        5900|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = 'Credit_Score'\n",
    "sdf.select(col).groupBy(col).count().orderBy('count', ascending=False).show(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100514"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def errors_correction(row):\n",
    "    d = row.asDict()\n",
    "    if d['Credit_Score'] is not None and d['Credit_Score'] > 5000:\n",
    "        d['Credit_Score'] = int(str(d['Credit_Score'])[:-1])\n",
    "    return Row(**d)\n",
    "    \n",
    "rdd = rdd.map(errors_correction)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ci sono diversi valori dell'attributo 'Current_Loan_Amount' che sono pari a 1.428.571 (99.999.999 in Rubli).  un numero eccessivamente più alto rispetto a tutti gli altri valori e che per questo viene eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|Current_Loan_Amount|count|\n",
      "+-------------------+-----+\n",
      "|            1428571|11484|\n",
      "|               null|  514|\n",
      "|               3209|   70|\n",
      "|               3094|   68|\n",
      "|               3099|   67|\n",
      "|               3088|   67|\n",
      "|               3077|   66|\n",
      "|               3138|   64|\n",
      "|               3187|   62|\n",
      "|               1576|   62|\n",
      "|               3176|   62|\n",
      "|               3066|   61|\n",
      "|               3110|   61|\n",
      "|               3195|   61|\n",
      "|               3192|   59|\n",
      "|               3132|   58|\n",
      "|               1845|   57|\n",
      "|               3089|   57|\n",
      "|               3182|   56|\n",
      "|               3185|   56|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('Current_Loan_Amount').groupBy('Current_Loan_Amount').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89030"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def errors_correction(row):\n",
    "    if row['Current_Loan_Amount'] is not None:\n",
    "        return row['Current_Loan_Amount'] != 1428571\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "rdd = rdd.filter(errors_correction)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eliminiamo le righe dove entrambi gli attributi 'Loan_ID' e 'Customer_ID' sono nulli, che corrispondono alle righe dove tutti i valori di tutti gli attributi sono nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88516"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.filter(lambda row: not ( (row['Loan_ID'] is None) and (row['Customer_ID'] is None) ))\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eliminiamo le righe duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78301"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.distinct()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema: ci sono coppie di righe con tutti i valori duplicati eccetto per le due colonne 'Credit_Score' e 'Annual_Income', per le quali uno dei due valori è presente e l'altro è nullo.\n",
    "\n",
    "Soluzione: si raggruppa per tutti gli attributi tranne quei due e poi si calcola la media di quei due. In questo modo se le uniche due righe uguali sono quelle con un valore nullo e uno non nullo per quegli attributi, lo media sarà uguale al valore non nullo; se invece ci fossero altre righe ugauli ma con altri valori diversi non nulli per quegli attributi, viene effettivamente calcolata la media, il che è auspicabile considerando che tutto il resto della riga è uguale e quindi si tratta molto probabilmente dello stesso oggetto, duplicato per errore, di cui dunque prendiamo un valore medio tra quelli presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rdd.toDF()\n",
    "\n",
    "columns_temp = [col for col in sdf.schema.names if (col != 'Credit_Score' and col != 'Annual_Income')]\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0}, BIGINT(AVG(Credit_Score)) AS Credit_Score, BIGINT(AVG(Annual_Income)) AS Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "GROUP BY {0}\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp, Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "come si nota qui sotto i Customer_ID e Loan_ID che si ripetevano nel dataset originale erano solo righe duplicate. Il dataset pulito non presenta nessuna riga uguale negli ID e possiamo quindi eliminarli. Anche escludendo questi attributi tutte le righe rimangono distinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+\n",
      "|nbr_rows|nbr_customers|nbr_loans|\n",
      "+--------+-------------+---------+\n",
      "|   74094|        74094|    74094|\n",
      "+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows, COUNT(DISTINCT Customer_ID) AS nbr_customers, COUNT(DISTINCT Loan_ID) AS nbr_loans\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in sdf.schema.names if col != 'Customer_ID' and col != 'Loan_ID']\n",
    "\n",
    "sdf = sdf.select(columns)\n",
    "\n",
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in sdf.schema.names if col not in columns_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Annual_Income: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestione dei Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bankruptcies:        156\n",
      "Current_Credit_Balance:          0\n",
      "Current_Loan_Amount:          0\n",
      "Home_Ownership:          0\n",
      "Loan_Status:          0\n",
      "Maximum_Open_Credit:          2\n",
      "Monthly_Debt:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Purpose:          0\n",
      "Tax_Liens:          7\n",
      "Term:          0\n",
      "Years_in_current_job:          0\n",
      "Years_of_Credit_History:          0\n",
      "Credit_Score:      14947\n",
      "Annual_Income:      14947\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procediamo così:\n",
    "\n",
    "- Filliamo i Missing values degli attributi \"Maximum_Open_Credit\", \"Bankruptcies\" e \"Tax_Liens\" usando la Mediana di diversi tipi di raggruppamenti sensati. Per questi attributi è stata fatta questa scelta perché presentano un numero relativamente basso di Missing Values e soprattutto perché una volta fillati sarà poi possibile usarli per eseguire il Clustering.\n",
    "\n",
    "- gli attributi \"Credit_Score\" e \"Annual_Income\" hanno invece un numero rilevante di Missing Values. Li filliamo sempre usando la Mediana di diversi tipi di raggruppamenti, ma in questo caso aggiungiamo come attributo di raggruppamento quello dei Clusters del dataset, derivante dal Clustering migliore che si otterrà. (non possiamo fillare solo sulla base del raggruppamento dei clusters perché il calcolo della Mediana è fattibile solo su un numero contenuto di ennuple e il clustering migliore alla fine è costituito solo da due clusters; inoltre fillare sulla base della mediana di due soli raggruppamenti sbilancerebbe le distribuzioni degli attributi interessati).\n",
    "\n",
    "Usiamo la sintassi dell'SQL analitico per generare le nuove colonne con i valori nulli fillati.\n",
    "\n",
    "in generale i raggruppamenti eseguiti sono quelli più specifici possibile che permettono comunque di fillare la maggior parte dei Missing Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home_Ownership:          4\n",
      "Months_since_last_delinquent:          5\n",
      "Number_of_Credit_Problems:         14\n",
      "Number_of_Open_Accounts:         51\n",
      "Purpose:         16\n",
      "Term:          2\n",
      "Years_in_current_job:         12\n",
      "Years_of_Credit_History:        506\n"
     ]
    }
   ],
   "source": [
    "# gli attributi tra i quali possiamo scegliere di raggruppare sono i seguenti,\n",
    "# quelli con un numero medio-basso di valori distinti (che ricomprendono sia i categorici che alcuni numerici).\n",
    "\n",
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset', print_result=False)\n",
    "\n",
    "for key, value in nbr_distincts.items():\n",
    "    if value < 1000 and (key != 'Loan_Status' and key != 'Credit_Score' and key != 'Bankruptcies' and key != 'Tax_Liens'):\n",
    "        print(key + ':', '{:>10}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_temp = [col for col in sdf.schema.names\n",
    "                if (col != 'Maximum_Open_Credit' and col != 'Bankruptcies' and col != 'Tax_Liens')]\n",
    "\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0},\n",
    "    CASE WHEN Maximum_Open_Credit IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Maximum_Open_Credit, 0.5) OVER(PARTITION BY Years_in_current_job, Home_Ownership, Number_of_Open_Accounts, Years_of_Credit_History) )\n",
    "    ELSE Maximum_Open_Credit END AS Maximum_Open_Credit,\n",
    "    \n",
    "    CASE WHEN Bankruptcies IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Bankruptcies, 0.5) OVER(PARTITION BY Months_since_last_delinquent, Number_of_Credit_Problems) )\n",
    "    ELSE Bankruptcies END AS Bankruptcies,\n",
    "    \n",
    "    CASE WHEN Tax_Liens IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Tax_Liens, 0.5) OVER(PARTITION BY Months_since_last_delinquent, Number_of_Credit_Problems) )\n",
    "    ELSE Tax_Liens END AS Tax_Liens\n",
    "    \n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp, Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_Credit_Balance:          0\n",
      "Current_Loan_Amount:          0\n",
      "Home_Ownership:          0\n",
      "Loan_Status:          0\n",
      "Monthly_Debt:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Purpose:          0\n",
      "Term:          0\n",
      "Years_in_current_job:          0\n",
      "Years_of_Credit_History:          0\n",
      "Credit_Score:      14947\n",
      "Annual_Income:      14947\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Eseguiamo il Clustering con la libreria ML (Con MLlib non è possibile iterare con più valori per l'iper-parametro k e non è possibile calcolare l'SSE per diversi tipi di modelli).\n",
    "\n",
    "Proviamo 3 diversi tipi di clustering (Kmeans, Bisecting Kmeans, Gaussian Mixture Model) per diverse iterazioni, ciascuna con un valore diverso per k: si testa un numero di clusters pari a (2, 3, 4, 5, 8, 10) più altri valori casuali maggiori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bankruptcies',\n",
       " 'Current_Credit_Balance',\n",
       " 'Current_Loan_Amount',\n",
       " 'Maximum_Open_Credit',\n",
       " 'Monthly_Debt',\n",
       " 'Number_of_Credit_Problems',\n",
       " 'Number_of_Open_Accounts',\n",
       " 'Tax_Liens',\n",
       " 'Years_in_current_job',\n",
       " 'Years_of_Credit_History']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per effettuare il clustering dobbiamo considerare solo le colonne numeriche diverse da 'Credit_Score' e 'Annual_Income'.\n",
    "\n",
    "columns_clustering = [col for col in columns_numerical if col != 'Credit_Score' and col != 'Annual_Income']\n",
    "\n",
    "columns_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_clustering(spark_df, columns_clustering, models=['kmeans', 'biskmeans', 'gaussmix'], k_clusters_iter=[2, 10],\n",
    "                   clusters_col_name='cluster_label'):\n",
    "    \n",
    "    \"\"\"funzione che prende in input tutto il dataframe, specificando le colonne su cui si vuole eseguire il clustering.\n",
    "    Standardizza i dati rendendo ciascuna colonna a media 0 e varianza unitaria.\n",
    "    Esegue fino a 3 tipi diversi di clustering, iterando più volte con i numeri di clusters specificati.\n",
    "    Restituisce il dataframe con una nuova colonna con la label del clustering migliore in termini di Silhouette,\n",
    "    il numero di clusters, il modello e la Silhouette.\"\"\"\n",
    "    \n",
    "    columns = spark_df.schema.names\n",
    "    \n",
    "    sdf = VectorAssembler(inputCols=columns_clustering, outputCol=\"columns_clustering\").transform(spark_df)\n",
    "    scaler = StandardScaler(inputCol=\"columns_clustering\", outputCol=\"scaled_columns_clustering\", withStd=True, withMean=True)\n",
    "    sdf = scaler.fit(sdf).transform(sdf).select(columns + ['scaled_columns_clustering'])\n",
    "    \n",
    "    best_silhouette = -1\n",
    "    \n",
    "    for k_clusters in k_clusters_iter:\n",
    "        \n",
    "        if 'kmeans' in models:\n",
    "            model = KMeans(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                           maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "                \n",
    "        if 'biskmeans' in models:\n",
    "            model = BisectingKMeans(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                                    maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "                  \n",
    "        if 'gaussmix' in models:\n",
    "            model = GaussianMixture(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                                    maxIter=20).setK(k_clusters).fit(sdf)\n",
    "            sdf = model.transform(sdf)\n",
    "\n",
    "            silhouette = ClusteringEvaluator(featuresCol='scaled_columns_clustering', predictionCol=clusters_col_name,\n",
    "                                             metricName=\"silhouette\").evaluate(sdf)\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model = model\n",
    "                nbr_clusters = k_clusters\n",
    "                sdf_clustered = sdf\n",
    "            \n",
    "            sdf = sdf.select(columns + ['scaled_columns_clustering'])\n",
    "    \n",
    "    sdf_clustered = sdf_clustered.select(columns + [clusters_col_name])\n",
    "    \n",
    "    return sdf_clustered, nbr_clusters, best_model, best_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################### NON RUNNARE!!\n",
    "# il tempo per eseguire può essere elavato.\n",
    "\n",
    "k_clusters_list = [k for k in [2, 3, 4, 5, 8, 10] + list(sample(range(11, 30), size=5, replace=False))]\n",
    "\n",
    "sdf, nbr_clusters, clustering_model, silhouette_score = best_clustering(spark_df=sdf,\n",
    "                                                                        columns_clustering=columns_clustering,\n",
    "                                                                        k_clusters_iter=k_clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best clustering model:  GaussianMixture_373175122b8d\n",
      "number of clusters:  2\n",
      "silhouette score:  0.777754770441628\n"
     ]
    }
   ],
   "source": [
    "print('best clustering model: ', clustering_model)\n",
    "print('number of clusters: ', nbr_clusters)\n",
    "print('silhouette score: ', silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il migior modello di Clustering è un Gaussian Mixture Model con k=2, cioè un modello che trova due distribuzioni Normali nel dataset che massimizzano la Log-Likelihood.\n",
    "\n",
    "Usiamo i clusters generati da questo modello per fillare i Missing Values mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- cluster_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_temp = [col for col in sdf.schema.names if (col != 'Credit_Score' and col != 'Annual_Income')]\n",
    "\n",
    "Project = ', '.join(columns_temp)\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT {0},\n",
    "    CASE WHEN Credit_Score IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Credit_Score, 0.5) OVER(PARTITION BY cluster_label, Years_in_current_job, Home_Ownership, Years_of_Credit_History) )\n",
    "    ELSE Credit_Score END AS Credit_Score,\n",
    "    \n",
    "    CASE WHEN Annual_Income IS NULL THEN\n",
    "        BIGINT( PERCENTILE(Annual_Income, 0.5) OVER(PARTITION BY cluster_label, Years_in_current_job, Home_Ownership, Years_of_Credit_History) )\n",
    "    ELSE Annual_Income END AS Annual_Income\n",
    "\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\".format(Project)\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "del columns_temp, Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_Credit_Balance:          0\n",
      "Current_Loan_Amount:          0\n",
      "Home_Ownership:          0\n",
      "Loan_Status:          0\n",
      "Monthly_Debt:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Purpose:          0\n",
      "Term:          0\n",
      "Years_in_current_job:          0\n",
      "Years_of_Credit_History:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n",
      "cluster_label:          0\n",
      "Credit_Score:       1097\n",
      "Annual_Income:       1097\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72997"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rimangono 1291 ennuple con valori nulli per i due attributi, che eliminiamo.\n",
    "# il motivo è che in alcuni raggruppamenti sono presenti solo valori nulli e dunque nessuna Mediana è calcolabile.\n",
    "\n",
    "sdf = sdf.rdd.filter(lambda row: not ( (row['Credit_Score'] is None) and (row['Annual_Income'] is None) )).toDF()\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_Credit_Balance:          0\n",
      "Current_Loan_Amount:          0\n",
      "Home_Ownership:          0\n",
      "Loan_Status:          0\n",
      "Monthly_Debt:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Purpose:          0\n",
      "Term:          0\n",
      "Years_in_current_job:          0\n",
      "Years_of_Credit_History:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Bankruptcies:          0\n",
      "Tax_Liens:          0\n",
      "cluster_label:          0\n",
      "Credit_Score:          0\n",
      "Annual_Income:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo pensato potesse essere interessante verificare a che percentuale del debito totale coincidesse il debito mensile così da avere un'idea delle dimensioni della rata mensile, in quanto potrebbe influenzare lo stato del prestito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND((Monthly_Debt/Current_Loan_Amount)*100, 4) AS Installment_Rate\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si è  poi deciso di creare una nuova variabile derivata: \"(Monthly_Debt*12)/Annual_Income \" per comprendere quanto del reddito annuale percepiuto dal soggetto viene utilizzato per pagare il debito totale creato verso la banca, poichè può influenzare il Loan Status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND(((Monthly_Debt*12)/Annual_Income)*100, 4) AS Debt_Income_Rate\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'interno del dataset è presente la variabile \"Number_of_Credit_Problems\" che fornisce un'informazione incompleta se non analizzata assieme alla variabile \"Number_of_Open_Accounts\", poichè un soggetto con un numero di problemi pari a 3 e con un solo \"Number_of_Open_Accounts\" potrebbe avere un diverso esito del Loan_Status rispetto ad un soggetto con lo stesso numero di problemi ma con \"x\">>1 account aperti. Per questa ragione si è pensato di accorpare le due informazioni in un'unica nuova variabile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql =\"\"\"\n",
    "SELECT *, ROUND((Number_of_Credit_Problems/Number_of_Open_Accounts)*100, 4) AS Credit_Problems_Perc\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- cluster_label: long (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Installment_Rate: double (nullable = true)\n",
      " |-- Debt_Income_Rate: double (nullable = true)\n",
      " |-- Credit_Problems_Perc: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------+------+--------+------+\n",
      "|avg(Installment_Rate)|DistinctVal|MinVal|  MaxVal|MEDIAN|\n",
      "+---------------------+-----------+------+--------+------+\n",
      "|    7.971951940490705|      55039|   0.0|282.5451|5.7369|\n",
      "+---------------------+-----------+------+--------+------+\n",
      "\n",
      "+---------------------+-----------+------+--------+-------+\n",
      "|avg(Debt_Income_Rate)|DistinctVal|MinVal|  MaxVal| MEDIAN|\n",
      "+---------------------+-----------+------+--------+-------+\n",
      "|   17.491117434963073|      27552|   0.0|181.9993|16.8002|\n",
      "+---------------------+-----------+------+--------+-------+\n",
      "\n",
      "+-------------------------+-----------+------+------+------+\n",
      "|avg(Credit_Problems_Perc)|DistinctVal|MinVal|MaxVal|MEDIAN|\n",
      "+-------------------------+-----------+------+------+------+\n",
      "|       1.7298004753750262|        102|   0.0| 300.0|   0.0|\n",
      "+-------------------------+-----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_columns=['Installment_Rate', 'Debt_Income_Rate' , 'Credit_Problems_Perc']\n",
    "\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "for col in new_columns:\n",
    "    sql = \"\"\"\n",
    "    SELECT  AVG({0}), COUNT(DISTINCT {0}) AS DistinctVal, MIN(CAST({0} AS DOUBLE)) AS MinVal, MAX(CAST({0} AS DOUBLE)) AS MaxVal, PERCENTILE(CAST({0} AS DOUBLE),0.5) AS MEDIAN\n",
    "    FROM Bank_Loan_Dataset\n",
    "    \"\"\".format(col)\n",
    "\n",
    "    spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per verificare la correttezza e coerenza della nuova variabile Installment_Rate(il tasso medio della rata del prestito) si è deciso di confrontare il suo valore medio e mediamo all'interno del dataset con il tasso governativo decennale Russo del 2016 che ha raggiunto come valore minimo 7.940% l'8 Settembre. Essendo la media ottenuta pari a 7.9719 possiamo affermare che il dato sia sufficientemente coerente.\n",
    "\n",
    "Controlliamo ora le correlazione tra le nuove variabili e quelle da cui hanno origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.32715453, -0.38400276],\n",
       "       [ 0.32715453,  1.        ,  0.43531804],\n",
       "       [-0.38400276,  0.43531804,  1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Installment_Rate', 'Monthly_Debt', 'Current_Loan_Amount'])\\\n",
    "                .rdd.map(lambda row: Vectors_mllib.dense(row)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.58420176, -0.16158333],\n",
       "       [ 0.58420176,  1.        ,  0.42504303],\n",
       "       [-0.16158333,  0.42504303,  1.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Debt_Income_Rate', 'Monthly_Debt','Annual_Income'])\\\n",
    "                .rdd.map(lambda row: Vectors_mllib.dense(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.8970937 , -0.11897398],\n",
       "       [ 0.8970937 ,  1.        , -0.01247425],\n",
       "       [-0.11897398, -0.01247425,  1.        ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(\\\n",
    "                sdf.select(['Credit_Problems_Perc', 'Number_of_Credit_Problems', 'Number_of_Open_Accounts'])\\\n",
    "                .rdd.filter(lambda row: row['Credit_Problems_Perc'] is not None).map(lambda row: Vectors_mllib.dense(row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la correlazione tra 'Credit_Problems_Perc' e 'Number_of_Credit_Problems' è troppo elevata, quindi non aggiungiamo l'attributo 'Credit_Problems_Perc' al dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.drop('Credit_Problems_Perc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check finale ed Export nell'HDFS del Dataset definitivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = sdf.schema.names\n",
    "\n",
    "columns.sort()\n",
    "\n",
    "sdf = sdf.select(columns)\n",
    "\n",
    "columns_categorical = [col.name for col in sdf.schema.fields if isinstance(col.dataType, StringType)]\n",
    "\n",
    "columns_numerical = [col for col in columns if col not in columns_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Annual_Income: long (nullable = true)\n",
      " |-- Bankruptcies: long (nullable = true)\n",
      " |-- Credit_Score: long (nullable = true)\n",
      " |-- Current_Credit_Balance: long (nullable = true)\n",
      " |-- Current_Loan_Amount: long (nullable = true)\n",
      " |-- Debt_Income_Rate: double (nullable = true)\n",
      " |-- Home_Ownership: string (nullable = true)\n",
      " |-- Installment_Rate: double (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      " |-- Maximum_Open_Credit: long (nullable = true)\n",
      " |-- Monthly_Debt: double (nullable = true)\n",
      " |-- Months_since_last_delinquent: string (nullable = true)\n",
      " |-- Number_of_Credit_Problems: long (nullable = true)\n",
      " |-- Number_of_Open_Accounts: long (nullable = true)\n",
      " |-- Purpose: string (nullable = true)\n",
      " |-- Tax_Liens: long (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Years_in_current_job: double (nullable = true)\n",
      " |-- Years_of_Credit_History: double (nullable = true)\n",
      " |-- cluster_label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annual_Income',\n",
       " 'Bankruptcies',\n",
       " 'Credit_Score',\n",
       " 'Current_Credit_Balance',\n",
       " 'Current_Loan_Amount',\n",
       " 'Debt_Income_Rate',\n",
       " 'Home_Ownership',\n",
       " 'Installment_Rate',\n",
       " 'Loan_Status',\n",
       " 'Maximum_Open_Credit',\n",
       " 'Monthly_Debt',\n",
       " 'Months_since_last_delinquent',\n",
       " 'Number_of_Credit_Problems',\n",
       " 'Number_of_Open_Accounts',\n",
       " 'Purpose',\n",
       " 'Tax_Liens',\n",
       " 'Term',\n",
       " 'Years_in_current_job',\n",
       " 'Years_of_Credit_History',\n",
       " 'cluster_label']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annual_Income',\n",
       " 'Bankruptcies',\n",
       " 'Credit_Score',\n",
       " 'Current_Credit_Balance',\n",
       " 'Current_Loan_Amount',\n",
       " 'Debt_Income_Rate',\n",
       " 'Installment_Rate',\n",
       " 'Maximum_Open_Credit',\n",
       " 'Monthly_Debt',\n",
       " 'Number_of_Credit_Problems',\n",
       " 'Number_of_Open_Accounts',\n",
       " 'Tax_Liens',\n",
       " 'Years_in_current_job',\n",
       " 'Years_of_Credit_History',\n",
       " 'cluster_label']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home_Ownership',\n",
       " 'Loan_Status',\n",
       " 'Months_since_last_delinquent',\n",
       " 'Purpose',\n",
       " 'Term']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Income:          0\n",
      "Bankruptcies:          0\n",
      "Credit_Score:          0\n",
      "Current_Credit_Balance:          0\n",
      "Current_Loan_Amount:          0\n",
      "Debt_Income_Rate:          0\n",
      "Home_Ownership:          0\n",
      "Installment_Rate:          0\n",
      "Loan_Status:          0\n",
      "Maximum_Open_Credit:          0\n",
      "Monthly_Debt:          0\n",
      "Months_since_last_delinquent:          0\n",
      "Number_of_Credit_Problems:          0\n",
      "Number_of_Open_Accounts:          0\n",
      "Purpose:          0\n",
      "Tax_Liens:          0\n",
      "Term:          0\n",
      "Years_in_current_job:          0\n",
      "Years_of_Credit_History:          0\n",
      "cluster_label:          0\n"
     ]
    }
   ],
   "source": [
    "nbr_nulls = get_nbr_nulls(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Income:      24774\n",
      "Bankruptcies:          8\n",
      "Credit_Score:        167\n",
      "Current_Credit_Balance:      13165\n",
      "Current_Loan_Amount:       9905\n",
      "Debt_Income_Rate:      27552\n",
      "Home_Ownership:          4\n",
      "Installment_Rate:      55039\n",
      "Loan_Status:          2\n",
      "Maximum_Open_Credit:      21774\n",
      "Monthly_Debt:      59926\n",
      "Months_since_last_delinquent:          5\n",
      "Number_of_Credit_Problems:         14\n",
      "Number_of_Open_Accounts:         51\n",
      "Purpose:         16\n",
      "Tax_Liens:         12\n",
      "Term:          2\n",
      "Years_in_current_job:         12\n",
      "Years_of_Credit_History:        499\n",
      "cluster_label:          2\n"
     ]
    }
   ],
   "source": [
    "nbr_distincts = get_nbr_distincts(spark_df = sdf, view_name = 'Bank_Loan_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il dataset poi viene letto tramite:\n",
    "# sdf = spark.read.parquet(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa04/bank_loan_status_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ricordati!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anche se non esistono valori distinti  COUNT(\\*)  può differire da COUNT(DISTINCT \\*).\n",
    "\n",
    "perché il primo conta tutte le righe mentre il secondo conta tutte e sole le righe dove non è presente neanche un NULL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(Credit_Score) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(DISTINCT *) AS nbr_rows\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tentativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  tentativo non riuscito\\nrdd = sdf.rdd.map(lambda row: row.asDict())\\n\\nrdd.keyBy(lambda row: (row['Years_in_current_job'],\\n                             row['Home_Ownership'],\\n                             row['Number_of_Open_Accounts'],\\n                             row['Years_of_Credit_History'])).groupByKey()\\n                             \\ndef fill_null(d):\\n    if d['Maximum_Open_Credit'] == None:\\n        d['Maximum_Open_Credit'] = \\n    return d\\n\\nrdd.mapValues(fill_null)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  tentativo non riuscito per fillare i missing values\n",
    "rdd = sdf.rdd.map(lambda row: row.asDict())\n",
    "\n",
    "rdd.keyBy(lambda row: (row['Years_in_current_job'],\n",
    "                             row['Home_Ownership'],\n",
    "                             row['Number_of_Open_Accounts'],\n",
    "                             row['Years_of_Credit_History'])).groupByKey()\n",
    "                             \n",
    "def fill_null(d):\n",
    "    if d['Maximum_Open_Credit'] == None:\n",
    "        d['Maximum_Open_Credit'] = \n",
    "    return d\n",
    "\n",
    "rdd.mapValues(fill_null)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = sdf.select(columns_clustering).rdd.map(lambda row: Vectors_mllib.dense(row))\\n\\nX_scaled = StandardScaler_mllib(withMean=True, withStd=True).fit(X).transform(X)\\n\\n\\n questo da un errore inspiegabilmente\\nfor k_clusters in sample(range(2, 10), size=1, replace=False):\\n    current_clustering_model = KMeans.train(X_scaled, k=k_clusters, maxIterations=20, initializationMode=\"random\")\\n\\n\\n\\nKMeans_model = KMeans_mllib.train(X_scaled, k = 1, maxIterations=10, initializationMode=\"random\")\\n\\n\\ndef error(point):\\n    center = KMeans_model.centers[KMeans_model.predict(point)]\\n    return sqrt(sum([x**2 for x in (point - center)]))\\n\\nWSSE = X_scaled.map(lambda point: error(point)).reduce(lambda x, y: x + y)\\nprint(\"Best Within Sum of Squared Error = \", WSSE)\\n\\n\\nGaussianMixture_model = GaussianMixture_mllib.train(X_scaled, k=1)\\n\\nBisectingKMeans_model = BisectingKMeans_mllib.train(X_scaled, k=1, maxIterations=10)\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X = sdf.select(columns_clustering).rdd.map(lambda row: Vectors_mllib.dense(row))\n",
    "\n",
    "X_scaled = StandardScaler_mllib(withMean=True, withStd=True).fit(X).transform(X)\n",
    "\n",
    "\n",
    "'''''' questo da un errore inspiegabilmente\n",
    "for k_clusters in sample(range(2, 10), size=1, replace=False):\n",
    "    current_clustering_model = KMeans.train(X_scaled, k=k_clusters, maxIterations=20, initializationMode=\"random\")\n",
    "''''''\n",
    "\n",
    "\n",
    "KMeans_model = KMeans_mllib.train(X_scaled, k = 1, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "\n",
    "def error(point):\n",
    "    center = KMeans_model.centers[KMeans_model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSE = X_scaled.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Best Within Sum of Squared Error = \", WSSE)\n",
    "\n",
    "\n",
    "GaussianMixture_model = GaussianMixture_mllib.train(X_scaled, k=1)\n",
    "\n",
    "BisectingKMeans_model = BisectingKMeans_mllib.train(X_scaled, k=1, maxIterations=10)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sdf.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    BIGINT( PERCENTILE(Maximum_Open_Credit, 0.5) OVER(PARTITION BY Years_in_current_job,\n",
    "                                                Home_Ownership,\n",
    "                                                Number_of_Open_Accounts,\n",
    "                                                Years_of_Credit_History) ) AS toFill_Maximum_Open_Credit,\n",
    "    BIGINT( PERCENTILE(Bankruptcies, 0.5) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                                Number_of_Credit_Problems) ) AS toFill_Bankruptcies,\n",
    "    BIGINT( PERCENTILE(Tax_Liens, 0.5) OVER(PARTITION BY Months_since_last_delinquent,\n",
    "                                            Number_of_Credit_Problems) ) AS toFill_Tax_Liens\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf = spark.sql(sql)\n",
    "\n",
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Maximum_Open_Credit'] is None:\n",
    "        d['Maximum_Open_Credit'] = d['toFill_Maximum_Open_Credit']\n",
    "    if d['Bankruptcies'] is None:\n",
    "        d['Bankruptcies'] = d['toFill_Bankruptcies']\n",
    "    if d['Tax_Liens'] is None:\n",
    "        d['Tax_Liens'] = d['toFill_Tax_Liens']\n",
    "    return Row(**d)\n",
    "\n",
    "sdf = sdf.rdd.map(fill_nulls).toDF().select(columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sdf_prova.createOrReplaceTempView('Bank_Loan_Dataset')\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *,\n",
    "    BIGINT( PERCENTILE(Credit_Score, 0.5) OVER(PARTITION BY cluster_label) ) AS toFill_Credit_Score,\n",
    "    BIGINT( PERCENTILE(Annual_Income, 0.5) OVER(PARTITION BY cluster_label) ) AS toFill_Annual_Income\n",
    "FROM Bank_Loan_Dataset\n",
    "\"\"\"\n",
    "\n",
    "sdf_prova = spark.sql(sql)\n",
    "\n",
    "def fill_nulls(row):\n",
    "    d = row.asDict()\n",
    "    if d['Credit_Score'] is None:\n",
    "        d['Credit_Score'] = d['toFill_Credit_Score']\n",
    "    if d['Annual_Income'] is None:\n",
    "        d['Annual_Income'] = d['toFill_Annual_Income']\n",
    "    return Row(**d)\n",
    "\n",
    "sdf_prova = sdf_prova.rdd.map(fill_nulls).toDF().select(columns_prova)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
